{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epRaBvj_bapY"
      },
      "source": [
        "# Project 3 (part 2): Language Translation with Neural Networks\n",
        "## CS4740/5740 Fall 2021\n",
        "\n",
        "Names:\n",
        "\n",
        "Netids:\n",
        "\n",
        "### Project Submission Due: November 23, 2021\n",
        "Please submit the **pdf file** of this notebook on **Gradescope**, and the **ipynb** on **CMS**. For instructions on generating pdf and ipynb files, please refer to project 1 or project 2 instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIEkIveBVYyU"
      },
      "source": [
        "## Introduction\n",
        "In this project we will consider **neural networks**:  a Recurrent Neural Network (RNN), for performing neural machine translation (i.e. translating from one language into another).\n",
        "\n",
        "The project is divided into parts. In **Part 1**, you will implement an RNN model for performing the neural machine translation. In **Part 2**, you will analyze these models in two types of comparative studies and in **Part 3** you will answer questions describing what you have learned through this project. You also will be required to submit a description of libraries used, how your group divided up the work, and your feedback regarding the assignment (**Part 4**).\n",
        "\n",
        "The writeup for the document is linked [here](https://docs.google.com/document/d/1IWgYqS6M4G_gJowM97Bq8g5smsGOa75dutIUGjolpAI/edit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1HSug5yWVrI"
      },
      "source": [
        "## Advice 🚀\n",
        "As always, the report is important! The report is where you get to show\n",
        "that you understand not only what you are doing but also why and how you are doing it. So be clear, organized and concise; avoid vagueness and excess verbiage. Spend time doing error analysis for the models. This is how you understand the advantages and drawbacks of the systems you build. The reports should read more like the papers that we have been writing critiques for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fyl3-8TWyR9"
      },
      "source": [
        "## Dataset\n",
        "You are given access to a set of parallel sentences. One sentence is written in modern English (the \"source\") and another is in Shakespearean English (the \"target\"). For this project, given modern English you will need to translate this into Shakespearean English. This is usually called (Neural) Machine Translation. We'll simply refer to it as NMT or Neural Machine Translation in the project.\n",
        "\n",
        "We will minimally preprocess the source/target sentences and handle tokenization in what we release. For this assignment, we do not anticipate any further preprocessing to be done by you. Should you choose to do so, it would be interesting to hear about in the report (along with whether or not it helped performance), but it is not a required aspect of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARl1pk1PGL2Y",
        "outputId": "09ce0a5f-2496-4567-a67c-b235f3a7d17f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "source_path = os.path.join(os.getcwd(), \"drive\", \"My Drive\", \"CS_4740_FA21_p3\", \"source.txt\") # replace based on your Google drive organization\n",
        "target_path = os.path.join(os.getcwd(), \"drive\", \"My Drive\", \"CS_4740_FA21_p3\", \"target.txt\") # replace based on your Google drive organization\n",
        "test_path = os.path.join(os.getcwd(), \"drive\", \"My Drive\", \"CS_4740_FA21_p3\", \"source_test.txt\") # replace based on your Google drive organization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeC3pYiebc6r"
      },
      "source": [
        "## Import libraries and connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21NRQju0KuEo",
        "outputId": "a1e8e94a-a883-41b8-df84-f2b698762b9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quIJujja-jS2",
        "outputId": "e1db6ae2-8d2e-4de7-f508-59933c104a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install sentencepiece\n",
        "from collections import Counter, namedtuple\n",
        "from itertools import chain\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import time\n",
        "import sys\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "\n",
        "\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torch.nn.utils\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "\n",
        "from tqdm.notebook import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ2HeOVlKp-4",
        "outputId": "ad4fb9d5-a2e0-4bc4-baa4-6d8da358183d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWy8nQ7WcNKR"
      },
      "outputs": [],
      "source": [
        "! pip install -qqq wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWYcm7ufcWC1",
        "outputId": "6d9d3f41-fa31-4f59-f663-ab1237885586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchipmunkez\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "! wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jAcTj9sciCZ",
        "outputId": "16904db8-a153-422f-b9c2-5076407f1683"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B online, running your script from this directory will now sync to the cloud.\n"
          ]
        }
      ],
      "source": [
        "!wandb online"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D2uTXRmbn5Q"
      },
      "source": [
        "# Part 1: Recurrent Neural Network\n",
        "Recurrent neural networks have been the workhorse of NLP for a number of years. A fundamental reason for this success is they can inherently deal with _variable_ length sequences. This is axiomatically important for natural language; words are formed from a variable number of characters, sentences from a variable number of words, paragraphs from a variable number of sentences, and so forth. This differs from a field like Computer Vision where images are (generally) of a fixed size.\n",
        "<br></br>\n",
        "This is a also very different scenario than that of the classifiers we have studied (e.g. Naive Bayes, Perceptron Learning, Feedforward Neural Networks), which take in a\n",
        "fixed-length vector.\n",
        "<br></br>\n",
        "To clarify this, we can think of the _types_ of the mathematical functions described by a FFNN and an RNN. What is critical to note in what follows is that k (the length of a sequence) need not be constant\n",
        "across examples.\n",
        "\n",
        "Below we define the general problem set up of FFNNs and RNNs.\n",
        "\n",
        "$\\textbf{FFNN.}$ \\\n",
        "$Input: \\text{We have an input vector }\\vec{x} \\in \\mathcal{R}^d$ \\\n",
        "$Model\\text{ }Output: \\text{The model has some intermediate output }\\vec{z} \\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid}$ \\\n",
        "$Final\\text{ }Output: \\text{ The model outputs a vector } \\vec{y} \\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid}$ \\\n",
        "$\\vec{y}$ satisfies the constraint of being a probability distribution, i.e. $\\underset{i \\in \\mid \\mathcal{Y} \\mid}{\\sum} \\vec{y}[i] = 1$ and $\\underset{i \\in \\mid \\mathcal{y} \\mid}{min} \\text{ }\\vec{y}[i] \\leq 1$, which is achieved via _Softmax_ applied to $\\vec{z}$.\n",
        "<br></br>\n",
        "$\\textbf{RNN.}$ \\\n",
        "$Input: \\text{The model takes as input a sequence of vectors} \\vec{x}_1,\\vec{x}_2, \\dots, \\vec{x}_k; \\vec{x}_i \\in \\mathcal{R}^d$ \\\n",
        "$Model\\text{ }Output: \\text{The model generates some intermediate sequence output} \\vec{z}_1,\\vec{z}_2, \\dots, \\vec{z}_k; \\vec{z}_i \\in \\mathcal{R}^{h}, \\text{ where h is the hidden state size.}$\n",
        "$Final\\text{ }Output: \\text{The model generates some final sequence output} \\vec{y}_1, \\dots, \\vec{y}_k \\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid}$ \\\n",
        "$\\vec{y}$ satisfies the constraint of being a probability distribution, i.e. $\\underset{i \\in \\mid \\mathcal{Y} \\mid}{\\sum} \\vec{y}_j[i] = 1$ and $\\underset{i \\in \\mid \\mathcal{y} \\mid}{min} \\text{ }\\vec{y}_j[i] \\geq 0$, which is achieved by the process described later in this report and as you have seen in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4n8IVFtflPQ"
      },
      "source": [
        "Intuitively, an RNN takes in a sequence of vectors and computes a new vector corresponding to each vector in the original sequence. It achieves this by processing the input sequence one vector at a time to (a) compute an updated representation of the entire sequence (which is then re-used when processing the next vector in the input sequence), and (b) produce an output for the current position. The vector computed in (a) therefore not only contains information about the current input vector but also about the previous input vectors. Hence, $\\vec{z}_j$ is computed after having observed $\\vec{x}_1, \\dots, \\vec{x}_j$. As such, a simple observation is we can treat the last vector computed by the RNN, ie $\\vec{z}_k$ as a representation of the entire sequence. Accordingly, we can use this as the input to a single-layer linear classifier to compute a vector $\\vec{y}$ as we will need for classification.\n",
        "\n",
        "$$\\vec{y}_j = Softmax(W\\vec{z}_j); \\text{ where }W\\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid \\times h} \\text{ is a weight matrix that is learned through training}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfjD4PdHLJeg"
      },
      "source": [
        "In Machine Translation, our goal is to convert a sentence from the source language (e.g. Modern English) to the target language (e.g. Shakespearean English). In this assignment, we will implement a sequence-to-sequence (Seq2Seq) network, to build a Neural Machine Translation (NMT) system. In this section, we describe the training procedure for the proposed NMT system, which uses a Bidirectional RNN Encoder and a Unidirectional RNN Decoder. We'll recap the theoretical component here and in the modules where you are writing code, we will repeat the steps more explicitly in an algorithmic manner.\n",
        "\n",
        "<Insert diagram here>\n",
        "\n",
        "Given a sentence in the source language, we look up the word embeddings from an embeddings matrix, yielding $x_1,\\dots, x_n$ ($x_i \\in R^{e}$), where n is the length of the source sentence and e is the embedding size. We feed these embeddings to the bidirectional encoder, yielding hidden states for both the forward (→) and backward (←) RNNs. The forward and backward versions are concatenated to give hidden states $h_i^{enc}$\n",
        "\n",
        "\n",
        "$$h_i^{enc} = [\\overrightarrow{h_i^{enc}}; \\overleftarrow{h_i^{enc}}] \\text{ where }h_i^{enc} \\in R^{2h}, \\overrightarrow{h_i^{enc}}, \\overleftarrow{h_i^{enc}} \\in R^{h}$$\n",
        "\n",
        "\n",
        "We then initialize the decoder’s first hidden state $h_0^{dec}$ with a linear projection of the encoder’s final hidden state\n",
        "\n",
        "$$h_0^{dec} = W_h[\\overrightarrow{h_n^{enc}}; \\overleftarrow{h_0^{enc}}] \\text{ where }h_0^{dec} \\in R^{h}, W_h \\in R^{h \\times 2h}$$\n",
        "\n",
        "With the decoder initialized, we must now feed it a target sentence. On the $t^{th}$ step, we look up the embedding for the $t^{th}$ word, $y_t \\in R^{e}$. We then concatenate $y_t$ with the combined-output vector $o_{t−1} \\in R^{h}$ from the previous timestep (we will explain what this is later but this is just the output from the previous step) to produce $y_t \\in R^{e+h}$. Note that for the first target (i.e. the start token) $o_0$ is usually a zero-vector (but it can be random or a learned vector as well). We then feed $y_t$ as input to the decoder.\n",
        "\n",
        "$$ h_t^{dec} = Decoder(y_t, h_{t-1}^{dec})\\text{ where }h_{t-1}^{dec} ∈ R^{h}$$\n",
        "\n",
        "We can take the decoder hidden state $h_t^{dec}$ and pass this through a linear layer to obtain an intermediate output $v_t$. This is then passed through an activation function (like tanh) to obtain our combined-output vector $o_t$\n",
        "\n",
        "$$v_t = W_v h_t^{dec} \\text{ where } W_v \\in R^{h \\times h}, v_t \\in R^{h}$$\n",
        "$$o_t = \\tanh{(v_t)} \\text{ where } o_t \\in R^{h}$$\n",
        "\n",
        "Then, we produce a probability distribution $P_t$ over target words at the $t^{th}$ timestep.\n",
        "\n",
        "$$P_t = Softmax(W_{v_{target}} o_t) \\text{ where }P_t \\in R^{V_{target}}, W_{v_{target}}\\in R^{V_{target} \\times h}$$\n",
        "\n",
        "\n",
        "Here, $V_{target}$ is the size of the target vocabulary. Finally, to train the network we then compute the softmax cross entropy loss between $P_t$ and $g_t$, where $g_t$ is the one-hot vector of the target word at timestep t:\n",
        "\n",
        "$$Loss(Model) = CrossEntropy(P_t, g_t)$$\n",
        "\n",
        "Now that we have described the model, let’s try implementing it for Modern English to Shakespearean English translation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ3IOuMC-sqh"
      },
      "source": [
        "### How do we evaluate NMT models?\n",
        "We can evaluate these models in a few different ways. Recall in lecture that we called these encoder-decoder models \"Conditional Language Models\" since they condition on some prefix before generating text similar to the language models we have seen before. Therefore, we can use **perplexity** to measure the performance of our model.\n",
        "\n",
        "However, perplexity is more of an intrinsic measure and so we'd like to directly measure how closely the model output is to our generated translations. How do we do this? We can look at how well our translation _overlaps_ with the reference translation. A common metric for this is the **BLEU** (Bilingual Evaluation Understudy) metric. The approach works by counting matching n-grams in the candidate translation to n-grams in the reference text, where 1-gram or unigram would be each token and a bigram comparison would be each word pair. The comparison is made regardless of word order. BLEU uses N-grams of size 1-4 in its computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFYJdd9kYhXq"
      },
      "source": [
        "## Part 1: Rules\n",
        "**Part 1** requires implementing an RNN in PyTorch for translation. Countless blog posts, internet tutorials and other implementations available publicly (and privately) do precisely this. In fact, many students in [Cornell NLP](https://nlp.cornell.edu/people/) likely have some code for doing this or something similar on their Github. You **cannot** use any such code (though you may use anything you find in course notes or course texts) irrespective of whether you cite it or not.\n",
        "\n",
        "Submissions will be passed through the MOSS system, which is a sophisticated system for detecting plagiarism in code and is robust in the sense that it tries to find alignments in the underlying semantics of the code and not just the surface level syntax. Similarly, the course staff are also quite astute with respect to programming neural models for NLP and we will strenuously look at your code. We flagged multiple groups for this last year, so we strongly suggest you resist any such temptation (if the Academic Integrity policy alone is insufficient at dissuading you)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS1lQBsWWNLc"
      },
      "source": [
        "## 1.1 RNN Implementation\n",
        "\n",
        "Recall from the previous portion of this assignment as well as the PyTorch tutorial we used a `Data loader` component; we will want to use something similar here as well as a new `NMT` component. We don't envision that it will be useful to copy and modify the previous `Data loader` here. We have included some stubs to help give you a place to start for the NMT.\n",
        "\n",
        "Additionally, we remind you that the previous assignment furnishes a near-functional implementation of a similar neural model (but for a different task). If you successfully completed the FFNN bug fixes , it will be wholly functional. Using it as a guide for Part 1 below is both prudent and suggested.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89rP1JrUyZwZ"
      },
      "source": [
        "### 1.1.1 Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0NUSRffyCYP"
      },
      "outputs": [],
      "source": [
        "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vziuNgbJpYvg"
      },
      "outputs": [],
      "source": [
        "def pad_sents(sents, pad_token):\n",
        "    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
        "        The paddings should be at the end of each sentence.\n",
        "    :param sents: list of sentences, where each sentence\n",
        "                                    is represented as a list of words\n",
        "    :type sents: list[list[str]]\n",
        "    :param pad_token: padding token\n",
        "    :type pad_token: str\n",
        "    :returns sents_padded: list of sentences where sentences shorter\n",
        "        than the max length sentence are padded out with the pad_token, such that\n",
        "        each sentence in the batch now has equal length.\n",
        "    :rtype: list[list[str]]\n",
        "    \"\"\"\n",
        "    sents_padded = []\n",
        "\n",
        "    max_len = max([len(sent) for sent in sents])\n",
        "    sents_padded = [(sent + ([pad_token] * (max_len - len(sent)))) for sent in sents]\n",
        "\n",
        "    return sents_padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T77mk7z8pyJw"
      },
      "outputs": [],
      "source": [
        "def read_corpus(file_path, source):\n",
        "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
        "    :param file_path: path to file containing corpus\n",
        "    :type file_path: str\n",
        "    :param source: \"tgt\" or \"src\" indicating whether text\n",
        "        is of the source language or target language\n",
        "    :type source: str\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for line in open(file_path):\n",
        "        sent = nltk.word_tokenize(line)\n",
        "        # only append <s> and </s> to the target sentence\n",
        "        if source == 'tgt':\n",
        "            sent = ['<s>'] + sent + ['</s>']\n",
        "        data.append(sent)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhtfKCGspU1U"
      },
      "outputs": [],
      "source": [
        "class Vocab(object):\n",
        "    \"\"\" Vocabulary, i.e. structure containing either\n",
        "    src or tgt language terms.\n",
        "    \"\"\"\n",
        "    def __init__(self, word2id=None):\n",
        "        \"\"\" Init Vocab Instance.\n",
        "        \n",
        "        :param word2id: dictionary mapping words 2 indices\n",
        "        :type word2id: dict[str, int]\n",
        "        \"\"\"\n",
        "        if word2id:\n",
        "            self.word2id = word2id\n",
        "        else:\n",
        "            self.word2id = dict()\n",
        "            self.word2id['<pad>'] = 0   # Pad Token\n",
        "            self.word2id['<s>'] = 1     # Start Token\n",
        "            self.word2id['</s>'] = 2    # End Token\n",
        "            self.word2id['<unk>'] = 3   # Unknown Token\n",
        "        self.unk_id = self.word2id['<unk>']\n",
        "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
        "\n",
        "    def __getitem__(self, word):\n",
        "        \"\"\" Retrieve word's index. Return the index for the unk\n",
        "        token if the word is out of vocabulary.\n",
        "        \n",
        "        :param word: word to look up\n",
        "        :type word: str\n",
        "        :returns: index of word\n",
        "        :rtype: int\n",
        "        \"\"\"\n",
        "        return self.word2id.get(word, self.unk_id)\n",
        "\n",
        "    def __contains__(self, word):\n",
        "        \"\"\" Check if word is captured by Vocab.\n",
        "        \n",
        "        :param word: word to look up\n",
        "        :type word: str\n",
        "        :returns: whether word is in vocab\n",
        "        :rtype: bool\n",
        "        \"\"\"\n",
        "        return word in self.word2id\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        \"\"\" Raise error, if one tries to edit the Vocab directly.\n",
        "        \"\"\"\n",
        "        raise ValueError('vocabulary is readonly')\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Compute number of words in Vocab.\n",
        "        \n",
        "        :returns: number of words in Vocab\n",
        "        :rtype: int\n",
        "        \"\"\"\n",
        "        return len(self.word2id)\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\" Representation of Vocab to be used\n",
        "        when printing the object.\n",
        "        \"\"\"\n",
        "        return 'Vocabulary[size=%d]' % len(self)\n",
        "\n",
        "    def id2word(self, wid):\n",
        "        \"\"\" Return mapping of index to word.\n",
        "        \n",
        "        :param wid: word index\n",
        "        :type wid: int\n",
        "        :returns: word corresponding to index\n",
        "        :rtype: str\n",
        "        \"\"\"\n",
        "        return self.id2word[wid]\n",
        "\n",
        "    def add(self, word):\n",
        "        \"\"\" Add word to Vocab, if it is previously unseen.\n",
        "        \n",
        "        :param word: to add to Vocab\n",
        "        :type word: str\n",
        "        :returns: index that the word has been assigned\n",
        "        :rtype: int\n",
        "        \"\"\"\n",
        "        if word not in self:\n",
        "            wid = self.word2id[word] = len(self)\n",
        "            self.id2word[wid] = word\n",
        "            return wid\n",
        "        else:\n",
        "            return self[word]\n",
        "\n",
        "    def words2indices(self, sents):\n",
        "        \"\"\" Convert list of words or list of sentences of words\n",
        "        into list or list of list of indices.\n",
        "        \n",
        "        :param sents: sentence(s) in words\n",
        "        :type sents: Union[List[str], List[List[str]]]\n",
        "        :returns: sentence(s) in indices\n",
        "        :rtype: Union[List[int], List[List[int]]]\n",
        "        \"\"\"\n",
        "        if type(sents[0]) == list:\n",
        "            return [[self[w] for w in s] for s in sents]\n",
        "        else:\n",
        "            return [self[w] for w in sents]\n",
        "\n",
        "    def indices2words(self, word_ids):\n",
        "        \"\"\" Convert list of indices into words.\n",
        "        \n",
        "        :param word_ids: list of word ids\n",
        "        :type word_ids: List[int]\n",
        "        :returns: list of words\n",
        "        :rtype: List[Str]\n",
        "        \"\"\"\n",
        "        return [self.id2word[w_id] for w_id in word_ids]\n",
        "\n",
        "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
        "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
        "        shorter sentences.\n",
        "        \n",
        "        :param sents: list of sentences (words)\n",
        "        :type sents: List[List[str]]\n",
        "        :param device: Device on which to load the tensor, ie. CPU or GPU\n",
        "        :type device: torch.device\n",
        "        :returns: Sentence tensor of (max_sentence_length, batch_size)\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "\n",
        "        word_ids = self.words2indices(sents)\n",
        "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
        "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
        "        return torch.t(sents_var)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_corpus(corpus, size, freq_cutoff=2):\n",
        "        \"\"\" Given a corpus construct a Vocab.\n",
        "        \n",
        "        :param corpus: corpus of text produced by read_corpus function\n",
        "        :type corpus: List[str]\n",
        "        :param size: # of words in vocabulary\n",
        "        :type size: int\n",
        "        :param freq_cutoff: if word occurs n < freq_cutoff times, drop the word\n",
        "        :type freq_cutoff: int\n",
        "        :returns: Vocab instance produced from provided corpus\n",
        "        :rtype: Vocab\n",
        "        \"\"\"\n",
        "        vocab_entry = Vocab()\n",
        "        word_freq = Counter(chain(*corpus))\n",
        "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
        "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
        "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
        "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
        "        for word in top_k_words:\n",
        "            vocab_entry.add(word)\n",
        "        return vocab_entry\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_subword_list(subword_list):\n",
        "        \"\"\"Given a list of subwords, construct the Vocab.\n",
        "        \n",
        "        :param subword_list: list of subwords in corpus\n",
        "        :type subword_list: List[str]\n",
        "        :returns: Vocab instance produced from provided list\n",
        "        :rtype: Vocab\n",
        "        \"\"\"\n",
        "        vocab_entry = Vocab()\n",
        "        for subword in subword_list:\n",
        "            vocab_entry.add(subword)\n",
        "        return vocab_entry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2CihDrhyMI_",
        "outputId": "b7bec703-76ac-4f93-cd20-5c131cef5798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initialize source vocabulary ..\n",
            "number of word types: 13252, number of word types w/ frequency >= 2: 9167\n",
            "initialize target vocabulary ..\n",
            "number of word types: 15216, number of word types w/ frequency >= 2: 10725\n"
          ]
        }
      ],
      "source": [
        "print('initialize source vocabulary ..')\n",
        "src_sents = read_corpus(source_path, \"src\")\n",
        "src = Vocab.from_corpus(src_sents, 20000, 2) # 7098, 9422\n",
        "\n",
        "print('initialize target vocabulary ..')\n",
        "tgt_sents = read_corpus(target_path, \"tgt\")\n",
        "tgt = Vocab.from_corpus(tgt_sents, 20000, 2) # 6893, 10956"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHkmbdw1zZXB"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE\n",
        "# Train embeddings or load embeddings\n",
        "# or use other feature representation for words (e.g 1 hot encoding)\n",
        "#\n",
        "# We want a numpy array that has shape |V| x |embedding size| that can potentially\n",
        "# be passed into our NMT model for our pretrained_source / pretrained_target\n",
        "# arguments. This allows our model to start off with a good starting point and\n",
        "# we can decide whether to keep our embeddings static or update them as we go.\n",
        "#\n",
        "# Some ideas as to what to do here are using pre-trained word embeddings from gensim\n",
        "# >>> import gensim.downloader as api\n",
        "# >>> model = api.load(\"glove-wiki-gigaword-300\")  # load glove vectors\n",
        "# >>> model.wv['chicken'] # Get word vector for chicken\n",
        "#\n",
        "# OR potentially train your own new embeddings using the SkipGram algorithm discussed in lecture.\n",
        "# >>> model = gensim.models.Word2Vec(sentences, min_count=1, vector_size=300, sg=1, negative=5)\n",
        "# Tutorial: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
        "#\n",
        "# OR explicitly choose to do nothing here and the embeddings are learned end-to-end during training in the NMT class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdkOaqC-uPJc"
      },
      "outputs": [],
      "source": [
        "# Some ideas as to what to do here are using pre-trained word embeddings from gensim\n",
        "# import gensim.downloader as api\n",
        "# model = api.load(\"glove-wiki-gigaword-300\")  # load glove vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j4-_sjmfQzej"
      },
      "outputs": [],
      "source": [
        "# train your own new embeddings using the SkipGram algorithm\n",
        "# model_src = gensim.models.Word2Vec(src_sents, min_count=1, vector_size=256, sg=1, negative=5)\n",
        "# model_tgt = gensim.models.Word2Vec(tgt_sents, min_count=1, vector_size=256, sg=1, negative=5)\n",
        "model_src = gensim.models.Word2Vec(src_sents+tgt_sents,min_count=1, vector_size=256, sg=1, negative=5)\n",
        "src_embed_vectors = torch.zeros([len(src), len(model_src.wv['i'])])\n",
        "tgt_embed_vectors = torch.zeros([len(tgt), len(model_src.wv['i'])])\n",
        "\n",
        "for i in range(0, len(src)):\n",
        "  word = list(src.word2id.keys())[i]\n",
        "  try:\n",
        "    emb_vect  = model_src.wv[word]\n",
        "    src_embed_vectors[list(src.word2id.values())[i], :] = torch.from_numpy(emb_vect)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "for j in range(0, len(tgt)):\n",
        "  word = list(tgt.word2id.keys())[j]\n",
        "  try:\n",
        "    emb_vect  = model_tgt.wv[word]\n",
        "    tgt_embed_vectors[j, :] = torch.from_numpy(emb_vect)\n",
        "  except:\n",
        "    continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nQmyoO0FzzmX"
      },
      "outputs": [],
      "source": [
        "# Split into training and validation data\n",
        "train_data_src, val_data_src, train_data_tgt, val_data_tgt = train_test_split(src_sents, tgt_sents, test_size=0.045922, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f0CcUGWHz7WE"
      },
      "outputs": [],
      "source": [
        "train_data = list(zip(train_data_src, train_data_tgt))\n",
        "val_data = list(zip(val_data_src, val_data_tgt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjWp9yhmyRWR"
      },
      "source": [
        "### 1.1.2 NMT Model Implementation\n",
        "\n",
        "For the implementation below, we have given a framework / skeleton for your code. Within the skeleton are sections that define where you should place your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xnwncrLnb6kx"
      },
      "outputs": [],
      "source": [
        "def generate_sent_masks(enc_hiddens: torch.Tensor, source_lengths: List[int], device: torch.device) -> torch.Tensor:\n",
        "    \"\"\" Generate sentence masks for encoder hidden states.\n",
        "\n",
        "    :param enc_hiddens: encodings of shape (b, src_len, 2*h), where b = batch size,\n",
        "        src_len = max source length, h = hidden size.\n",
        "    :type enc_hiddens: torch.Tensor\n",
        "    :param source_lengths: List of actual lengths for each of the sentences in the batch.   \n",
        "    :type source_lengths: List[int]\n",
        "    :param device: Device on which to load the tensor, ie. CPU or GPU\n",
        "    :type device: torch.device\n",
        "    :returns: Tensor of sentence masks of shape (b, src_len),\n",
        "        where src_len = max source length, h = hidden size.\n",
        "    :rtype: torch.Tensor\n",
        "    \"\"\"\n",
        "    enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n",
        "    for e_id, src_len in enumerate(source_lengths):\n",
        "        enc_masks[e_id, src_len:] = 1\n",
        "    return enc_masks.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EApec5FFcNsY"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, source_embeddings, enc_type):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_size = embed_size\n",
        "        self.embedding = source_embeddings\n",
        "        self.enc_type = enc_type\n",
        "        ### YOUR CODE HERE (~2 Lines)\n",
        "        ### TODO - Initialize the following variables:\n",
        "        if(self.enc_type == 'LSTM'):\n",
        "          self.encoder = torch.nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size, bias = True, bidirectional=True) #  (Bidirectional RNN with bias) \n",
        "          self.c_projection = nn.Linear(in_features = 2*self.hidden_size, out_features = self.hidden_size, bias = False)\n",
        "        if(self.enc_type == 'RNN'):\n",
        "          self.encoder = torch.nn.RNN(input_size=self.embed_size, hidden_size=self.hidden_size, bias = True, bidirectional=True) #  (Bidirectional RNN with bias) \n",
        "        self.h_projection = nn.Linear(in_features = 2*self.hidden_size, out_features = self.hidden_size, bias = False) # (Linear Layer with no bias), called W_{h} above.\n",
        "        \n",
        "        ###\n",
        "        ### Note that you are free to use any architecture (vanilla RNN, LSTM, GRU)\n",
        "        ### that you would like. Additionally, you are free to use any hyperparameters\n",
        "        ### that you would like (e.g. number of layers). You will discuss your choice\n",
        "        ### of hyperparameters in the write up later as well.\n",
        "        ###\n",
        "        ### Use the following docs to properly initialize these variables:\n",
        "        ###     RNN:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.RNN\n",
        "        ###     LSTM:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
        "        ###     Linear Layer:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
        "\n",
        "        \n",
        "    \n",
        "    def forward(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        enc_hiddens, dec_init_state = None, None\n",
        "\n",
        "        ### YOUR CODE HERE (~ 8 Lines)\n",
        "        ### TODO:\n",
        "        ###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.\n",
        "        X = self.embedding(source_padded)\n",
        "        # print('X.size()',X.size())\n",
        "        ###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note\n",
        "        ###         that there is no initial hidden state or cell for the encoder.\n",
        "        ###     2. Compute `enc_hiddens`, `last_hidden` by applying the encoder to `X`.\n",
        "        ###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.\n",
        "        packed = pack_padded_sequence(X, source_lengths) # \n",
        "      \n",
        "        enc_hiddens, last_hidden = self.encoder(packed) #  \n",
        "        \n",
        "        ###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.\n",
        "        pad, _ = pad_packed_sequence(enc_hiddens,batch_first=False)\n",
        "        ###         - Note that the shape of the tensor returned by the encoder is (src_len, b, h*2) and we want to\n",
        "        ###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.\n",
        "        # print('pad.size()', pad.size())\n",
        "        \n",
        "        enc_hiddens = torch.permute(pad, (1, 0, 2))\n",
        "        # print(enc_hiddens.size())\n",
        "        \n",
        "        # torch.permute(x, (2, 0, 1)).size()\n",
        "        ###     3. Compute `dec_init_state` = init_decoder_hidden:\n",
        "        ###         - `init_decoder_hidden`:\n",
        "        ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forward and backwards.\n",
        "        ###             Concatenate the forward and backward tensors to obtain a tensor shape (b, 2*h).\n",
        "        ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.\n",
        "        ###             This is h_0^{dec} in above in the writeup. Here b = batch size, h = hidden size\n",
        "        ###\n",
        "        \n",
        "        if(self.enc_type == 'LSTM'):\n",
        "          # self.c_projection = nn.Linear(in_features = 2*self.hidden_size, out_features = self.hidden_size, bias = False)\n",
        "          last_hidden_hid = last_hidden[0]\n",
        "          last_cell = last_hidden[1]\n",
        "          last_hidden_interm = torch.cat((last_hidden_hid[0], last_hidden_hid[1]), 1)\n",
        "          \n",
        "          last_cell_interm = torch.cat((last_cell[0], last_cell[1]), 1)\n",
        "          init_decoder_hidden = self.h_projection(last_hidden_interm)\n",
        "          init_decoder_cell = self.c_projection(last_cell_interm)\n",
        "\n",
        "          dec_init_state_hidden = init_decoder_hidden\n",
        "          dec_init_state_cell = init_decoder_cell\n",
        "          dec_init_state = (dec_init_state_hidden, dec_init_state_cell)\n",
        "       \n",
        "\n",
        "        \n",
        "          dec_init_state = (dec_init_state_hidden, dec_init_state_cell)\n",
        "       \n",
        "        if(self.enc_type == 'RNN'):\n",
        "          last_hidden_interm = torch.cat((last_hidden[0], last_hidden[1]), 1)\n",
        "          init_decoder_hidden = self.h_projection(last_hidden_interm)\n",
        "          dec_init_state_hidden = init_decoder_hidden\n",
        "          dec_init_state = dec_init_state_hidden\n",
        "        ### See the following docs, as you may need to use some of the following functions in your implementation:\n",
        "        ###     Pack the padded sequence X before passing to the encoder:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n",
        "        ###     Pad the packed sequence, enc_hiddens, returned by the encoder:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence\n",
        "        ###     Tensor Concatenation:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
        "        ###     Tensor Permute:\n",
        "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute\n",
        "        \n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        return enc_hiddens, dec_init_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zj-HTuLCcWum"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, target_embedding, device, dec_type):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        self.embedding = target_embedding\n",
        "        output_vocab_size = self.embedding.weight.size(0)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.dec_type = dec_type\n",
        "\n",
        "        ### YOUR CODE HERE (~3 lines)\n",
        "        if(self.dec_type == 'LSTM'):\n",
        "          self.decoder = torch.nn.LSTMCell(input_size=self.embed_size+self.hidden_size, hidden_size=self.hidden_size, bias=True, device=None, dtype=None)  # (RNN Cell with bias)\n",
        "        if(self.dec_type == 'RNN'):\n",
        "          self.decoder = torch.nn.RNNCell(input_size=self.embed_size+self.hidden_size, hidden_size=self.hidden_size, bias=True, nonlinearity='tanh', device=None, dtype=None)  # (RNN Cell with bias)\n",
        "        \n",
        "        self.combined_output_projection = nn.Linear(in_features = self.hidden_size, out_features = self.hidden_size, bias = False) # (Linear Layer with no bias), called W_{h} above.\n",
        "        self.target_vocab_projection = nn.Linear(in_features = self.hidden_size, out_features = output_vocab_size)\n",
        "        ###     self.combined_output_projection (Linear Layer with no bias), called W_{v} above.\n",
        "        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{vocab} above.\n",
        "        ###\n",
        "        ### Use the following docs to properly initialize these variables:\n",
        "        ###     RNN Cell:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell\n",
        "        ###     LSTM Cell:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell\n",
        "        ###     Linear Layer:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
        "\n",
        " \n",
        "\n",
        "    \n",
        "    def forward(self, enc_hiddens: torch.Tensor,\n",
        "                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        # Chop of the <END> token for max length sentences.\n",
        "        target_padded = target_padded[:-1]\n",
        "\n",
        "        dec_state = dec_init_state\n",
        "\n",
        "        # Initialize previous combined output vector o_{t-1} as zero\n",
        "        # print('enc_hidden_size', enc_hiddens.size())\n",
        "        batch_size = enc_hiddens.size(0)\n",
        "        # print('batch_size', batch_size)\n",
        "        # print('hidden_size', hidden_size)\n",
        "        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "        # Initialize a list we will use to collect the combined output o_t on each step\n",
        "        combined_outputs = []\n",
        "\n",
        "        ### YOUR CODE HERE (~9 Lines)\n",
        "        ### TODO:\n",
        "        ###     1. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.\n",
        "        ###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.\n",
        "        ###     2. Use the torch.split function to iterate over the time dimension of Y.\n",
        " \n",
        "        ###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.\n",
        "        ###             - Squeeze Y_t into a tensor of dimension (b, e). \n",
        "        ###             - Construct Ybar_t by concatenating Y_t with o_prev on their last dimension\n",
        "        ###             - Use the step function to compute the the Decoder's next (cell, state) values\n",
        "        ###               as well as the new combined output o_t.\n",
        "        ###             - Append o_t to combined_outputs\n",
        "        ###             - Update o_prev to the new o_t.\n",
        "        ###     3. Use torch.stack to convert combined_outputs from a list length tgt_len of\n",
        "        ###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)\n",
        "        ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.\n",
        "        ###\n",
        "        ### Note:\n",
        "        ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze\n",
        "        ###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n",
        "        ###   \n",
        "        ### You may find some of these functions useful:\n",
        "        ###     Zeros Tensor:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.zeros\n",
        "        ###     Tensor Splitting (iteration):\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.split\n",
        "        ###     Tensor Dimension Squeezing:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n",
        "        ###     Tensor Concatenation:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
        "        ###     Tensor Stacking:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.stack\n",
        "        \n",
        "        Y = self.embedding(target_padded)\n",
        "       # print(\"target_padded\", target_padded.size())\n",
        "       # print(\"Y.size\", Y.size())\n",
        "\n",
        "        for Y_t in torch.split(Y, 1):\n",
        "         # print(\"Y_t\", Y_t.size())\n",
        "          Y_t = torch.squeeze(Y_t, 0) # (1,1,3)\n",
        "         \n",
        "          Ybar_t = torch.cat((Y_t, o_prev), 1)\n",
        "          dec_state, o_t = self.step(Ybar_t, dec_state, enc_hiddens)\n",
        "          combined_outputs.append(o_t)\n",
        "          o_prev = o_t\n",
        "        combined_outputs = torch.stack(combined_outputs)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        return combined_outputs\n",
        "    \n",
        "    def step(self, Ybar_t: torch.Tensor,\n",
        "            dec_state: Tuple[torch.Tensor, torch.Tensor],\n",
        "            enc_hiddens: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n",
        "\n",
        "        :param Ybar_t: Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n",
        "                                where b = batch size, e = embedding size, h = hidden size.\n",
        "        :type Ybar_t: torch.Tensor\n",
        "        :param dec_state: Tensors with shape (b, h), where b = batch size, h = hidden size.\n",
        "                Tensor is decoder's prev hidden state\n",
        "        :type dec_state: torch.Tensor\n",
        "        :param enc_hiddens: Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n",
        "                                    src_len = maximum source length, h = hidden size.\n",
        "        :type enc_hiddens: torch.Tensor\n",
        "\n",
        "        :returns dec_state: Tensors with shape (b, h), where b = batch size, h = hidden size.\n",
        "                Tensor is decoder's new hidden state. For an LSTM, this should be a tuple\n",
        "                of the hidden state and cell state.\n",
        "        returns combined_output: Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n",
        "        \"\"\"\n",
        "\n",
        "        combined_output = None\n",
        "\n",
        "        ### YOUR CODE HERE (~2 Lines)\n",
        "        ### TODO:\n",
        "        ###     1. Apply the decoder to `Ybar_t` and `dec_state` to obtain the new dec_state.\n",
        "        ###     2. Rename dec_state to dec_hidden\n",
        "        ###\n",
        "        ###       Hints:\n",
        "        ###         - dec_hidden is shape (b, h) and corresponds to h^dec_t above\n",
        "        ###\n",
        "       \n",
        "        ### END YOUR CODE\n",
        "        # Ybar_t_dec_state = torch.cat((Ybar_t, dec_state), 1)\n",
        "        dec_hidden = self.decoder(Ybar_t, dec_state)\n",
        "        \n",
        "        ### YOUR CODE HERE (~2 Lines)\n",
        "        ### TODO:\n",
        "        ###     1. Apply the combined output projection layer to h^dec_t to compute tensor V_t\n",
        "        ###     2. Compute tensor O_t by applying the Tanh function.\n",
        "        ###\n",
        "        if(self.dec_type == 'LSTM'):\n",
        "          V_t = self.combined_output_projection(dec_hidden[0])\n",
        "          # (RNN Cell with bias)\n",
        "        if(self.dec_type == 'RNN'):\n",
        "          V_t = self.combined_output_projection(dec_hidden)        \n",
        "        O_t = self.activation(V_t)\n",
        "\n",
        "        ### Use the following docs to implement this functionality:\n",
        "        ###     Softmax:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax\n",
        "        ###     Batch Multiplication:\n",
        "        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
        "        ###     Tensor View:\n",
        "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
        "        ###     Tensor Concatenation:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
        "        ###     Tanh:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.tanh\n",
        "\n",
        "\n",
        "        ### END YOUR CODE\n",
        "        dec_state = dec_hidden\n",
        "        combined_output = O_t\n",
        "        return dec_state, combined_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HtWqP3WNcYH3"
      },
      "outputs": [],
      "source": [
        "class NMT(nn.Module):\n",
        "    \"\"\" Simple Neural Machine Translation Model:\n",
        "        - Bidrectional RNN Encoder\n",
        "        - Unidirection RNN Decoder\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, hidden_size, src_vocab, tgt_vocab, device=torch.device(\"cpu\"), pretrained_source=None,pretrained_target=None,LSTM_RNN='RNN',):\n",
        "        \"\"\" Init NMT Model.\n",
        "\n",
        "        :param embed_size: Embedding size (dimensionality)\n",
        "        :type embed_size: int\n",
        "        :param hidden_size: Hidden Size, the size of hidden states (dimensionality)\n",
        "        :type hidden_size: int\n",
        "        :param src_vocab: Vocabulary object containing src language\n",
        "        :type src_vocab: Vocab\n",
        "        :param tgt_vocab: Vocabulary object containing tgt language\n",
        "        :type tgt_vocab: Vocab\n",
        "        :param device: torch device to put all modules on\n",
        "        :type device: torch.device\n",
        "        :param pretrained_source: Matrix of pre-trained source word embeddings\n",
        "        :type pretrained_source: Optional[torch.Tensor]\n",
        "        :param pretrained_target: Matrix of pre-trained target word embeddings\n",
        "        :type pretrained_target: Optional[torch.Tensor]\n",
        "        \"\"\"\n",
        "        super(NMT, self).__init__()\n",
        "        self.device=device\n",
        "        self.embed_size = embed_size\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        src_pad_token_idx = src_vocab['<pad>']\n",
        "        tgt_pad_token_idx = tgt_vocab['<pad>']\n",
        "        self.source_embedding = nn.Embedding(len(src_vocab), embed_size, padding_idx=src_pad_token_idx)\n",
        "        self.target_embedding = nn.Embedding(len(tgt_vocab), embed_size, padding_idx=tgt_pad_token_idx)\n",
        "        self.LSTM_RNN = LSTM_RNN\n",
        "        with torch.no_grad():\n",
        "            if pretrained_source is not None:\n",
        "                self.source_embedding.weight.data = pretrained_source\n",
        "                # TODO: Decide if we want the embeddings to update as we train\n",
        "                self.source_embedding.weight.requires_grad = True #False\n",
        "        \n",
        "            if pretrained_target is not None:\n",
        "                self.target_embedding.weight.data = pretrained_target\n",
        "                # TODO: Decide if we want the embeddings to update as we train\n",
        "                self.target_embedding.weight.requires_grad = True # False\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            embed_size=embed_size,\n",
        "            hidden_size=hidden_size,\n",
        "            source_embeddings=self.source_embedding,\n",
        "            enc_type = self.LSTM_RNN,\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            embed_size=embed_size,\n",
        "            hidden_size=hidden_size,\n",
        "            target_embedding=self.target_embedding,\n",
        "            device=self.device,\n",
        "            dec_type = self.LSTM_RNN,\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n",
        "        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n",
        "        target sentences under the language models learned by the NMT system.\n",
        "\n",
        "        :param source: list of source sentence tokens\n",
        "        :type source: List[List[str]]\n",
        "        :param target: list of target sentence tokens, wrapped by `<s>` and `</s>`\n",
        "        :type target: List[List[str]]\n",
        "        :returns scores: a variable/tensor of shape (b, ) representing the\n",
        "                                    log-likelihood of generating the gold-standard target sentence for\n",
        "                                    each example in the input batch. Here b = batch size.\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        # Compute sentence lengths\n",
        "        source_lengths = [len(s) for s in source]\n",
        "\n",
        "        # Convert list of lists into tensors\n",
        "        source_padded = self.src_vocab.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n",
        "        target_padded = self.tgt_vocab.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n",
        "        \n",
        "        ###     Run the network forward:\n",
        "        ###     1. Apply the encoder to `source_padded` by calling `self.encode()`\n",
        "        ###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`\n",
        "        ###     3. Apply the decoder to compute combined-output by calling `self.decode()`\n",
        "        ###     4. Compute log probability distribution over the target vocabulary using the\n",
        "        ###        combined_outputs returned by the `self.decode()` function.\n",
        "\n",
        "        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n",
        "        enc_masks = generate_sent_masks(enc_hiddens, source_lengths, self.device)\n",
        "        combined_outputs = self.decode(enc_hiddens, dec_init_state, target_padded)\n",
        "        P = F.log_softmax(self.decoder.target_vocab_projection(combined_outputs), dim=-1)\n",
        "\n",
        "        # Zero out, probabilities for which we have nothing in the target text\n",
        "        target_masks = (target_padded != self.tgt_vocab['<pad>']).float()\n",
        "        \n",
        "        # Compute log probability of generating true target words\n",
        "        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n",
        "        scores = target_gold_words_log_prob.sum(dim=0)\n",
        "        return scores\n",
        "\n",
        "\n",
        "    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n",
        "            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
        "\n",
        "        :param source_padded: Tensor of padded source sentences with shape (src_len, b), where\n",
        "            b = batch_size, src_len = maximum source sentence length. Note that these have\n",
        "            already been sorted in order of longest to shortest sentence.\n",
        "        :type source_padded: torch.Tensor\n",
        "        :param source_lengths: List of actual lengths for each of the source sentences in the batch\n",
        "        :type source_lengths: List[int]\n",
        "        :returns: Tuple of two items. The first is Tensor of hidden units with shape (b, src_len, h*2),\n",
        "            where b = batch size, src_len = maximum source sentence length, h = hidden size. The second is\n",
        "            Tuple of tensors representing the decoder's initial hidden state and cell.\n",
        "        :rtype: Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n",
        "        \"\"\"\n",
        "        return self.encoder(source_padded, source_lengths)\n",
        "\n",
        "\n",
        "    def decode(self, enc_hiddens: torch.Tensor,\n",
        "                dec_init_state: torch.Tensor, target_padded: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute combined output vectors for a batch.\n",
        "\n",
        "        :param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n",
        "                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
        "        :param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n",
        "        :param target_padded: Gold-standard padded target sentences (tgt_len, b), where\n",
        "                                       tgt_len = maximum target sentence length, b = batch size. \n",
        "\n",
        "        :returns combined_outputs: combined output tensor  (tgt_len, b,  h), where\n",
        "                                    tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        return self.decoder(enc_hiddens, dec_init_state, target_padded)\n",
        "\n",
        "    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n",
        "        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n",
        "        :param src_sent: a single source sentence (words)\n",
        "        :type src_sent: List[str]\n",
        "        :param beam_size: beam size\n",
        "        :type beam_size: int\n",
        "        :param max_decoding_time_step: maximum number of time steps to unroll the decoding RNN\n",
        "        :type max_decoding_time_step: int\n",
        "        :returns hypotheses: a list of hypothesis, each hypothesis has two fields:\n",
        "                value: List[str]: the decoded target sentence, represented as a list of words\n",
        "                score: float: the log-likelihood of the target sentence\n",
        "        :rtype: List[Hypothesis]\n",
        "        \"\"\"\n",
        "        src_sents_var = self.src_vocab.to_input_tensor([src_sent], self.device)\n",
        "\n",
        "        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n",
        "\n",
        "        h_tm1 = dec_init_vec\n",
        "        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n",
        "\n",
        "        eos_id = self.tgt_vocab['</s>']\n",
        "\n",
        "        hypotheses = [['<s>']]\n",
        "        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n",
        "        completed_hypotheses = []\n",
        "\n",
        "        t = 0\n",
        "        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n",
        "            t += 1\n",
        "            hyp_num = len(hypotheses)\n",
        "\n",
        "            exp_src_encodings = src_encodings.expand(hyp_num,\n",
        "                                                     src_encodings.size(1),\n",
        "                                                     src_encodings.size(2))\n",
        "\n",
        "            y_tm1 = torch.tensor([self.tgt_vocab[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n",
        "            y_t_embed = self.target_embedding(y_tm1)\n",
        "\n",
        "            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n",
        "\n",
        "            h_t, att_t = self.decoder.step(x, h_tm1,\n",
        "                                exp_src_encodings)\n",
        "            \n",
        "            ## TODO: Uncomment the line below if this is an LSTM\n",
        "            if(self.LSTM_RNN == 'LSTM'):\n",
        "              h_t, c_t = h_t\n",
        "\n",
        "            # log probabilities over target words\n",
        "            log_p_t = F.log_softmax(self.decoder.target_vocab_projection(att_t), dim=-1)\n",
        "\n",
        "            live_hyp_num = beam_size - len(completed_hypotheses)\n",
        "            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n",
        "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n",
        "\n",
        "            prev_hyp_ids = torch.div(top_cand_hyp_pos, len(self.tgt_vocab), rounding_mode='floor')\n",
        "            hyp_word_ids = top_cand_hyp_pos % len(self.tgt_vocab)\n",
        "\n",
        "            new_hypotheses = []\n",
        "            live_hyp_ids = []\n",
        "            new_hyp_scores = []\n",
        "\n",
        "            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n",
        "                prev_hyp_id = prev_hyp_id.item()\n",
        "                hyp_word_id = hyp_word_id.item()\n",
        "                cand_new_hyp_score = cand_new_hyp_score.item()\n",
        "\n",
        "                hyp_word = self.tgt_vocab.id2word[hyp_word_id]\n",
        "                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n",
        "                if hyp_word == '</s>':\n",
        "                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n",
        "                                                           score=cand_new_hyp_score))\n",
        "                else:\n",
        "                    new_hypotheses.append(new_hyp_sent)\n",
        "                    live_hyp_ids.append(prev_hyp_id)\n",
        "                    new_hyp_scores.append(cand_new_hyp_score)\n",
        "\n",
        "            if len(completed_hypotheses) == beam_size:\n",
        "                break\n",
        "\n",
        "            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n",
        "            if(self.LSTM_RNN == 'RNN'): \n",
        "              h_tm1 = h_t[live_hyp_ids]\n",
        "            ### TODO: Uncomment the below if it is an LSTM and comment out line\n",
        "            # above. Otherwise leave.\n",
        "            if(self.LSTM_RNN == 'LSTM'):\n",
        "              h_tm1 = h_t[live_hyp_ids], c_t[live_hyp_ids]\n",
        "            att_tm1 = att_t[live_hyp_ids]\n",
        "\n",
        "            hypotheses = new_hypotheses\n",
        "            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n",
        "\n",
        "        if len(completed_hypotheses) == 0:\n",
        "            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n",
        "                                                   score=hyp_scores[0].item()))\n",
        "\n",
        "        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n",
        "\n",
        "        return completed_hypotheses\n",
        "\n",
        "\n",
        "    def greedy(self, src_sent: List[str], max_decoding_time_step: int=70) -> List[Hypothesis]:\n",
        "        return self.beam_search(src_sent, beam_size=1, max_decoding_time_step=max_decoding_time_step)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def load(model_path: str):\n",
        "        \"\"\" Load the model from a file.\n",
        "        @param model_path (str): path to model\n",
        "        \"\"\"\n",
        "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "        args = params['args']\n",
        "        model = NMT(\n",
        "            src_vocab=params['vocab']['source'],\n",
        "            tgt_vocab=params['vocab']['target'],\n",
        "            **args\n",
        "        )\n",
        "        model.load_state_dict(params['state_dict'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\" Save the model to a file.\n",
        "        @param path (str): path to the model\n",
        "        \"\"\"\n",
        "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
        "\n",
        "        params = {\n",
        "            'args': dict(embed_size=self.embed_size, hidden_size=self.hidden_size),\n",
        "            'vocab': dict(source=self.src_vocab, target=self.tgt_vocab),\n",
        "            'state_dict': self.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(params, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GNAHNVKnniOi"
      },
      "outputs": [],
      "source": [
        "def batch_iter(data, batch_size, shuffle=False):\n",
        "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
        "    :param data: list of tuples containing source and target sentence. ie.\n",
        "        (list of (src_sent, tgt_sent))\n",
        "    :type data: List[Tuple[List[str], List[str]]]\n",
        "    :param batch_size: batch size\n",
        "    :type batch_size: int\n",
        "    :param shuffle: whether to randomly shuffle the dataset\n",
        "    :type shuffle: boolean\n",
        "    \"\"\"\n",
        "    batch_num = math.ceil(len(data) / batch_size)\n",
        "    index_array = list(range(len(data)))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(index_array)\n",
        "\n",
        "    for i in range(batch_num):\n",
        "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
        "        examples = [data[idx] for idx in indices]\n",
        "\n",
        "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
        "        src_sents = [e[0] for e in examples]\n",
        "        tgt_sents = [e[1] for e in examples]\n",
        "\n",
        "        yield src_sents, tgt_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wGIm-Vo8mWot"
      },
      "outputs": [],
      "source": [
        "def evaluate_ppl(model, val_data, batch_size=32):\n",
        "    \"\"\" Evaluate perplexity on dev sentences\n",
        "    :param model: NMT Model\n",
        "    :type model: NMT\n",
        "    :param dev_data: list of tuples containing source and target sentence.\n",
        "        i.e. (list of (src_sent, tgt_sent))\n",
        "    :param val_data: List[Tuple[List[str], List[str]]]\n",
        "    :param batch_size: size of batches to extract\n",
        "    :type batch_size: int\n",
        "    :returns ppl: perplexity on val sentences\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    cum_loss = 0.\n",
        "    cum_tgt_words = 0.\n",
        "\n",
        "    # no_grad() signals backend to throw away all gradients\n",
        "    with torch.no_grad():\n",
        "        for src_sents, tgt_sents in batch_iter(val_data, batch_size):\n",
        "            loss = -model(src_sents, tgt_sents).sum()\n",
        "\n",
        "            cum_loss += loss.item()\n",
        "            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "            cum_tgt_words += tgt_word_num_to_predict\n",
        "\n",
        "        ppl = np.exp(cum_loss / cum_tgt_words)\n",
        "        avg_val_loss = cum_loss/len(val_data)\n",
        "\n",
        "        wandb.log({\n",
        "            \"avg. val loss\": avg_val_loss,\n",
        "            \"avg. val perplexity\": ppl\n",
        "        })\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n",
        "    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n",
        "    :param references: a list of gold-standard reference target sentences\n",
        "    :type references: List[List[str]]\n",
        "    :param hypotheses: a list of hypotheses, one for each reference\n",
        "    :type hypotheses: List[Hypothesis]\n",
        "    :returns bleu_score: corpus-level BLEU score\n",
        "    \"\"\"\n",
        "    if references[0][0] == '<s>':\n",
        "        references = [ref[1:-1] for ref in references]\n",
        "    bleu_score = corpus_bleu([[ref] for ref in references],\n",
        "                             [hyp.value for hyp in hypotheses])\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def evaluate_bleu(references, model, source):\n",
        "    \"\"\"Generate decoding results and compute BLEU score.\n",
        "    :param model: NMT Model\n",
        "    :type model: NMT\n",
        "    :param references: a list of gold-standard reference target sentences\n",
        "    :type references: List[List[str]]\n",
        "    :param source: a list of source sentences\n",
        "    :type source: List[List[str]]\n",
        "    :returns bleu_score: corpus-level BLEU score\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        top_hypotheses = []\n",
        "        for s in tqdm(source, leave=False):\n",
        "            hyps = model.beam_search(s, beam_size=16, max_decoding_time_step=(len(s)+10))\n",
        "            top_hypotheses.append(hyps[0])\n",
        "    \n",
        "    s1 = compute_corpus_level_bleu_score(references, top_hypotheses)\n",
        "    \n",
        "    return s1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eSq1z1L7lumv"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(model, train_data, val_data, optimizer, epochs=10, train_batch_size=32, clip_grad=2, log_every = 100, valid_niter = 500, model_save_path=\"NMT_model.ckpt\"):\n",
        "    num_trail = 0\n",
        "    cum_examples = report_examples = epoch = valid_num = 0\n",
        "    hist_valid_scores = []\n",
        "    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n",
        "\n",
        "    print('Begin Maximum Likelihood training')\n",
        "    train_time = begin_time = time.time()\n",
        "\n",
        "    val_data_tgt = [tgt for _, tgt in val_data]\n",
        "    val_data_src = [src for src, _ in val_data]\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        wandb.log({\"epoch\": epoch})\n",
        "        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n",
        "            train_iter += 1\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            batch_size = len(src_sents)\n",
        "            \n",
        "            example_losses = -model(src_sents, tgt_sents)\n",
        "            batch_loss = example_losses.sum()\n",
        "            loss = batch_loss / batch_size\n",
        "            loss.backward()\n",
        "            \n",
        "            # clip gradient\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            batch_losses_val = batch_loss.item()\n",
        "            report_loss += batch_losses_val\n",
        "            cum_loss += batch_losses_val\n",
        "            \n",
        "            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "            report_tgt_words += tgt_words_num_to_predict\n",
        "            cum_tgt_words += tgt_words_num_to_predict\n",
        "            report_examples += batch_size\n",
        "            cum_examples += batch_size\n",
        "\n",
        "            if train_iter % log_every == 0:\n",
        "                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n",
        "                        'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n",
        "                                                                                            report_loss / report_examples,\n",
        "                                                                                            math.exp(report_loss / report_tgt_words),\n",
        "                                                                                            cum_examples,\n",
        "                                                                                            report_tgt_words / (time.time() - train_time),\n",
        "                                                                                            time.time() - begin_time))\n",
        "\n",
        "                wandb.log({\n",
        "                    \"avg. train loss\": (report_loss/report_examples),\n",
        "                    \"avg. train perplexity\": math.exp(report_loss / report_tgt_words)\n",
        "                })                                                                            \n",
        "                train_time = time.time()\n",
        "                report_loss = report_tgt_words = report_examples = 0.\n",
        "\n",
        "                \n",
        "\n",
        "            # perform validation\n",
        "            if train_iter % valid_niter == 0:\n",
        "                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n",
        "                                                                                            cum_loss / cum_examples,\n",
        "                                                                                            np.exp(cum_loss / cum_tgt_words),\n",
        "                                                                                            cum_examples))\n",
        "                \n",
        "\n",
        "                cum_loss = cum_examples = cum_tgt_words = 0.\n",
        "                valid_num += 1\n",
        "\n",
        "                print('begin validation ...')\n",
        "\n",
        "                # compute dev. ppl and bleu\n",
        "                dev_ppl = evaluate_ppl(model, val_data, batch_size=128)   # dev batch size can be a bit larger\n",
        "                valid_metric = -dev_ppl\n",
        "                \n",
        "                bleu_score = evaluate_bleu(val_data_tgt, model, val_data_src)*100\n",
        "\n",
        "                print('validation: iter %d, dev. ppl %f, bleu_score %f' % (train_iter, dev_ppl, bleu_score))\n",
        "                wandb.log({\n",
        "                    \"bleu_score\": bleu_score\n",
        "                })\n",
        "                is_better = len(hist_valid_scores) == 0 or bleu_score > max(hist_valid_scores)\n",
        "                hist_valid_scores.append(bleu_score)\n",
        "\n",
        "                if is_better:\n",
        "                    print('save currently the best model to [%s]' % model_save_path)\n",
        "                    model.save(model_save_path)\n",
        "\n",
        "                    # also save the optimizers' state\n",
        "                    torch.save(optimizer.state_dict(), model_save_path + '.optim')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ShIVo6llLvy",
        "outputId": "a02c59a5-be0a-48ab-bcaa-843ac3d4dd1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "src_vocab = src\n",
        "tgt_vocab = tgt\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WJVs7GFffP6q"
      },
      "outputs": [],
      "source": [
        "wandb.config = {\n",
        "    \"embed_size\": 512,\n",
        "    \"hidden_size\": 512,\n",
        "    \"epochs\": 30,\n",
        "    \"train_batch_size\": 256, \n",
        "    \"clip_grad\":2,\n",
        "    \"lr\": 1e-3\n",
        "}\n",
        "config = wandb.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r2aRBV96RcGe"
      },
      "outputs": [],
      "source": [
        "epochs = 70\n",
        "train_batch_size = 128\n",
        "clip_grad = 2\n",
        "log_every = 100\n",
        "valid_niter = 500\n",
        "model_save_path = \"NMT_model.ckpt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6MMQuj0S0KQ9"
      },
      "outputs": [],
      "source": [
        "# len(train_data)/config(\"train_batch_size\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6pBNYqULlVDJ"
      },
      "outputs": [],
      "source": [
        "model = NMT(\n",
        "    config[\"embed_size\"],\n",
        "    config[\"hidden_size\"],\n",
        "    src_vocab,\n",
        "    tgt_vocab,\n",
        "    device=device,\n",
        "    pretrained_source=None, # src_embed_vectors,\n",
        "    pretrained_target=None # tgt_embed_vectors,\n",
        ")\n",
        "# model = baseline_nmt\n",
        "model.to(device)\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AQU-R5IUgpJ",
        "outputId": "5bb9edeb-f797-4642-976d-2049831b1b2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10727, 256])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tgt_embed_vectors.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "36e5a7a8a2344f42994a8091a7d0ea00",
            "ab83b8d512904a3dbea443345fd94838",
            "5362fb6186ec43ddbd94c0fecc6afe44",
            "bf605e3617c942a8a9631e0114cceb2d",
            "ed2fafb760444b58a929019095edae2a",
            "c2be6291459746ea95b3483d041b767e",
            "f713782da7c14585b7c6e47b46af8c33",
            "59ef3d5c41574d3daf8d20f6e3ea7852"
          ]
        },
        "id": "I11DvY80odkg",
        "outputId": "0357ec72-97da-4c34-dd9c-27c5ca012255"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:bsviv185) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1163... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36e5a7a8a2344f42994a8091a7d0ea00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">ethereal-planet-122</strong>: <a href=\"https://wandb.ai/chipmunkez/nlp-p3-demo/runs/bsviv185\" target=\"_blank\">https://wandb.ai/chipmunkez/nlp-p3-demo/runs/bsviv185</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211129_214314-bsviv185/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:bsviv185). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define each of the variables then you can run this command!\n",
        "wandb.init(project = 'nlp-p3-demo', entity = \"chipmunkez\", reinit = True)\n",
        "wandb.watch(model) #not necessary but will help you track gradients\n",
        "train_and_evaluate(\n",
        "    model,\n",
        "    train_data,\n",
        "    val_data,\n",
        "    optimizer,\n",
        "    config[\"epochs\"],\n",
        "    config[\"train_batch_size\"],\n",
        "    config[\"clip_grad\"],\n",
        "    log_every,\n",
        "    valid_niter,\n",
        "    model_save_path\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mf2j9xRpg1k"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wqcCNH_qmLo"
      },
      "outputs": [],
      "source": [
        "# import gc\n",
        "# del variables\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UbsqRZtWsp_"
      },
      "source": [
        "## 1.2 Part 2 Report\n",
        "For Section 1, your report should have a description of each major step of implementing the RNN accompanied by the associated code-snippet. Each step should have an explanation for why you decided to do something (when one could reasonably accomplish the same step in a different way); your justification should not be based on empirical results in this section but should relate to something we said in class, something mentioned in any of the course texts, or some other source (i.e. literature in NLP or official PyTorch documentation). **Unjustified, vague, and/or under-substantiated explanations will not receive credit.** As a reminder, the template for the write up is linked [here](https://docs.google.com/document/d/1IWgYqS6M4G_gJowM97Bq8g5smsGOa75dutIUGjolpAI/edit).\n",
        "\n",
        "Things to include:\n",
        "\n",
        "1. _Representation_ \\\n",
        "Each $\\vec{x}_i$ needs to be produced in some way and should correspond to word $i$ in the text. This is different from the text classification approaches we have studied previously (BoW for example) where the entire document is represented with a single vector. Where and how is this being done for the RNN?\n",
        "\n",
        "2. _Initialization_ \\\n",
        "There will be weights that you update in training the RNN. Where and how are these initialized?\n",
        "\n",
        "3. _Training_ \\\n",
        "You are given the entire training set of N examples. How do you make use of this training set? How does the model modify its weights in training (this likely entails somewhere where gradients are computed and somehwere else where these gradients are used to update the model)? Note: This is code you may not have written but that we have written for you!\n",
        "\n",
        "4. _Model_ \\\n",
        "This is the core model code, ie. where and how you apply the RNN to the $\\vec{x}_i$\n",
        "\n",
        "\n",
        "5. _Stopping_ \\\n",
        "How does your training procedure terminate? Note: This is code you may not have written but that we have written for you!\n",
        "\n",
        "6. _Hyperparameters_ \\\n",
        "To run your model, you must fix some hyperparameters, such as $h$ (the hidden dimensionality of the $\\vec{z}_i$ referenced above). Be sure to exhaustively describe these hyperparameters and why you set them as you did ( this almost certainly will require some brief exploration: we suggest the course text by Yoav Goldberg as well as possibly the PyTorch official documentation). Be sure to accurately cite either source.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA519cwBuhIW"
      },
      "source": [
        "### 1.2.1 Representation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pqvbTja2Fmv"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MwU9A8jumDH"
      },
      "source": [
        "### 1.2.2 Initialization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB2JyiWM2GX0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHzl4RR6vZZO"
      },
      "source": [
        "### 1.2.3 Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx_wSNoq2KyT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCBvcHgRvn8x"
      },
      "source": [
        "### 1.2.4 Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzUweVa72MoU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjia6Ge32O4v"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEfJKPWlvvPO"
      },
      "source": [
        "### 1.2.5 Stopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb4Tc3Fe2Shr"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpT2Ntgmv0RD"
      },
      "source": [
        "### 2.2.6 Hyperparameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdSHK0HF2ipj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJqQTVxdZjQ3"
      },
      "source": [
        "# Part 2: Analysis\n",
        "In **Part 2**, you will conduct a comprehensive analysis of these Neural Machine Translation models, focusing on two comparative settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szhOtAUuheQU"
      },
      "source": [
        "## Part 2 Note\n",
        "You will be required to submit the code used in finding these results on CMSX. This code should be legible and we will consult it if we find issues in the results. It is worth noting that in **Part 1** , we primarily are considering the correctness of the code-snippets in the report. If your model is flawed in a way that isn’t exposed by those snippets, this will likely surface in your results for **Part 2**. We will deduct points for correctness in this section to reflect this and we will try to localize where the error is (or think it is, if it is opaque from your code). That said, we will be lenient about absolute performance (within reason) in this section. As a reminder, the template for the write up is linked [here](https://docs.google.com/document/d/1IWgYqS6M4G_gJowM97Bq8g5smsGOa75dutIUGjolpAI/edit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Of6kWAcfgde"
      },
      "source": [
        "## Part 2.1: Within-model comparison\n",
        "In **Part 2.1: Within-Model Comparison**, you will need to study what happens when you change parameters within a model.\n",
        "\n",
        "A large aspect of rigorous experimentation in NLP (and other domains) is the _ablation study_. In this, we _ablate_ or remove aspects of a more complex model, making it less complex, to evaluate whether each aspect was neccessary. To be concrete, for this part, you should train 4 variants of the RNN model and describe them as we do below:\n",
        "\n",
        "1. Baseline model\n",
        "2. Baseline model made more complex by modification $A$ (e.g. changing the hidden dimensionality from $h$ to $2h$).\n",
        "3. Baseline model made more complex by modification $B$ (where $B$ is an entirely distinct/different update from $A$).\n",
        "4. Baseline model with both modifications $A$ and $B$ applied.\n",
        "\n",
        "Under the framing of an ablation study, you would describe this as beginning with model 4 and then ablating (i.e. removing) each of the two modifications, in turn; and then removing both to see if they were genuinely neccessary for the performance you observe. \n",
        "\n",
        "Once you describe each of the four models, report the quantitative bleu score and perplexity. Conclude by performing a nuanced analysis.\n",
        "\n",
        "The descriptive analysis can take one of two forms:\n",
        "\n",
        "1. _Nuanced quantitative analysis_ \\\n",
        "If you choose this option, you will need to further break down the quantitative statistics you reported initially. We provide some initial strategies to prime you for what you should think about in doing this: one possible starting point is to consider: if model $X$ achieves greater accuracy than model $Y$, to what extent is $X$ getting everything correct that $Y$ gets correct? Alternatively, how is model performance affected if you measure performance on a specific strata/subset of the source sentences?\n",
        "\n",
        "2. _Nuanced qualitative analysis_ \\\n",
        "If you choose this option, you will need to select individual examples and try to explain or reason about why one model may be getting them right whereas the other isn’t. Are there any examples that all 4 models get right or wrong and, if so, can you hypothesize a reason why this occurs?\n",
        "\n",
        "\n",
        "**NOTE:** Although we code individual sections below for each of the configurations. The report should be written keeping all of them in mind discussing all of their performances as well as doing the nuanced analysis with _all_ of the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIt_mg_H2b4h"
      },
      "source": [
        "The function below will be useful for analyzing translations by piecing back together the prediction into a cohesive sequence of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwKsMKaU2X0P"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def untokenize(words):\n",
        "    \"\"\"\n",
        "    Untokenizing a text undoes the tokenizing operation, restoring\n",
        "    punctuation and spaces to the places that people expect them to be.\n",
        "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
        "    except for line breaks.\n",
        "    \"\"\"\n",
        "    text = ' '.join(words)\n",
        "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
        "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
        "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
        "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
        "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
        "         \"can not\", \"cannot\")\n",
        "    step6 = step5.replace(\" ` \", \" '\")\n",
        "    return step6.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TULtij8lzQpt"
      },
      "source": [
        "### 2.1.1 Configuration 1\n",
        "Modify the code below for this configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBoo8VMeD103"
      },
      "outputs": [],
      "source": [
        "baseline_nmt = NMT(\n",
        "    config[\"embed_size\"],\n",
        "    config[\"hidden_size\"],\n",
        "    src_vocab,\n",
        "    tgt_vocab,\n",
        "    device=device,\n",
        "    pretrained_source=None, # src_embed_vectors,\n",
        "    pretrained_target=None, # tgt_embed_vectors,\n",
        "    LSTM_RNN = 'RNN'\n",
        ")\n",
        "baseline_nmt.to(device)\n",
        "baseline_nmt.train()\n",
        "optimizer = torch.optim.Adam(baseline_nmt.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3Qkj5uSqq-8"
      },
      "outputs": [],
      "source": [
        "# Define each of the variables then you can run this command!\n",
        "wandb.init(project = 'nlp-p3-demo', entity = \"chipmunkez\", reinit = True)\n",
        "wandb.watch(baseline_nmt) #not necessary but will help you track gradients\n",
        "train_and_evaluate(\n",
        "    baseline_nmt,\n",
        "    train_data,\n",
        "    val_data,\n",
        "    optimizer,\n",
        "    config[\"epochs\"],\n",
        "    config[\"train_batch_size\"],\n",
        "    config[\"clip_grad\"],\n",
        "    log_every,\n",
        "    valid_niter,\n",
        "    model_save_path\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Q4LpZOzxa6"
      },
      "source": [
        "### 2.1.2 Configuration 2\n",
        "Modify the code below for this configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSE9bbSnGI2A"
      },
      "outputs": [],
      "source": [
        "mod_a_nmt = NMT( config[\"embed_size\"],\n",
        "    config[\"hidden_size\"],\n",
        "    src_vocab,\n",
        "    tgt_vocab,\n",
        "    device=device,\n",
        "    pretrained_source=src_embed_vectors, \n",
        "    pretrained_target=tgt_embed_vectors, \n",
        "    LSTM_RNN = 'RNN')\n",
        "mod_a_nmt.to(device)\n",
        "mod_a_nmt.train()\n",
        "optimizer = torch.optim.Adam(mod_a_nmt.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "bc9da0bdd53c46ad9248ec029ac4aacd",
            "ebb8702be63942628b20e2ab6cd1b1c2",
            "b96664c7e3f14586bf1d74d372c95794",
            "a2b9e33d44df4ebfaa9330b8d9b0bb74",
            "d30d0b912a7e467da8a2764fb871d399",
            "1d4b7371d66e4cefb9f08b20bf447f29",
            "8168fd182d304c13b75a93ecd4d4cf1f",
            "56a69f3772974335b30cbe5e4aefb1f1",
            "8044d6bafd1b42e8950323abc24b3ec5",
            "9f8eb6e30bad41d0b600cdc310a65b07"
          ]
        },
        "id": "muwnIFDLqgF8",
        "outputId": "ec592d6f-26d7-4350-99af-853aaf79b643"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchipmunkez\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/chipmunkez/nlp-p3-demo/runs/3mk1u1g5\" target=\"_blank\">balmy-snowball-124</a></strong> to <a href=\"https://wandb.ai/chipmunkez/nlp-p3-demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Begin Maximum Likelihood training\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc9da0bdd53c46ad9248ec029ac4aacd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, iter 100, avg. loss 83.98, avg. ppl 433.10 cum. examples 25600, speed 22173.16 words/sec, time elapsed 15.97 sec\n",
            "epoch 1, iter 200, avg. loss 76.21, avg. ppl 240.00 cum. examples 50963, speed 21901.40 words/sec, time elapsed 32.07 sec\n",
            "epoch 1, iter 300, avg. loss 74.04, avg. ppl 198.32 cum. examples 76326, speed 21633.70 words/sec, time elapsed 48.48 sec\n",
            "epoch 2, iter 400, avg. loss 70.90, avg. ppl 162.55 cum. examples 101926, speed 22266.14 words/sec, time elapsed 64.50 sec\n",
            "epoch 3, iter 500, avg. loss 68.53, avg. ppl 140.53 cum. examples 127289, speed 22372.50 words/sec, time elapsed 80.21 sec\n",
            "epoch 3, iter 500, cum. loss 74.74, cum. ppl 216.10 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebb8702be63942628b20e2ab6cd1b1c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 500, dev. ppl 143.045360, bleu_score 0.319993\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3, iter 600, avg. loss 67.92, avg. ppl 130.15 cum. examples 25363, speed 5167.76 words/sec, time elapsed 148.67 sec\n",
            "epoch 4, iter 700, avg. loss 65.50, avg. ppl 113.62 cum. examples 50963, speed 21233.79 words/sec, time elapsed 165.36 sec\n",
            "epoch 5, iter 800, avg. loss 64.42, avg. ppl 102.20 cum. examples 76326, speed 22993.70 words/sec, time elapsed 180.72 sec\n",
            "epoch 5, iter 900, avg. loss 63.62, avg. ppl 95.00 cum. examples 101689, speed 21873.71 words/sec, time elapsed 196.92 sec\n",
            "epoch 6, iter 1000, avg. loss 61.31, avg. ppl 82.94 cum. examples 127289, speed 22045.59 words/sec, time elapsed 213.03 sec\n",
            "epoch 6, iter 1000, cum. loss 64.55, cum. ppl 103.54 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b96664c7e3f14586bf1d74d372c95794",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 1000, dev. ppl 92.994019, bleu_score 0.639857\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 7, iter 1100, avg. loss 60.55, avg. ppl 76.12 cum. examples 25363, speed 4369.16 words/sec, time elapsed 294.16 sec\n",
            "epoch 7, iter 1200, avg. loss 59.17, avg. ppl 70.99 cum. examples 50726, speed 23122.37 words/sec, time elapsed 309.39 sec\n",
            "epoch 8, iter 1300, avg. loss 57.90, avg. ppl 63.39 cum. examples 76326, speed 22128.11 words/sec, time elapsed 325.53 sec\n",
            "epoch 9, iter 1400, avg. loss 56.54, avg. ppl 58.92 cum. examples 101689, speed 21973.71 words/sec, time elapsed 341.55 sec\n",
            "epoch 9, iter 1500, avg. loss 55.95, avg. ppl 55.86 cum. examples 127052, speed 22542.34 words/sec, time elapsed 357.20 sec\n",
            "epoch 9, iter 1500, cum. loss 58.02, cum. ppl 64.64 cum. examples 127052\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2b9e33d44df4ebfaa9330b8d9b0bb74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 1500, dev. ppl 70.818666, bleu_score 0.924782\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 10, iter 1600, avg. loss 54.37, avg. ppl 49.79 cum. examples 25600, speed 4526.13 words/sec, time elapsed 435.89 sec\n",
            "epoch 11, iter 1700, avg. loss 53.42, avg. ppl 47.02 cum. examples 50963, speed 22387.74 words/sec, time elapsed 451.61 sec\n",
            "epoch 11, iter 1800, avg. loss 53.07, avg. ppl 44.93 cum. examples 76326, speed 21885.95 words/sec, time elapsed 467.77 sec\n",
            "epoch 12, iter 1900, avg. loss 51.41, avg. ppl 39.89 cum. examples 101926, speed 22358.52 words/sec, time elapsed 483.74 sec\n",
            "epoch 13, iter 2000, avg. loss 50.32, avg. ppl 37.70 cum. examples 127289, speed 22140.99 words/sec, time elapsed 499.63 sec\n",
            "epoch 13, iter 2000, cum. loss 52.52, cum. ppl 43.64 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d30d0b912a7e467da8a2764fb871d399",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 2000, dev. ppl 57.859854, bleu_score 1.171994\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 13, iter 2100, avg. loss 50.03, avg. ppl 36.37 cum. examples 25363, speed 4310.16 words/sec, time elapsed 581.54 sec\n",
            "epoch 14, iter 2200, avg. loss 48.09, avg. ppl 32.36 cum. examples 50963, speed 22026.96 words/sec, time elapsed 597.62 sec\n",
            "epoch 15, iter 2300, avg. loss 48.30, avg. ppl 31.14 cum. examples 76326, speed 22093.98 words/sec, time elapsed 613.74 sec\n",
            "epoch 15, iter 2400, avg. loss 47.16, avg. ppl 30.05 cum. examples 101689, speed 22609.23 words/sec, time elapsed 629.29 sec\n",
            "epoch 16, iter 2500, avg. loss 45.65, avg. ppl 26.95 cum. examples 127289, speed 22388.02 words/sec, time elapsed 645.14 sec\n",
            "epoch 16, iter 2500, cum. loss 47.84, cum. ppl 31.22 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d4b7371d66e4cefb9f08b20bf447f29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 2500, dev. ppl 49.106040, bleu_score 1.381714\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 17, iter 2600, avg. loss 45.06, avg. ppl 25.75 cum. examples 25363, speed 4428.80 words/sec, time elapsed 724.58 sec\n",
            "epoch 17, iter 2700, avg. loss 45.32, avg. ppl 25.44 cum. examples 50726, speed 21902.97 words/sec, time elapsed 740.80 sec\n",
            "epoch 18, iter 2800, avg. loss 43.68, avg. ppl 22.79 cum. examples 76326, speed 21918.44 words/sec, time elapsed 757.12 sec\n",
            "epoch 19, iter 2900, avg. loss 42.74, avg. ppl 21.94 cum. examples 101689, speed 22504.58 words/sec, time elapsed 772.71 sec\n",
            "epoch 19, iter 3000, avg. loss 42.79, avg. ppl 21.61 cum. examples 127052, speed 21539.55 words/sec, time elapsed 789.11 sec\n",
            "epoch 19, iter 3000, cum. loss 43.92, cum. ppl 23.44 cum. examples 127052\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8168fd182d304c13b75a93ecd4d4cf1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 3000, dev. ppl 44.612018, bleu_score 1.623025\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 20, iter 3100, avg. loss 41.22, avg. ppl 19.60 cum. examples 25600, speed 4555.79 words/sec, time elapsed 866.95 sec\n",
            "epoch 21, iter 3200, avg. loss 41.15, avg. ppl 18.95 cum. examples 50963, speed 23187.82 words/sec, time elapsed 882.25 sec\n",
            "epoch 21, iter 3300, avg. loss 40.42, avg. ppl 18.35 cum. examples 76326, speed 21158.47 words/sec, time elapsed 898.91 sec\n",
            "epoch 22, iter 3400, avg. loss 39.07, avg. ppl 16.79 cum. examples 101926, speed 22604.83 words/sec, time elapsed 914.59 sec\n",
            "epoch 23, iter 3500, avg. loss 39.39, avg. ppl 16.48 cum. examples 127289, speed 21796.15 words/sec, time elapsed 930.95 sec\n",
            "epoch 23, iter 3500, cum. loss 40.25, cum. ppl 17.99 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56a69f3772974335b30cbe5e4aefb1f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 3500, dev. ppl 40.377807, bleu_score 1.264198\n",
            "epoch 23, iter 3600, avg. loss 38.45, avg. ppl 16.13 cum. examples 25363, speed 4602.94 words/sec, time elapsed 1007.15 sec\n",
            "epoch 24, iter 3700, avg. loss 37.44, avg. ppl 14.73 cum. examples 50963, speed 22723.13 words/sec, time elapsed 1022.83 sec\n",
            "epoch 25, iter 3800, avg. loss 37.08, avg. ppl 14.47 cum. examples 76326, speed 22121.89 words/sec, time elapsed 1038.74 sec\n",
            "epoch 25, iter 3900, avg. loss 37.00, avg. ppl 14.22 cum. examples 101689, speed 21930.99 words/sec, time elapsed 1054.86 sec\n",
            "epoch 26, iter 4000, avg. loss 35.61, avg. ppl 13.05 cum. examples 127289, speed 21752.75 words/sec, time elapsed 1071.18 sec\n",
            "epoch 26, iter 4000, cum. loss 37.12, cum. ppl 14.48 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8044d6bafd1b42e8950323abc24b3ec5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 4000, dev. ppl 38.179109, bleu_score 1.232265\n",
            "epoch 27, iter 4100, avg. loss 35.96, avg. ppl 12.83 cum. examples 25363, speed 4971.61 words/sec, time elapsed 1143.08 sec\n",
            "epoch 27, iter 4200, avg. loss 35.04, avg. ppl 12.72 cum. examples 50726, speed 23061.02 words/sec, time elapsed 1158.23 sec\n",
            "epoch 28, iter 4300, avg. loss 34.48, avg. ppl 11.81 cum. examples 76326, speed 22058.01 words/sec, time elapsed 1174.44 sec\n",
            "epoch 29, iter 4400, avg. loss 34.07, avg. ppl 11.64 cum. examples 101689, speed 22206.94 words/sec, time elapsed 1190.29 sec\n",
            "epoch 29, iter 4500, avg. loss 33.65, avg. ppl 11.29 cum. examples 127052, speed 22605.00 words/sec, time elapsed 1205.87 sec\n",
            "epoch 29, iter 4500, cum. loss 34.64, cum. ppl 12.04 cum. examples 127052\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f8eb6e30bad41d0b600cdc310a65b07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 4500, dev. ppl 37.224074, bleu_score 1.511197\n"
          ]
        }
      ],
      "source": [
        "# Define each of the variables then you can run this command!\n",
        "wandb.init(project = 'nlp-p3-demo', entity = \"chipmunkez\", reinit = True)\n",
        "wandb.watch(mod_a_nmt) #not necessary but will help you track gradients\n",
        "train_and_evaluate(\n",
        "    mod_a_nmt,\n",
        "    train_data,\n",
        "    val_data,\n",
        "    optimizer,\n",
        "    config[\"epochs\"],\n",
        "    config[\"train_batch_size\"],\n",
        "    config[\"clip_grad\"],\n",
        "    log_every,\n",
        "    valid_niter,\n",
        "    model_save_path\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVG-Pqa9z60-"
      },
      "source": [
        "### 2.1.3 Configuration 3\n",
        "Modify the code below for this configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4WsI_BPpGJ5r"
      },
      "outputs": [],
      "source": [
        "mod_b_nmt = NMT(\n",
        "    config[\"embed_size\"],\n",
        "    config[\"hidden_size\"],\n",
        "    src_vocab,\n",
        "    tgt_vocab,\n",
        "    device=device,\n",
        "    pretrained_source=None, \n",
        "    pretrained_target=None, \n",
        "    LSTM_RNN = 'LSTM'\n",
        ")\n",
        "# model = baseline_nmt\n",
        "mod_b_nmt.to(device)\n",
        "mod_b_nmt.train()\n",
        "optimizer = torch.optim.Adam(mod_b_nmt.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "72485069058046f395c9464c88f432b2",
            "7ae8ddcb5acb4c3cb85ff4abe437d1c1",
            "9104bd46df64487d85a8192b867c1cda",
            "c9ef0347d6fe4730a38ac72492a84d9b",
            "2c687ebb1a9b4f4d8b118cd28a4b5fb7",
            "ee4fa562f9ec415a8b7fecd8a9065e51",
            "c96764e81bbf4357a257bb522a12904d",
            "d719997d469146be855403a5eef3fa30",
            "984b295ecd444af394fece8f02eaf43e",
            "963cdfd59a264b65b2771bc7658e4041",
            "337e5911839a478bbbd729a70ce8f887"
          ]
        },
        "id": "R4GJO_2NqUyR",
        "outputId": "dc2c6e18-72c0-40a0-b4b5-2782a30264bf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:3mk1u1g5) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1706... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72485069058046f395c9464c88f432b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg. train loss</td><td>█▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>avg. train perplexity</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg. val loss</td><td>█▆▄▃▂▂▁▁▁</td></tr><tr><td>avg. val perplexity</td><td>█▅▃▂▂▁▁▁▁</td></tr><tr><td>bleu_score</td><td>▁▃▄▆▇█▆▆▇</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg. train loss</td><td>33.65379</td></tr><tr><td>avg. train perplexity</td><td>11.28784</td></tr><tr><td>avg. val loss</td><td>49.86752</td></tr><tr><td>avg. val perplexity</td><td>37.22407</td></tr><tr><td>bleu_score</td><td>1.5112</td></tr><tr><td>epoch</td><td>29</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">balmy-snowball-124</strong>: <a href=\"https://wandb.ai/chipmunkez/nlp-p3-demo/runs/3mk1u1g5\" target=\"_blank\">https://wandb.ai/chipmunkez/nlp-p3-demo/runs/3mk1u1g5</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211129_220459-3mk1u1g5/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:3mk1u1g5). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/chipmunkez/nlp-p3-demo/runs/8hr9w3zd\" target=\"_blank\">snowy-wind-125</a></strong> to <a href=\"https://wandb.ai/chipmunkez/nlp-p3-demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Begin Maximum Likelihood training\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ae8ddcb5acb4c3cb85ff4abe437d1c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, iter 100, avg. loss 81.18, avg. ppl 333.84 cum. examples 25600, speed 15836.58 words/sec, time elapsed 22.58 sec\n",
            "epoch 1, iter 200, avg. loss 68.54, avg. ppl 138.57 cum. examples 50963, speed 15964.64 words/sec, time elapsed 44.67 sec\n",
            "epoch 1, iter 300, avg. loss 62.26, avg. ppl 89.21 cum. examples 76326, speed 15458.43 words/sec, time elapsed 67.41 sec\n",
            "epoch 2, iter 400, avg. loss 57.41, avg. ppl 62.24 cum. examples 101926, speed 16280.68 words/sec, time elapsed 89.27 sec\n",
            "epoch 3, iter 500, avg. loss 54.09, avg. ppl 49.01 cum. examples 127289, speed 15338.14 words/sec, time elapsed 112.25 sec\n",
            "epoch 3, iter 500, cum. loss 64.71, cum. ppl 104.97 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9104bd46df64487d85a8192b867c1cda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 500, dev. ppl 53.734972, bleu_score 3.826354\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3, iter 600, avg. loss 51.60, avg. ppl 40.53 cum. examples 25363, speed 4182.21 words/sec, time elapsed 196.77 sec\n",
            "epoch 4, iter 700, avg. loss 47.40, avg. ppl 30.15 cum. examples 50963, speed 15865.35 words/sec, time elapsed 219.23 sec\n",
            "epoch 5, iter 800, avg. loss 44.58, avg. ppl 24.87 cum. examples 76326, speed 15750.20 words/sec, time elapsed 241.57 sec\n",
            "epoch 5, iter 900, avg. loss 42.70, avg. ppl 21.37 cum. examples 101689, speed 15886.35 words/sec, time elapsed 263.83 sec\n",
            "epoch 6, iter 1000, avg. loss 38.42, avg. ppl 15.86 cum. examples 127289, speed 15960.84 words/sec, time elapsed 286.13 sec\n",
            "epoch 6, iter 1000, cum. loss 44.93, cum. ppl 25.26 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9ef0347d6fe4730a38ac72492a84d9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 1000, dev. ppl 31.324339, bleu_score 8.271961\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 7, iter 1100, avg. loss 36.18, avg. ppl 13.52 cum. examples 25363, speed 4219.70 words/sec, time elapsed 369.63 sec\n",
            "epoch 7, iter 1200, avg. loss 34.52, avg. ppl 11.90 cum. examples 50726, speed 15900.18 words/sec, time elapsed 391.87 sec\n",
            "epoch 8, iter 1300, avg. loss 30.44, avg. ppl 8.93 cum. examples 76326, speed 16314.95 words/sec, time elapsed 413.68 sec\n",
            "epoch 9, iter 1400, avg. loss 29.12, avg. ppl 8.01 cum. examples 101689, speed 15569.27 words/sec, time elapsed 436.48 sec\n",
            "epoch 9, iter 1500, avg. loss 26.93, avg. ppl 7.00 cum. examples 127052, speed 16098.21 words/sec, time elapsed 458.28 sec\n",
            "epoch 9, iter 1500, cum. loss 31.44, cum. ppl 9.58 cum. examples 127052\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c687ebb1a9b4f4d8b118cd28a4b5fb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 1500, dev. ppl 24.790620, bleu_score 13.729393\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 10, iter 1600, avg. loss 23.77, avg. ppl 5.54 cum. examples 25600, speed 4248.34 words/sec, time elapsed 541.95 sec\n",
            "epoch 11, iter 1700, avg. loss 22.76, avg. ppl 5.14 cum. examples 50963, speed 15827.47 words/sec, time elapsed 564.23 sec\n",
            "epoch 11, iter 1800, avg. loss 21.36, avg. ppl 4.63 cum. examples 76326, speed 15294.58 words/sec, time elapsed 587.36 sec\n",
            "epoch 12, iter 1900, avg. loss 18.66, avg. ppl 3.80 cum. examples 101926, speed 16020.35 words/sec, time elapsed 609.68 sec\n",
            "epoch 13, iter 2000, avg. loss 17.53, avg. ppl 3.56 cum. examples 127289, speed 15671.36 words/sec, time elapsed 632.05 sec\n",
            "epoch 13, iter 2000, cum. loss 20.82, cum. ppl 4.47 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee4fa562f9ec415a8b7fecd8a9065e51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 2000, dev. ppl 23.752127, bleu_score 19.820404\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 13, iter 2100, avg. loss 16.77, avg. ppl 3.33 cum. examples 25363, speed 4243.15 words/sec, time elapsed 715.40 sec\n",
            "epoch 14, iter 2200, avg. loss 14.43, avg. ppl 2.83 cum. examples 50963, speed 15933.09 words/sec, time elapsed 737.68 sec\n",
            "epoch 15, iter 2300, avg. loss 13.84, avg. ppl 2.70 cum. examples 76326, speed 15715.36 words/sec, time elapsed 760.18 sec\n",
            "epoch 15, iter 2400, avg. loss 13.08, avg. ppl 2.56 cum. examples 101689, speed 15781.46 words/sec, time elapsed 782.56 sec\n",
            "epoch 16, iter 2500, avg. loss 11.19, avg. ppl 2.24 cum. examples 127289, speed 15841.51 words/sec, time elapsed 804.94 sec\n",
            "epoch 16, iter 2500, cum. loss 13.86, cum. ppl 2.71 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c96764e81bbf4357a257bb522a12904d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 2500, dev. ppl 24.492402, bleu_score 25.907401\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 17, iter 2600, avg. loss 10.88, avg. ppl 2.18 cum. examples 25363, speed 4241.56 words/sec, time elapsed 888.62 sec\n",
            "epoch 17, iter 2700, avg. loss 10.28, avg. ppl 2.10 cum. examples 50726, speed 15785.52 words/sec, time elapsed 910.94 sec\n",
            "epoch 18, iter 2800, avg. loss 8.88, avg. ppl 1.89 cum. examples 76326, speed 15923.05 words/sec, time elapsed 933.32 sec\n",
            "epoch 19, iter 2900, avg. loss 8.42, avg. ppl 1.84 cum. examples 101689, speed 15862.73 words/sec, time elapsed 955.47 sec\n",
            "epoch 19, iter 3000, avg. loss 8.34, avg. ppl 1.82 cum. examples 127052, speed 15526.22 words/sec, time elapsed 978.28 sec\n",
            "epoch 19, iter 3000, cum. loss 9.36, cum. ppl 1.96 cum. examples 127052\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d719997d469146be855403a5eef3fa30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 3000, dev. ppl 27.293550, bleu_score 30.816940\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 20, iter 3100, avg. loss 6.98, avg. ppl 1.65 cum. examples 25600, speed 4260.70 words/sec, time elapsed 1061.61 sec\n",
            "epoch 21, iter 3200, avg. loss 6.82, avg. ppl 1.63 cum. examples 50963, speed 15964.74 words/sec, time elapsed 1083.75 sec\n",
            "epoch 21, iter 3300, avg. loss 6.55, avg. ppl 1.60 cum. examples 76326, speed 15855.67 words/sec, time elapsed 1106.03 sec\n",
            "epoch 22, iter 3400, avg. loss 5.61, avg. ppl 1.50 cum. examples 101926, speed 15389.61 words/sec, time elapsed 1129.10 sec\n",
            "epoch 23, iter 3500, avg. loss 5.43, avg. ppl 1.47 cum. examples 127289, speed 16132.21 words/sec, time elapsed 1151.22 sec\n",
            "epoch 23, iter 3500, cum. loss 6.28, cum. ppl 1.57 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "984b295ecd444af394fece8f02eaf43e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 3500, dev. ppl 30.551156, bleu_score 35.628826\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 23, iter 3600, avg. loss 5.27, avg. ppl 1.47 cum. examples 25363, speed 4212.91 words/sec, time elapsed 1234.28 sec\n",
            "epoch 24, iter 3700, avg. loss 4.52, avg. ppl 1.38 cum. examples 50963, speed 15787.64 words/sec, time elapsed 1256.85 sec\n",
            "epoch 25, iter 3800, avg. loss 4.46, avg. ppl 1.38 cum. examples 76326, speed 15527.59 words/sec, time elapsed 1279.55 sec\n",
            "epoch 25, iter 3900, avg. loss 4.34, avg. ppl 1.37 cum. examples 101689, speed 16340.10 words/sec, time elapsed 1301.16 sec\n",
            "epoch 26, iter 4000, avg. loss 3.64, avg. ppl 1.30 cum. examples 127289, speed 16122.93 words/sec, time elapsed 1323.11 sec\n",
            "epoch 26, iter 4000, cum. loss 4.45, cum. ppl 1.38 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "963cdfd59a264b65b2771bc7658e4041",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 4000, dev. ppl 34.794210, bleu_score 40.209022\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 27, iter 4100, avg. loss 3.69, avg. ppl 1.30 cum. examples 25363, speed 4284.76 words/sec, time elapsed 1406.38 sec\n",
            "epoch 27, iter 4200, avg. loss 3.52, avg. ppl 1.29 cum. examples 50726, speed 15695.98 words/sec, time elapsed 1428.74 sec\n",
            "epoch 28, iter 4300, avg. loss 3.08, avg. ppl 1.25 cum. examples 76326, speed 15921.45 words/sec, time elapsed 1450.94 sec\n",
            "epoch 29, iter 4400, avg. loss 3.01, avg. ppl 1.24 cum. examples 101689, speed 15927.04 words/sec, time elapsed 1473.30 sec\n",
            "epoch 29, iter 4500, avg. loss 3.02, avg. ppl 1.24 cum. examples 127052, speed 15555.08 words/sec, time elapsed 1495.95 sec\n",
            "epoch 29, iter 4500, cum. loss 3.26, cum. ppl 1.26 cum. examples 127052\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "337e5911839a478bbbd729a70ce8f887",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 4500, dev. ppl 39.951869, bleu_score 43.040271\n",
            "save currently the best model to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        }
      ],
      "source": [
        "# Define each of the variables then you can run this command!\n",
        "wandb.init(project = 'nlp-p3-demo', entity = \"chipmunkez\", reinit = True)\n",
        "wandb.watch(mod_b_nmt) #not necessary but will help you track gradients\n",
        "train_and_evaluate(\n",
        "    mod_b_nmt,\n",
        "    train_data,\n",
        "    val_data,\n",
        "    optimizer,\n",
        "    config[\"epochs\"],\n",
        "    config[\"train_batch_size\"],\n",
        "    config[\"clip_grad\"],\n",
        "    log_every,\n",
        "    valid_niter,\n",
        "    model_save_path\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfukYWxcz-Za"
      },
      "source": [
        "### 2.1.4 Configuration 4\n",
        "Modify the code below for this configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d-wrAMzOpiil"
      },
      "outputs": [],
      "source": [
        "# config 4\n",
        "both_mods_nmt = NMT(\n",
        "    config[\"embed_size\"],\n",
        "    config[\"hidden_size\"],\n",
        "    src_vocab,\n",
        "    tgt_vocab,\n",
        "    device=device,\n",
        "    pretrained_source=src_embed_vectors, \n",
        "    pretrained_target=tgt_embed_vectors, \n",
        "    LSTM_RNN = 'LSTM'\n",
        ")\n",
        "# model = baseline_nmt\n",
        "both_mods_nmt.to(device)\n",
        "both_mods_nmt.train()\n",
        "optimizer = torch.optim.Adam(both_mods_nmt.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "9b8a7a536cb042379582798a0aded19d",
            "486b1e20aa104d53921cb14f9188bf1e",
            "0babe079a0c2426c81d9fafb3121128a",
            "45699453aee34426927c2d666cc78a1c",
            "aa7ae71158a140ef88687264fc95b1e9",
            "8c869f69592e466dba1eb68eb06aa6dd",
            "92c3778ae62b4a50a5dee5e9e6b8e61f",
            "33ba8b7921c6491c9b015b0875bff821",
            "8c7b4d5cbade4d03a415f06d69d3dd55",
            "282d66fc231c42249de8ed693df0bb2f",
            "1a0bb30320e0434dbfbb562ef281bbe9"
          ]
        },
        "id": "b-80n1Qup5rZ",
        "outputId": "d4b2881d-866d-4507-e9c1-2cb3274b76f2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:8hr9w3zd) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1749... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b8a7a536cb042379582798a0aded19d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg. train loss</td><td>█▇▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg. train perplexity</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg. val loss</td><td>█▃▁▁▁▂▃▄▅</td></tr><tr><td>avg. val perplexity</td><td>█▃▁▁▁▂▃▄▅</td></tr><tr><td>bleu_score</td><td>▁▂▃▄▅▆▇▇█</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg. train loss</td><td>3.02135</td></tr><tr><td>avg. train perplexity</td><td>1.24299</td></tr><tr><td>avg. val loss</td><td>50.84255</td></tr><tr><td>avg. val perplexity</td><td>39.95187</td></tr><tr><td>bleu_score</td><td>43.04027</td></tr><tr><td>epoch</td><td>29</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">snowy-wind-125</strong>: <a href=\"https://wandb.ai/chipmunkez/nlp-p3-demo/runs/8hr9w3zd\" target=\"_blank\">https://wandb.ai/chipmunkez/nlp-p3-demo/runs/8hr9w3zd</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211129_222604-8hr9w3zd/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:8hr9w3zd). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/chipmunkez/nlp-p3-demo/runs/277qlksl\" target=\"_blank\">polished-plant-126</a></strong> to <a href=\"https://wandb.ai/chipmunkez/nlp-p3-demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Begin Maximum Likelihood training\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "486b1e20aa104d53921cb14f9188bf1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, iter 100, avg. loss 129.44, avg. ppl 10702.97 cum. examples 25600, speed 22303.24 words/sec, time elapsed 16.01 sec\n",
            "epoch 1, iter 200, avg. loss 129.12, avg. ppl 10699.69 cum. examples 50963, speed 21375.15 words/sec, time elapsed 32.53 sec\n",
            "epoch 1, iter 300, avg. loss 128.65, avg. ppl 10700.78 cum. examples 76326, speed 23410.70 words/sec, time elapsed 47.55 sec\n",
            "epoch 2, iter 400, avg. loss 129.12, avg. ppl 10701.73 cum. examples 101926, speed 22485.86 words/sec, time elapsed 63.39 sec\n",
            "epoch 3, iter 500, avg. loss 128.50, avg. ppl 10700.62 cum. examples 127289, speed 22752.51 words/sec, time elapsed 78.83 sec\n",
            "epoch 3, iter 500, cum. loss 128.97, cum. ppl 10701.16 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0babe079a0c2426c81d9fafb3121128a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "save model parameters to [NMT_model.ckpt]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 500, dev. ppl 10700.690941, bleu_score 18.117572\n",
            "save currently the best model to [NMT_model.ckpt]\n",
            "epoch 3, iter 600, avg. loss 129.59, avg. ppl 10701.10 cum. examples 25363, speed 2789.04 words/sec, time elapsed 205.85 sec\n",
            "epoch 4, iter 700, avg. loss 128.46, avg. ppl 10697.72 cum. examples 50963, speed 22918.41 words/sec, time elapsed 221.32 sec\n",
            "epoch 5, iter 800, avg. loss 129.78, avg. ppl 10705.90 cum. examples 76326, speed 21179.08 words/sec, time elapsed 238.07 sec\n",
            "epoch 5, iter 900, avg. loss 128.98, avg. ppl 10699.83 cum. examples 101689, speed 23406.75 words/sec, time elapsed 253.13 sec\n",
            "epoch 6, iter 1000, avg. loss 128.72, avg. ppl 10700.44 cum. examples 127289, speed 22381.17 words/sec, time elapsed 269.00 sec\n",
            "epoch 6, iter 1000, cum. loss 129.10, cum. ppl 10701.00 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45699453aee34426927c2d666cc78a1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 1000, dev. ppl 10700.690941, bleu_score 18.117572\n",
            "epoch 7, iter 1100, avg. loss 129.24, avg. ppl 10701.35 cum. examples 25363, speed 2797.99 words/sec, time elapsed 395.27 sec\n",
            "epoch 7, iter 1200, avg. loss 129.26, avg. ppl 10701.67 cum. examples 50726, speed 22077.11 words/sec, time elapsed 411.27 sec\n",
            "epoch 8, iter 1300, avg. loss 128.59, avg. ppl 10699.24 cum. examples 76326, speed 22583.19 words/sec, time elapsed 426.99 sec\n",
            "epoch 9, iter 1400, avg. loss 129.24, avg. ppl 10700.25 cum. examples 101689, speed 22648.08 words/sec, time elapsed 442.59 sec\n",
            "epoch 9, iter 1500, avg. loss 129.39, avg. ppl 10703.97 cum. examples 127052, speed 22049.09 words/sec, time elapsed 458.63 sec\n",
            "epoch 9, iter 1500, cum. loss 129.14, cum. ppl 10701.30 cum. examples 127052\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa7ae71158a140ef88687264fc95b1e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 1500, dev. ppl 10700.690941, bleu_score 18.117572\n",
            "epoch 10, iter 1600, avg. loss 128.97, avg. ppl 10701.25 cum. examples 25600, speed 2802.51 words/sec, time elapsed 585.60 sec\n",
            "epoch 11, iter 1700, avg. loss 128.97, avg. ppl 10699.89 cum. examples 50963, speed 23039.76 words/sec, time elapsed 600.91 sec\n",
            "epoch 11, iter 1800, avg. loss 129.28, avg. ppl 10702.32 cum. examples 76326, speed 22187.31 words/sec, time elapsed 616.83 sec\n",
            "epoch 12, iter 1900, avg. loss 129.67, avg. ppl 10700.93 cum. examples 101926, speed 21711.06 words/sec, time elapsed 633.31 sec\n",
            "epoch 13, iter 2000, avg. loss 128.33, avg. ppl 10700.40 cum. examples 127289, speed 22662.17 words/sec, time elapsed 648.79 sec\n",
            "epoch 13, iter 2000, cum. loss 129.04, cum. ppl 10700.96 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c869f69592e466dba1eb68eb06aa6dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 2000, dev. ppl 10700.690941, bleu_score 18.117572\n",
            "epoch 13, iter 2100, avg. loss 129.21, avg. ppl 10702.13 cum. examples 25363, speed 2777.43 words/sec, time elapsed 775.97 sec\n",
            "epoch 14, iter 2200, avg. loss 129.20, avg. ppl 10699.67 cum. examples 50963, speed 22647.78 words/sec, time elapsed 791.71 sec\n",
            "epoch 15, iter 2300, avg. loss 129.29, avg. ppl 10701.85 cum. examples 76326, speed 21810.06 words/sec, time elapsed 807.91 sec\n",
            "epoch 15, iter 2400, avg. loss 128.72, avg. ppl 10701.96 cum. examples 101689, speed 22552.35 words/sec, time elapsed 823.52 sec\n",
            "epoch 16, iter 2500, avg. loss 128.81, avg. ppl 10703.36 cum. examples 127289, speed 22224.01 words/sec, time elapsed 839.51 sec\n",
            "epoch 16, iter 2500, cum. loss 129.05, cum. ppl 10701.79 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92c3778ae62b4a50a5dee5e9e6b8e61f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 2500, dev. ppl 10700.690941, bleu_score 18.117572\n",
            "epoch 17, iter 2600, avg. loss 129.69, avg. ppl 10699.48 cum. examples 25363, speed 2793.06 words/sec, time elapsed 966.44 sec\n",
            "epoch 17, iter 2700, avg. loss 128.72, avg. ppl 10700.62 cum. examples 50726, speed 22853.54 words/sec, time elapsed 981.83 sec\n",
            "epoch 18, iter 2800, avg. loss 128.24, avg. ppl 10701.41 cum. examples 76326, speed 22712.77 words/sec, time elapsed 997.41 sec\n",
            "epoch 19, iter 2900, avg. loss 129.67, avg. ppl 10701.40 cum. examples 101689, speed 22534.76 words/sec, time elapsed 1013.14 sec\n",
            "epoch 19, iter 3000, avg. loss 129.31, avg. ppl 10700.65 cum. examples 127052, speed 21721.63 words/sec, time elapsed 1029.42 sec\n",
            "epoch 19, iter 3000, cum. loss 129.12, cum. ppl 10700.71 cum. examples 127052\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33ba8b7921c6491c9b015b0875bff821",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 3000, dev. ppl 10700.690941, bleu_score 18.117572\n",
            "epoch 20, iter 3100, avg. loss 129.36, avg. ppl 10701.59 cum. examples 25600, speed 2817.72 words/sec, time elapsed 1156.09 sec\n",
            "epoch 21, iter 3200, avg. loss 128.24, avg. ppl 10698.79 cum. examples 50963, speed 22805.74 words/sec, time elapsed 1171.46 sec\n",
            "epoch 21, iter 3300, avg. loss 129.61, avg. ppl 10703.05 cum. examples 76326, speed 22195.86 words/sec, time elapsed 1187.43 sec\n",
            "epoch 22, iter 3400, avg. loss 128.64, avg. ppl 10699.52 cum. examples 101926, speed 23145.55 words/sec, time elapsed 1202.76 sec\n",
            "epoch 23, iter 3500, avg. loss 129.45, avg. ppl 10700.90 cum. examples 127289, speed 22749.02 words/sec, time elapsed 1218.32 sec\n",
            "epoch 23, iter 3500, cum. loss 129.06, cum. ppl 10700.78 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c7b4d5cbade4d03a415f06d69d3dd55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 3500, dev. ppl 10700.690941, bleu_score 18.117572\n",
            "epoch 23, iter 3600, avg. loss 129.12, avg. ppl 10703.05 cum. examples 25363, speed 2779.72 words/sec, time elapsed 1345.30 sec\n",
            "epoch 24, iter 3700, avg. loss 129.23, avg. ppl 10702.55 cum. examples 50963, speed 22449.09 words/sec, time elapsed 1361.18 sec\n",
            "epoch 25, iter 3800, avg. loss 128.99, avg. ppl 10696.72 cum. examples 76326, speed 22452.67 words/sec, time elapsed 1376.89 sec\n",
            "epoch 25, iter 3900, avg. loss 128.99, avg. ppl 10704.17 cum. examples 101689, speed 22466.22 words/sec, time elapsed 1392.58 sec\n",
            "epoch 26, iter 4000, avg. loss 128.77, avg. ppl 10700.84 cum. examples 127289, speed 22208.13 words/sec, time elapsed 1408.58 sec\n",
            "epoch 26, iter 4000, cum. loss 129.02, cum. ppl 10701.47 cum. examples 127289\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "282d66fc231c42249de8ed693df0bb2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 4000, dev. ppl 10700.690941, bleu_score 18.117572\n",
            "epoch 27, iter 4100, avg. loss 129.09, avg. ppl 10699.34 cum. examples 25363, speed 2783.54 words/sec, time elapsed 1535.36 sec\n",
            "epoch 27, iter 4200, avg. loss 129.36, avg. ppl 10703.28 cum. examples 50726, speed 22413.61 words/sec, time elapsed 1551.14 sec\n",
            "epoch 28, iter 4300, avg. loss 129.45, avg. ppl 10699.47 cum. examples 76326, speed 22476.90 words/sec, time elapsed 1567.03 sec\n",
            "epoch 29, iter 4400, avg. loss 128.94, avg. ppl 10704.43 cum. examples 101689, speed 22136.06 words/sec, time elapsed 1582.95 sec\n",
            "epoch 29, iter 4500, avg. loss 128.81, avg. ppl 10699.58 cum. examples 127052, speed 22537.99 words/sec, time elapsed 1598.58 sec\n",
            "epoch 29, iter 4500, cum. loss 129.13, cum. ppl 10701.22 cum. examples 127052\n",
            "begin validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a0bb30320e0434dbfbb562ef281bbe9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation: iter 4500, dev. ppl 10700.690941, bleu_score 18.117572\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define each of the variables then you can run this command!\n",
        "wandb.init(project = 'nlp-p3-demo', entity = \"chipmunkez\", reinit = True)\n",
        "wandb.watch(baseline_nmt) #not necessary but will help you track gradients\n",
        "train_and_evaluate(\n",
        "    baseline_nmt,\n",
        "    train_data,\n",
        "    val_data,\n",
        "    optimizer,\n",
        "    config[\"epochs\"],\n",
        "    config[\"train_batch_size\"],\n",
        "    config[\"clip_grad\"],\n",
        "    log_every,\n",
        "    valid_niter,\n",
        "    model_save_path\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbwLS9lEGLJU"
      },
      "outputs": [],
      "source": [
        "both_mods_nmt_1 = NMT(\n",
        "    config[\"embed_size\"],\n",
        "    config[\"hidden_size\"],\n",
        "    src_vocab,\n",
        "    tgt_vocab,\n",
        "    device=device,\n",
        "    pretrained_source=src_embed_vectors, \n",
        "    pretrained_target=tgt_embed_vectors, \n",
        "    LSTM_RNN = 'LSTM'\n",
        ")\n",
        "both_mods_nmt_1.to(device)\n",
        "both_mods_nmt_1.train()\n",
        "optimizer = torch.optim.Adam(both_mods_nmt_1.parameters(), lr=1e-3)\n",
        "# Define each of the variables then you can run this command!\n",
        "wandb.init(project = 'nlp-p3-demo', entity = \"chipmunkez\", reinit = True)\n",
        "wandb.watch(both_mods_nmt_1) #not necessary but will help you track gradients\n",
        "train_and_evaluate(\n",
        "    both_mods_nmt_1,\n",
        "    train_data,\n",
        "    val_data,\n",
        "    optimizer,\n",
        "    config[\"epochs\"],\n",
        "    config[\"train_batch_size\"],\n",
        "    config[\"clip_grad\"],\n",
        "    log_every,\n",
        "    valid_niter,\n",
        "    model_save_path\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CiPPaO-0Dmy"
      },
      "source": [
        "### 2.1.5 Report\n",
        "Describe variants in the ablation style described, report the results, and then perform a nuanced analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81c2YMpY3MYH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACmdw-pyh4Lj"
      },
      "source": [
        "# Part 3: Questions\n",
        "In **Part 3**, you will need to answer the three questions below. We expect answers to be to-the-point; answers that are vague, meandering, or imprecise **will receive fewer points** than a precise but partially correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juQW_iF_ETtg"
      },
      "source": [
        "## 3.1 Q1\n",
        "Earlier in the course, we studied models that make use of _Markov_ assumptions. Recurrent neural networks do not make any such assumption. That said, RNNs are known to struggle with long-distance dependencies. What is a fundamental reason for why this is the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIZAmdtoVpBK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CmtdmOWEboE"
      },
      "source": [
        "## 3.2 Q2\n",
        "In applying RNNs to tasks in NLP, we have discovered that (at least for tasks in English) feeding a sentence into an RNN backwards (i.e. inputting the sequence of vectors corresponding to ($course$, $great$, $a$, $is$, $NLP$) instead of ($NLP$, $is$, $a$, $great$, $course$)) tends to improve performance. Why might this be the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i69IrRd4Vpdh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB6y4__iEk6_"
      },
      "source": [
        "## 3.3 Q3\n",
        "In using RNNs and word embeddings for NLP tasks, we are no longer required to engineer specific features that are useful for the task; the model discovers them automatically. Stated differently, it seems that neural models tend to discover better features than human researchers can directly specify. This comes at the cost of systems having to consume tremendous amounts of data to learn these kinds of patterns from the data. Beyond concerns of dataset size (and the computational resources required to process and train using this data as well as the further environmental harm that results from this process), why might we disfavor RNN models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOdh784UVp9f"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD0pgD0Jh84T"
      },
      "source": [
        "# Part 4: Miscellaneous\n",
        "List the libraries you used and sources you referenced and cited (labelled with the section in which you referred to them). Include a description of how your group split\n",
        "up the work. Include brief feedback on this asignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snL2qqbR3SbN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VeQ8uBwiR-a"
      },
      "source": [
        "**Each section must be clearly labelled, complete, and the corresponding pages should be correctly assigned to the corresponding Gradescope rubric item.** If you follow these steps for each of the 4 components requested, you are guaranteed full credit for this section. Otherwise, you will receive no credit for this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqPxplOuZ876"
      },
      "source": [
        "# Part 5: Gradescope Submission\n",
        "\n",
        "Note: This section is not required however we will have a Gradescope submission open to submit predictions and see how your models compare against one another!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5wMEwKX_4Hs"
      },
      "outputs": [],
      "source": [
        "# Create Gradescope submission function\n",
        "gradescope_model = None\n",
        "nmt_document_preprocessor = lambda x: nltk.word_tokenize(x) # This is for your RNN\n",
        "file_name = \"submission.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFc6rb5y8ScE"
      },
      "outputs": [],
      "source": [
        "def generate_submission(filename, model, document_preprocessor, test):\n",
        "    with Path(filename).open(\"w\") as fp:\n",
        "        fp.write(\"Id\\tPredicted\\n\")\n",
        "        for idx, input_string in tqdm(enumerate(test), total=len(test)):\n",
        "            translation = untokenize(\n",
        "                model.beam_search(\n",
        "                    document_preprocessor(input_string),\n",
        "                    beam_size=16,\n",
        "                    max_decoding_time_step=len(input_string)+10\n",
        "                )[0].value)\n",
        "            fp.write(f\"{idx}\\t{translation}\\n\")\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMRiusKQ4UpF"
      },
      "outputs": [],
      "source": [
        "with open(test_path) as fp:\n",
        "    test = [line for line in fp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu2n9Kn9-c1U"
      },
      "outputs": [],
      "source": [
        "generate_submission(file_name, gradescope_model, nmt_document_preprocessor, test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nu4zL4nWnGB"
      },
      "source": [
        "# Live running demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4qO4HFGI92-5"
      },
      "outputs": [],
      "source": [
        "#@title Translation\n",
        "#@markdown Enter a sentence to see the translation\n",
        "input_string = \"\" #@param {type:\"string\"}\n",
        "model_type = \"both_mods_nmt\" #@param [\"baseline_nmt\", \"mod_a_nmt\", \"mod_b_nmt\", \"both_mods_nmt\"]\n",
        "from IPython.display import HTML\n",
        "\n",
        "import re\n",
        "def untokenize(words):\n",
        "    \"\"\"\n",
        "    Untokenizing a text undoes the tokenizing operation, restoring\n",
        "    punctuation and spaces to the places that people expect them to be.\n",
        "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
        "    except for line breaks.\n",
        "    \"\"\"\n",
        "    text = ' '.join(words)\n",
        "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
        "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
        "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
        "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
        "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
        "         \"can not\", \"cannot\")\n",
        "    step6 = step5.replace(\" ` \", \" '\")\n",
        "    return step6.strip()\n",
        "\n",
        "output = \"\"\n",
        "\n",
        "# BAD THING TO DO BELOW!!\n",
        "model_used = globals()[model_type]\n",
        "\n",
        "with torch.no_grad():\n",
        "    # RUN MODEL\n",
        "    translation = untokenize(model_used.beam_search(\n",
        "        nmt_document_preprocessor(input_string),\n",
        "        beam_size=64,\n",
        "        max_decoding_time_step=len(input_string)+10\n",
        "    )[0].value)\n",
        "\n",
        "# Generate nice display\n",
        "output += '<p style=\"font-family:verdana; font-size:110%;\">'\n",
        "output += \" Input sequence: \"+input_string+\"</p>\"\n",
        "output += '<p style=\"font-family:verdana; font-size:110%;\">'\n",
        "output += f\" Translation to Shakespeare: {translation}</p><hr>\"\n",
        "output = \"<h3>Results:</h3>\" + output\n",
        "\n",
        "display(HTML(output))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Combined LSTM of 4740_FA21_p3_part2_release.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "36e5a7a8a2344f42994a8091a7d0ea00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab83b8d512904a3dbea443345fd94838",
              "IPY_MODEL_5362fb6186ec43ddbd94c0fecc6afe44"
            ],
            "layout": "IPY_MODEL_bf605e3617c942a8a9631e0114cceb2d"
          }
        },
        "5362fb6186ec43ddbd94c0fecc6afe44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f713782da7c14585b7c6e47b46af8c33",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59ef3d5c41574d3daf8d20f6e3ea7852",
            "value": 1
          }
        },
        "59ef3d5c41574d3daf8d20f6e3ea7852": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab83b8d512904a3dbea443345fd94838": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed2fafb760444b58a929019095edae2a",
            "placeholder": "​",
            "style": "IPY_MODEL_c2be6291459746ea95b3483d041b767e",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r"
          }
        },
        "bf605e3617c942a8a9631e0114cceb2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2be6291459746ea95b3483d041b767e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed2fafb760444b58a929019095edae2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f713782da7c14585b7c6e47b46af8c33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}