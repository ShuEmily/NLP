{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNdFlbLJoUXM"
   },
   "source": [
    "# Project 2: Named Entity Recognition (NER) with Sequence Labeling Models\n",
    "## CS4740/5740 Fall 2021\n",
    "\n",
    "### Project Submission Due: Oct 15th, 2021 (11.59PM)\n",
    "Please submit **pdf file** of this notebook on **Gradescope**, and **ipynb** on **CMS**. For instructions on generating pdf and ipynb files, please refer to project 1 instructions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "871f2XhgtoYX"
   },
   "source": [
    "**Names:** Liangchen Li, Shuhui Zhu\n",
    "\n",
    "**Netids:** ll924, sz649\n",
    "\n",
    "Don't forget to share your newly copied notebook with your partner!\n",
    "\n",
    "\n",
    "**Reminder: both of you can't work in this notebook at the same time from different computers/browser windows because of sync issues. We even suggest to close the tab with this notebook when you are not working on it so your partner doesn't get sync issues.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iguUiw0mor52"
   },
   "source": [
    "# **Introduction** ðŸ”Ž\n",
    "\n",
    "---\n",
    "\n",
    "In this project, you will implement a model that identifies named entities in text and tags them with the appropriate label. Particularly, the task of this project is **Named Entity Recognition**. A primer on this task is provided further on. The given dataset is a modified version of the CoNLL-2003 ([Sang et al](https://arxiv.org/pdf/cs/0306050v1.pdf)) dataset. Please use the datasets that we have released to you instead of versions found online as we have made simplifications to the dataset for your benefit. Your task is to develop NLP models to identify these named entities automatically. We will treat this as a **sequence-tagging task**: for each token in the input text, assign one of the following 5 labels: **ORG** (Organization), **PER** (Person), **LOC** (Location), **MISC** (Miscellaneous), and **O** (Not Named Entity). More information about the dataset is provided later\n",
    "\n",
    "For this project, you will implement two sequence labeling approaches:\n",
    "- Model 1 : a Hidden Markov Model (HMM)\n",
    "- Model 2 : a Maximum Entropy Markov Model (MEMM), which is an adaptation of an HMM in which a Logistic Regression classifier (also known as a MaxEnt classifier) is used to obtain the lexical generation probabilities (i.e., the observation/emission probability matrix, so \"observations\" == \"emissions\" == \"lexical generations\"). Feature engineering is strongly suggested for this model!\n",
    "\n",
    "Implementation of the Viterbi algorithm (for finding the most likely tag sequence to assign to an input text) is required for both models above, so make sure that you understand it ASAP.\n",
    "\n",
    "You will implement and train two sequence tagging models, generate your predictions for the provided test set, and submit them to **Kaggle**. Please enter all code in this colab notebook and answer all the questions in the supporting document.\n",
    "\n",
    "To refresh your memory on HMMs, MEMMs, and Viterbi you can refer to **Jurafsky & Martin Ch. 8.3â€“8.5** and the lecture slides which can be found on EdStem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mib4pTXj3hir"
   },
   "source": [
    "## **Logistics**\n",
    "\n",
    "---\n",
    "\n",
    "- You **must** work in **groups of 2 students**. Students in the same group will get the same grade. Thus, you should make sure that everyone in your group contributes to the project. \n",
    "- **Remember to form groups on BOTH CMS and Gradescope** or not all group members will receive grades. You can use make a post on EdStem to find a partner for this project.\n",
    "- Please complete the written questions of this notebook in a clear and informative way. We have created a template document for you to answer the written questions. This document can be found [here](https://docs.google.com/document/d/1vnxYFS-rxxLOYfKG6YN35YktJZnzqQBhE1Mj1Xu7IF0/edit?usp=sharing). Please make a copy of this document for yourself and add your names and netids in the header and answer the written questions on it. You will need to submit this document to gradescope as well (do not forget to do this please!).\n",
    "- At the end: please make sure to submit the following 3 items:\n",
    "  1. PDF version of Colab notebook on Gradescope (instructions for converting to PDF are at the end).\n",
    "  2. PDF version of Google Doc with written answers to the numbered questions on this colab on Gradescope.\n",
    "  3. .ipynb version of your colab notebook on CMS.\n",
    "\n",
    "- Note: When submitting the PDF documents to Gradescope (colab notebook & writeup doc) please join/concatenate the PDFs and then submit them as one. You may do this any way you please. You can use [this](https://pdfjoiner.com/) website if you wish to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2z7TIHV3kCH"
   },
   "source": [
    "## **Advice**\n",
    "\n",
    "---\n",
    "\n",
    "1. Please read through the entire notebook before you start coding. That might inform your code structure.\n",
    "2. Grading breakdown is found at the end; please consult it.\n",
    "3. Google colab does **not** provide good synchronization; we do not recommend multiple people to work on the same notebook at the same time.\n",
    "4. The project is somewhat open ended. (\"But that's a good thing.  Really. It's more fun that way\", says Claire.) We will ask you to implement some model, but precise data structures and so on can be chosen by you. However, to integrate with Kaggle, you will need to submit Kaggle predictions using the given evaluation code (more instructions later).\n",
    "5. You will be asked to fill in your code at various points of the document. You will also be asked to answer questions that analyze your results and motivate your implementation. Please answer these on an additional writeup document. A template has been provided to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBsjyfPu5V4t"
   },
   "source": [
    "## **Named Entity Recognition: A Primer**\n",
    "\n",
    "---\n",
    "\n",
    "Let us now take a look at the task at hand: Named Entity Recognition (NER). This section provides a brief introduction to the task and why it is important.\n",
    "\n",
    "**What is NER?**\n",
    "NER refers to the information extraction technique of identifying and categorizing key information about entities within textual data. Let's look at an example: \n",
    "\n",
    "<br/>\n",
    "\n",
    "![picture](https://drive.google.com/uc?id=1mxwn1_2Ef16_MJeyl9jJwwR6IohUOeHO)\n",
    "\n",
    "<br/>\n",
    "\n",
    "In the above example, we can see that the text has numerous named entities that can be categorized as LOC (location), ORG (organization), PER (person), etc. Today, the task of NER has been overwhelmed by deep learning approaches. However, for this assignment, we will try to do NER using something simpler: HMMs and MEMMs. NER is important for a number of reasons and has a wide variety of use cases such as but not limited to:\n",
    "  - Detect entities in search engines and voice assistants for more relavent search results.\n",
    "  - Automatically parsing resumes.\n",
    "  - ...and many more!\n",
    "\n",
    "\n",
    "To read more on NER, we refer to any of the following sources:\n",
    "1. Medium post [1](https://umagunturi789.medium.com/everything-you-need-to-know-about-named-entity-recognition-2a136f38c08f) and [2](https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d).\n",
    "2. Try out [this](https://demo.allennlp.org/named-entity-recognition/named-entity-recognition) AlllenNLP demo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSFfegs8LKY8"
   },
   "source": [
    "## **Entity Level Mean F1**\n",
    "\n",
    "---\n",
    "\n",
    "Let's take a look at the metrics that you will focus on in this assignment. The standard measures to report for NER are recall, precision, and F1 score\n",
    "(also called F-measure) evaluated at the **named entity level** (not at the token level). The code for this has been provided later under the validation section under Part 2. Please use this code when evaluating your models. \n",
    "\n",
    "\n",
    "If P and T are the sets of predicted and true *named entity spans*, respectively, (e.g, the five named entity spans in the above example are \"Zifa\", \"Renate Goetschl\", \"Austria\", \"World Cup\", and \"Germany\") then\n",
    "\n",
    "####<center>Precision = $\\frac{|\\text{P}\\;\\cap\\;\\text{C}|}{|\\text{C}|}$ and Recall = $\\frac{|\\text{P}\\;\\cap\\;\\text{C}|}{|\\text{P}|}$.</center><br/>\n",
    "\n",
    "\n",
    "####<center>F1 = $\\frac{2 * \\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$. </center><br/>\n",
    "\n",
    "For each type of named entity, e.g. *LOC*ation, *MISC*ellaneous, *ORG*anization and *PER*son, we calculate the F1 score as shown above, and take the mean of all these F1 scores to get the **Entity Level Mean F1** score for the test set. If $N$ is the total number of labels (i.e., named entity types), then\n",
    "\n",
    "####<center>Entity Level Mean F1 = $\\frac{\\sum_{i = 1}^{N} \\text{F1}_{{label}_i}}{N}$. </center>\n",
    "\n",
    "More details under the validation section in Part 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iP63fHj5saG"
   },
   "source": [
    "# **Part 1: Dataset** ðŸ“ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pljkH2ow5U9x"
   },
   "source": [
    "Load the dataset as follows:\n",
    "  1. Obtain the data from Kaggle at https://www.kaggle.com/c/cs4740-fa21-p2/data.\n",
    "  2. Unzip the data. Put it into your google drive, and mount it on colab as per below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3dQChuccqfN",
    "outputId": "44f9c7bc-f0ee-4f0b-c401-6fe44722f7be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOB4ucaK1u8f"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFXI7NRHn1Cc"
   },
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"CS_4740_FA21_p2\")\n",
    "\n",
    "with open(os.path.join(path,'train.json'), 'r') as f:\n",
    "     train = json.loads(f.read())\n",
    "\n",
    "with open(os.path.join(path,'test.json'), 'r') as f:\n",
    "     test = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfjKFeE_7T7C"
   },
   "source": [
    "Here's a few things to note about the dataset above:\n",
    "1. We have just loaded 2 json files: train and test. Please note that these files are different from the original release of the CoNNL-2003 since we have already processed and tokenized them for you. Hence, the documents are represented as a list of strings. Note that it is **not** split into separate training and development/validation sets. You will need to do this yourself as needed using the train set.\n",
    "2. The train file contains the following 4 fields (each is a nested list): \n",
    "  - **'text'** - actual input tokens\n",
    "  - **'NER'** - the token-level entity tag (ORG/PER/LOC/MISC/O) where **O is used to denote tokens that are not part of any named entity**\n",
    "  - **'POS'** - the part of speech tag (will be handy for feature engineering of the MEMM model)\n",
    "  - **'index'** - index of the token in the dataset\n",
    "3. The test data only has 'text', 'POS' and 'index' fields. You will need to submit your prediction of the 'NER' tag to Kaggle. More instructions on this later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cradDk-37G8L"
   },
   "source": [
    "Let's take a look at a sample sentence from the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwcyAkEeAIJj"
   },
   "source": [
    "As you can see, the above the sentence, \"Romania state budget soars in June.\", has already been tokenized into an array of word tokens. The index array corresponds to the index of the token in the entire dataset (not the sentence). The POS tags and the NER tags correspond to the given indices. For example, the token: **Romania** has:\n",
    "  - index: 0\n",
    "  - POS: 'NNP'\n",
    "  - NER: **'ORG'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO7go9ivDusU"
   },
   "source": [
    "### **Q1: Initial Data Observations**\n",
    "What are your initial observations after you explore the dataset?  Provide some quantitative data exploration. Assess dataset size, document lengths and the token-level NER class distribution, and the entity-level NER class distribution (skipping the 'O' label for the latter). Give some examples of sentences with their named entities bracketed, e.g. [[LOC Romania] state budget soars in June .] and [[ORG Zifa] said [PER Renate Goetschl] of [LOC Austria]...]. \n",
    "\n",
    "Present your findings in the supporting template document!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q_2Lh41XB57"
   },
   "source": [
    "#### **Assess Datset Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0PzewjgRiia",
    "outputId": "1199e696-cdf1-470f-f56e-7bb3a56006a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sentence amount :  756\n",
      "test sentence amount :  189\n"
     ]
    }
   ],
   "source": [
    "print(\"train sentence amount : \",len(train['text']))\n",
    "print(\"test sentence amount : \",len(test['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm5XhqekWMsI"
   },
   "source": [
    "#### **Chcek Sentence Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCyTEDxtTIMV"
   },
   "outputs": [],
   "source": [
    "# def lenth\n",
    "length_list = []\n",
    "for sentence in train['text']:\n",
    "  lenSen = len(sentence)\n",
    "  length_list.append(lenSen)\n",
    "\n",
    "less_400 = []\n",
    "for i in length_list:\n",
    "  if i<= 400:\n",
    "    less_400.append(True)\n",
    "  else:\n",
    "    less_400.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "204HFi5-c3lH",
    "outputId": "37a2ffa4-e70c-471e-a2f7-ee03b452f136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest length is : 24\n",
      "The longest length is : 1335\n",
      "There are 673 sentences having tokens less than 400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# copy this\n",
    "plt.title(\"Train data sentence length distribution\")\n",
    "plt.hist(length_list)\n",
    "print(\"The shortest length is :\", min(length_list))\n",
    "print(\"The longest length is :\", max(length_list))\n",
    "print(\"There are {} sentences having tokens less than 400\".format(sum(less_400)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfqPNo-agHBW"
   },
   "outputs": [],
   "source": [
    "length_list = []\n",
    "for sentence in test['text']:\n",
    "  lenSen = len(sentence)\n",
    "  length_list.append(lenSen)\n",
    "\n",
    "less_400 = []\n",
    "for i in length_list:\n",
    "  if i<= 400:\n",
    "    less_400.append(True)\n",
    "  else:\n",
    "    less_400.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "lPSXWiVlgKro",
    "outputId": "37617e9c-2c9a-461d-b6e6-c7f92f2d87cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest length is : 24\n",
      "The longest length is : 958\n",
      "There are 167 sentences having tokens less than 400\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEICAYAAAC+iFRkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXHklEQVR4nO3de5BmdX3n8fdHhotcBEZGHEAcFBbFXRWdQlgvQVCDlxVq1/ISS0dDlrhlsrKaUtTKignWQlUimjJBUVTiBUQigrirwRGTdWPQIaLhuoxc5D4DgihmVeS7f5xfm4e2e7qnn37oYX7vV9VTfc75neec77n05znP7zxPd6oKSdLW7RFLXYAkafIMe0nqgGEvSR0w7CWpA4a9JHXAsJekDhj2W4EkNyR5wVLXsbVJUkn2X4L1Hp7k5jGef2KST7fhfZP8NMk2i1Tbh5P88WLUOcOyn5vkmsVanh7MsB9T+0WaejyQ5F9Gxl+7gOV9I8nvTaLWtvwlCbBNGQ2nHk3ymFTVD6tq56r61Rw1vCHJN+exvDdV1Z8uRm3Tt7uq/ndVHbgYy9ZvWrbUBTzcVdXOU8NJbgB+r6q+tnQVSZORZJu5XjS05fLKfkKSPCLJCUl+kOSuJOckWd7adkjy6Tb9niTfSbJnkvcBzwU+1N4ZfGiWZb8uyY3t+e+e1nZIkm+15d6W5ENJtmttf99m+15b/quS7J7kwiQbk9zdhvfZxHa9I8ktSX6S5JokR85je1e1q7g1SX6Y5M6pupMcBbwLeFWr6Xtt+q5JzmjbcEuSk6a6IqauQpP8Wav5+iQvHqlxeZJPJLm1tX9xpO1lSS5r++cfkjx1nsdz+7a+Hya5o3VnPLK1HZ7k5iRvS7Kh1fzGkec+OsmXktzbjvVJU1fRMx2TkefNuLwZatsvyd+1Y3IRsMdI29S+Xzay765r816f5LVJngx8GDis1XBPm/eTSU5L8j+T3Ac8v007adr639WO6Q0ZeTebae9SM/LuYZZz8UHdQkme3JZxT5Irkrx8pO2TSf4yyZfbtlyS5IlzHceuVZWPRXoANwAvaMNvAf4R2AfYHvgIcFZr+33gS8COwDbAM4FHtbZvMLw7mG0dBwE/BZ7Xlvt+4P6R9T4TOJThXdsq4Crg+JHnF7D/yPijgf/UatkF+DzwxVnWfSBwE7BXG18FPHEe27uqrfejwCOBpwE/B57c2k8EPj1tXee1ZewEPAb4NvD7re0NwC+B/9z2338BbgXS2r8MfA7YHdgW+K02/WBgA/Cs9rw17ZhtP8v2/npfAacCFwDL2376EvA/Wtvh7Rj8SVvfS4CfAbu39rPbY8d2/G4CvrmJY7LJ5c1Q57faebB9Oy9+MrU/R/b9srYv7wUObG0rgaeM7NNvTlvuJ4EfA89muDDcoU07aVqdU+v+LeC+keV/g5Fzefo6Ztnum9vwtsB6hguB7YAj2nYdOFLbXcAhbds+A5y91BmwJT+WvICt6cGDw/4q4MiRtpUMAbUM+F3gH4CnzrCMB/2CzND+30dP6vYL/Iup9c4w//HAeSPjD/oFm2H+pwN3z9K2P0NYvgDYdlrbprZ3KnD2GWn/NvDqNnwiI2EP7MnwYvDIkWmvAS5uw28A1o+07diW/9i23geYIRiB04A/nTbtGtqLwQzzV9vmtBB74kjbYcD1bfhw4F+AZSPtGxhedLdp++HAkbaT5hF6My5vhhr3ZQjcnUamfZbZw/4ehhf3R05bzhuYOez/eoZp08N+dN3nAH8807k8fR2zbPdU2D8XuB14xEj7WcCJI3V8bKTtJcDVi/07vTU97LOfnMcD5yV5YGTarxiC7FPA44Czk+wGfBp4d1X9ch7L3YvhyhCAqrovyV1T40n+DcOV1mqGEFwGXDrbwpLsyHDVehTDlTDALpmhf7aq1ic5niGcn5Lkq8Bbq+rWObZ3yu0jwz8DdmZmj2e4srstydS0R4xu9+iyqupnbb6dGa68f1RVd8+y3DVJ/nBk2nYM+3RTVjDsy0tH6glDkE+5q6ruHxmf2r4VDMdgtPbR4dnMtrzp9mJ4cb5vZNqNDOfXg7Rz5VXAHwFnJPk/wNuq6upN1DFXrTOte679OR97ATdV1ej5dCOw98j4fM8nYZ/9JN0EvLiqdht57FBVt1TVL6vqvVV1EPDvgZcBr2/Pm+vPkN7GyC9yC+tHj7SfBlwNHFBVj2J4Gxxm9zaG7plntfmfN7XomWauqs9W1XMYgrOAU+ba3jm2B35zm29iuLLfY2RZj6qqp8xjWTcBy9uL6Ext75tW445VddYcy7yT4Ur7KSPP27VGbs5vwkaGq9/R+yC/EcRjuA3YPclOI9P2nW3mqvpqVb2Q4R3Q1QxdazD7eTfX+TjTum9tw/cxvEhOeewcyxp1K/C4JKMZtS8wn/NJMzDsJ+fDwPuSPB4gyYokR7fh5yf5d+2G470Mb/OnrmDuAJ6wieWeC7wsyXMy3Hj9Ex58HHdpy/xpkicx9GePmr78XRiC7J4MN1TfM9uKkxyY5Igk2wP/rz1vqu5Zt3ce7gBWTf1iV9VtwN8Cf57kURlu/j4xyW/NtaD23P8F/FWGm8/bJpl6Afso8KYkz8pgpyQvTbLLHMt8oD331CSPadu3d5Lfnkc9vwK+AJyYZMd2TF4/bba5jvmmln8jsA54b5LtkjwH+A8zzZvhQwBHt3D+OcO9n9Hzbp92Tm2uqXU/l+HC5fNt+mXAf2zbvT9w7LTnbWq7L2G4Wn97O4aHt+06ewH1CcN+kj7IcEPvb5P8hOHm5bNa22MZQvtehr7uv2Po2pl63isyfIrkL6YvtKquAN7M0C97G3A3MPrFlj8CfofhZtZHGW5UjjoROLN9wuGVwAcYbpre2Wr8yia2aXvg5Dbv7Qw3Tt85j+2dy1Q43JXkn9rw6xm6WK5s23guw9XofLyO4QX0aoa+7uMBqmodw03dD7VlrmfoR56Pd7T5/zHJvcDXGN4RzccfALsy7LNPMfQ9/3yk/UQefEw21+8w7OsfMbxY//Us8z0CeCvDVfOPGG6oTl0MfB24Arg9yZ2bse7bGfblrQw3Sd800i10KsP9pDuAM1v7qBOZZbur6hcM4f5ihvPtr4DXz9HlpE2Y+vSCpIdIklOAx1bVmqWuRf3wyl6asCRPSvLU1nV0CEN3xnlLXZf64qdxpMnbhaHrZi+GLo0/B85f0orUHbtxJKkDduNIUgce0m6cPfbYo1atWvVQrlKSHvYuvfTSO6tqxTjLeEjDftWqVaxbt+6hXKUkPewluXHcZdiNI0kdMOwlqQOGvSR1wLCXpA4Y9pLUAcNekjpg2EtSBwx7SeqAYS9JHfCvXs5h1QlfXrJ133DyS5ds3ZK2Ll7ZS1IHDHtJ6sC8wj7JbknOTXJ1kquSHJZkeZKLklzbfu4+6WIlSQsz3yv7DwJfqaonAU9j+CfZJwBrq+oAYG0blyRtgeYM+yS7As8DzoDhv75X1T3A0Qz/MZ7285hJFSlJGs98ruz3AzYCn0jy3SQfS7ITsGdV3dbmuR3Yc6YnJzkuybok6zZu3Lg4VUuSNst8wn4Z8AzgtKo6GLiPaV02Nfwj2xn/mW1VnV5Vq6tq9YoVY/2jFUnSAs0n7G8Gbq6qS9r4uQzhf0eSlQDt54bJlChJGtecYV9VtwM3JTmwTToSuBK4AFjTpq0Bzp9IhZKksc33G7R/CHwmyXbAdcAbGV4ozklyLHAj8MrJlChJGte8wr6qLgNWz9B05OKWI0maBL9BK0kdMOwlqQOGvSR1wLCXpA4Y9pLUAcNekjpg2EtSBwx7SeqAYS9JHTDsJakDhr0kdcCwl6QOGPaS1AHDXpI6YNhLUgcMe0nqgGEvSR0w7CWpA4a9JHXAsJekDhj2ktQBw16SOmDYS1IHDHtJ6oBhL0kdWDafmZLcAPwE+BVwf1WtTrIc+BywCrgBeGVV3T2ZMiVJ49icK/vnV9XTq2p1Gz8BWFtVBwBr27gkaQs0TjfO0cCZbfhM4Jjxy5EkTUKqau6ZkuuBu4ECPlJVpye5p6p2a+0B7p4an/bc44DjAPbdd99n3njjjQsqdNUJX17Q87T5bjj5pUtdgqQRSS4d6VVZkHn12QPPqapbkjwGuCjJ1aONVVVJZnzVqKrTgdMBVq9ePfcriyRp0c2rG6eqbmk/NwDnAYcAdyRZCdB+bphUkZKk8cwZ9kl2SrLL1DDwIuBy4AJgTZttDXD+pIqUJI1nPt04ewLnDd3yLAM+W1VfSfId4JwkxwI3Aq+cXJmSpHHMGfZVdR3wtBmm3wUcOYmiJEmLy2/QSlIHDHtJ6oBhL0kdMOwlqQOGvSR1wLCXpA4Y9pLUAcNekjpg2EtSBwx7SeqAYS9JHTDsJakDhr0kdcCwl6QOGPaS1AHDXpI6YNhLUgcMe0nqgGEvSR0w7CWpA4a9JHXAsJekDhj2ktQBw16SOjDvsE+yTZLvJrmwje+X5JIk65N8Lsl2kytTkjSOzbmyfwtw1cj4KcCpVbU/cDdw7GIWJklaPPMK+yT7AC8FPtbGAxwBnNtmORM4ZhIFSpLGN98r+w8AbwceaOOPBu6pqvvb+M3A3jM9MclxSdYlWbdx48axipUkLcycYZ/kZcCGqrp0ISuoqtOranVVrV6xYsVCFiFJGtOyeczzbODlSV4C7AA8CvggsFuSZe3qfh/glsmVKUkax5xX9lX1zqrap6pWAa8Gvl5VrwUuBl7RZlsDnD+xKiVJYxnnc/bvAN6aZD1DH/4Zi1OSJGmxzacb59eq6hvAN9rwdcAhi1+SJGmx+Q1aSeqAYS9JHTDsJakDhr0kdcCwl6QOGPaS1AHDXpI6YNhLUgcMe0nqgGEvSR0w7CWpA4a9JHXAsJekDhj2ktQBw16SOmDYS1IHDHtJ6oBhL0kdMOwlqQOGvSR1wLCXpA4Y9pLUAcNekjpg2EtSB+YM+yQ7JPl2ku8luSLJe9v0/ZJckmR9ks8l2W7y5UqSFmI+V/Y/B46oqqcBTweOSnIocApwalXtD9wNHDu5MiVJ45gz7Gvw0za6bXsUcARwbpt+JnDMRCqUJI1tXn32SbZJchmwAbgI+AFwT1Xd32a5Gdh7MiVKksY1r7Cvql9V1dOBfYBDgCfNdwVJjkuyLsm6jRs3LrBMSdI4NuvTOFV1D3AxcBiwW5JlrWkf4JZZnnN6Va2uqtUrVqwYq1hJ0sLM59M4K5Ls1oYfCbwQuIoh9F/RZlsDnD+pIiVJ41k29yysBM5Msg3Di8M5VXVhkiuBs5OcBHwXOGOCdUqSxjBn2FfV94GDZ5h+HUP/vSRpC+c3aCWpA4a9JHXAsJekDhj2ktQBw16SOmDYS1IHDHtJ6oBhL0kdMOwlqQOGvSR1wLCXpA4Y9pLUAcNekjpg2EtSBwx7SeqAYS9JHTDsJakDhr0kdcCwl6QOGPaS1AHDXpI6YNhLUgcMe0nqgGEvSR0w7CWpA3OGfZLHJbk4yZVJrkjyljZ9eZKLklzbfu4++XIlSQsxnyv7+4G3VdVBwKHAm5McBJwArK2qA4C1bVyStAWaM+yr6raq+qc2/BPgKmBv4GjgzDbbmcAxkypSkjSezeqzT7IKOBi4BNizqm5rTbcDe87ynOOSrEuybuPGjWOUKklaqHmHfZKdgb8Bjq+qe0fbqqqAmul5VXV6Va2uqtUrVqwYq1hJ0sLMK+yTbMsQ9J+pqi+0yXckWdnaVwIbJlOiJGlc8/k0ToAzgKuq6v0jTRcAa9rwGuD8xS9PkrQYls1jnmcDrwP+Ocllbdq7gJOBc5IcC9wIvHIyJUqSxjVn2FfVN4HM0nzk4pYjSZoEv0ErSR0w7CWpA4a9JHXAsJekDhj2ktQBw16SOmDYS1IHDHtJ6oBhL0kdMOwlqQOGvSR1wLCXpA4Y9pLUAcNekjpg2EtSBwx7SeqAYS9JHTDsJakDhr0kdcCwl6QOGPaS1AHDXpI6YNhLUgcMe0nqwJxhn+TjSTYkuXxk2vIkFyW5tv3cfbJlSpLGMZ8r+08CR02bdgKwtqoOANa2cUnSFmrOsK+qvwd+NG3y0cCZbfhM4JhFrkuStIgW2me/Z1Xd1oZvB/acbcYkxyVZl2Tdxo0bF7g6SdI4xr5BW1UF1CbaT6+q1VW1esWKFeOuTpK0AAsN+zuSrARoPzcsXkmSpMW20LC/AFjThtcA5y9OOZKkSZjPRy/PAr4FHJjk5iTHAicDL0xyLfCCNi5J2kItm2uGqnrNLE1HLnItkqQJ8Ru0ktSBOa/spYfSqhO+vCTrveHkly7JeqHPbdZDzyt7SeqAYS9JHTDsJakDhr0kdcCwl6QOGPaS1AHDXpI6YNhLUgcMe0nqgGEvSR0w7CWpA4a9JHXAP4QmdWqp/gAb+EfYloJX9pLUAcNekjpg2EtSB+yz129Yyr7cpdLjNqsvXtlLUgcMe0nqgGEvSR0w7CWpA4a9JHXAT+NI6sZSfepqS/jG8FhX9kmOSnJNkvVJTlisoiRJi2vBYZ9kG+AvgRcDBwGvSXLQYhUmSVo841zZHwKsr6rrquoXwNnA0YtTliRpMY3TZ783cNPI+M3As6bPlOQ44Lg2+tMk18yyvD2AO8eoZ2vgPhi4H7byfZBT5jXbVrMP5rm9M5naB48ft4aJ36CtqtOB0+eaL8m6qlo96Xq2ZO6DgfvBfQDuA1jcfTBON84twONGxvdp0yRJW5hxwv47wAFJ9kuyHfBq4ILFKUuStJgW3I1TVfcn+QPgq8A2wMer6ooxapmzq6cD7oOB+8F9AO4DWMR9kKparGVJkrZQ/rkESeqAYS9JHdgiwr6XP7uQ5HFJLk5yZZIrkrylTV+e5KIk17afu7fpSfIXbb98P8kzlnYLFk+SbZJ8N8mFbXy/JJe0bf1cu+lPku3b+PrWvmop614sSXZLcm6Sq5NcleSw3s6DJP+t/R5cnuSsJDv0cB4k+XiSDUkuH5m22cc+yZo2/7VJ1sy13iUP+87+7ML9wNuq6iDgUODNbVtPANZW1QHA2jYOwz45oD2OA0576EuemLcAV42MnwKcWlX7A3cDx7bpxwJ3t+mntvm2Bh8EvlJVTwKexrAvujkPkuwN/FdgdVX9W4YPebyaPs6DTwJHTZu2Wcc+yXLgPQxfZD0EeM/UC8SsqmpJH8BhwFdHxt8JvHOp63qItv184IXANcDKNm0lcE0b/gjwmpH5fz3fw/nB8J2MtcARwIVAGL4luGz6OcHwaa/D2vCyNl+WehvG3P5dgeunb0dP5wH/+g385e24Xgj8di/nAbAKuHyhxx54DfCRkekPmm+mx5Jf2TPzn13Ye4lqeci0t6EHA5cAe1bVba3pdmDPNry17psPAG8HHmjjjwbuqar72/jodv56H7T2H7f5H872AzYCn2hdWR9LshMdnQdVdQvwZ8APgdsYjuul9HUejNrcY7/Z58SWEPbdSbIz8DfA8VV172hbDS/TW+3nYZO8DNhQVZcudS1LaBnwDOC0qjoYuI9/fdsOdHEe7M7whxP3A/YCduI3uza6NKljvyWEfVd/diHJtgxB/5mq+kKbfEeSla19JbChTd8a982zgZcnuYHhL6UewdB/vVuSqS/5jW7nr/dBa98VuOuhLHgCbgZurqpL2vi5DOHf03nwAuD6qtpYVb8EvsBwbvR0Hoza3GO/2efElhD23fzZhSQBzgCuqqr3jzRdAEzdTV/D0Jc/Nf317Y78ocCPR97qPSxV1Turap+qWsVwrL9eVa8FLgZe0Wabvg+m9s0r2vwP6yveqroduCnJgW3SkcCVdHQeMHTfHJpkx/Z7MbUPujkPptncY/9V4EVJdm/vkl7Ups1uqW9UtOP1EuD/Aj8A3r3U9UxwO5/D8Pbs+8Bl7fEShr7HtcC1wNeA5W3+MHxS6QfAPzN8cmHJt2MR98fhwIVt+AnAt4H1wOeB7dv0Hdr4+tb+hKWue5G2/enAunYufBHYvbfzAHgvcDVwOfApYPsezgPgLIb7FL9keJd37EKOPfC7bX+sB94413r9cwmS1IEtoRtHkjRhhr0kdcCwl6QOGPaS1AHDXpI6YNhLUgcMe0nqwP8H5LY3DIe6a9cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lt.title(\"Test data sentence length distribution\")\n",
    "plt.hist(length_list)\n",
    "print(\"The shortest length is :\", min(length_list))\n",
    "print(\"The longest length is :\", max(length_list))\n",
    "print(\"There are {} sentences having tokens less than 400\".format(sum(less_400)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-m9AZm-XNmR"
   },
   "source": [
    "#### **Level NER Class Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TilQ_J-RWH3T"
   },
   "outputs": [],
   "source": [
    "def flatten(list_of_list):\n",
    "  '''\n",
    "  flatten list of list of token to a single list of token\n",
    "  \n",
    "  '''\n",
    "  flatten = []\n",
    "  for sub_sentence in list_of_list:\n",
    "    flatten = flatten + sub_sentence\n",
    "  return flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu9B5Trv0ii0"
   },
   "outputs": [],
   "source": [
    "# copy this\n",
    "flat_NER = flatten(train['NER'])\n",
    "flat_NER_filtered = []\n",
    "for i in flat_NER:\n",
    "  if i in ['PER', 'ORG', 'LOC', 'MISC']:\n",
    "    flat_NER_filtered.append(i)\n",
    "NER_dict = {\"PER\":0,\"LOC\":0,\"ORG\":0,\"MISC\":0}\n",
    "for i in flat_NER_filtered:\n",
    "  if i == 'PER': NER_dict[\"PER\"]+=1\n",
    "  if i == 'LOC': NER_dict[\"LOC\"]+=1\n",
    "  if i == 'ORG': NER_dict[\"ORG\"]+=1\n",
    "  if i == 'MISC': NER_dict[\"MISC\"]+=1\n",
    "plt.title(\"NER tag distribution\")\n",
    "plt.hist(flat_NER_filtered,density=False,orientation='vertical')\n",
    "print(\"LOC : {}, ORG: {}, MISC : {}, PER : {}\".format(NER_dict[\"LOC\"],NER_dict[\"ORG\"],NER_dict[\"MISC\"],NER_dict[\"PER\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SlLifvobwtR"
   },
   "source": [
    "#### **Give** some examples of sentences with their named entities bracketed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NYvfchqqBf6"
   },
   "source": [
    "# **Part 2: Hidden Markov Model** ðŸ§¨\n",
    "\n",
    "---\n",
    "\n",
    "1. Code for counting and smoothing of labels and words and unkown word handing as necessary to support the Viterbi algorithm. (This is pretty much what you already know how to do from project 1.)\n",
    "2. Build a Hidden Markov Model in accordance with the starter code that has been provided. If you wish to change this starter code you can. However, please ensure that your code is clear, concise, and, most important of all, modular. So break your implementation down into functions or write it within a class. We suggest you compute all probabilities in a log form when building the HMM.\n",
    "3. An implementation of the **Viterbi algorithm** that can be used to infer token-level labels (identifying the appropriate named entity) for an input document. This process is commonly referred to as **decoding**. Bigram-based Viterbi is $ \\mathcal{O}(sm^2)$ where s is the length of the sentence and m is the number of tags. Your implementation should have similar efficiency. The code for this can be used later on for the MEMM too.\n",
    "\n",
    "Code of Academic Integrity:  We encourage collaboration regarding ideas, etc. However, please **do not copy code from online or share code with other students**. We will be running programs to detect plagiarism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jUVJwSaE1tI"
   },
   "source": [
    "## **Unknown Word Handling**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44Bja4eQEMJR"
   },
   "outputs": [],
   "source": [
    "def unknow_train(text, beyound_die = 0.5,seed=1):\n",
    "  '''\n",
    "  This function takes in raw-tokenized text in list of list and returns modified text in list of list with unknow in it.\n",
    "  For unknow word, dividing those only occur once word types into two groups: number-string, word-string. convert all number-string into unkown,\n",
    "  keep word-string if the die is greater than the threshold.\n",
    "  input: text - list of list raw data\n",
    "         beyound_die - threshold to ignore word type occured only once\n",
    "         seed - random numpy seed\n",
    "  '''\n",
    "  \n",
    "  word_count = {}\n",
    "  for comment in text:\n",
    "    for i in comment:\n",
    "      if i not in word_count.keys(): word_count[i] = 1\n",
    "      else: word_count[i] += 1\n",
    "  \n",
    "  once_word = []\n",
    "  for k, v in word_count.items():\n",
    "    if v == 1:\n",
    "      once_word.append(k)\n",
    "\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  to_unknow = []\n",
    "  for i in once_word:\n",
    "    try:\n",
    "      num = int(i)\n",
    "      to_unknow.append(i)\n",
    "    except:\n",
    "      die = np.random.uniform()\n",
    "      if die > beyound_die:\n",
    "        to_unknow.append(i)\n",
    "\n",
    "  for comment in text:\n",
    "    for index, token in enumerate(comment):\n",
    "      if token in to_unknow:\n",
    "        comment[index] = \"unknow\"\n",
    "        \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esdPAvT2U9EI"
   },
   "outputs": [],
   "source": [
    "def flatten(list_of_list):\n",
    "  '''\n",
    "  This function flatten the list of list tokens into a single list.\n",
    "  input: list_of_list - raw tokenized text data in the form of list of list\n",
    "  '''\n",
    "\n",
    "  flatten = []\n",
    "  for sub_sentence in list_of_list:\n",
    "    flatten = flatten + sub_sentence\n",
    "  return flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pol_kLkVpBjk"
   },
   "outputs": [],
   "source": [
    "def unknow_test(train,test):\n",
    "  '''\n",
    "  Return test data with unknow in list of list. \n",
    "  If token seen in train, keep it as it is. If token not seen in train, replace it with unknow.\n",
    "  input: train - training data with unknow in form of list of list\n",
    "         test - raw test data in form of list of list.\n",
    "  '''\n",
    "  \n",
    "  flatten_train = flatten(train)\n",
    "  for comment in test:\n",
    "    for index, token in enumerate(comment):\n",
    "      if token in flatten_train:\n",
    "        continue\n",
    "      else:\n",
    "        comment[index] = 'unknow'\n",
    "  return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeadArpLW83k"
   },
   "outputs": [],
   "source": [
    "def unknow_obs(train_text,comment):\n",
    "  '''\n",
    "  Return observation data with unknow in single list.\n",
    "  input: train_test - training data with unknow in form of list of list\n",
    "         comment - single tokenized comment in list\n",
    "  '''\n",
    "  flatten_train = flatten(train_text)\n",
    "  for index, token in enumerate(comment):\n",
    "    if token in flatten_train:\n",
    "      continue\n",
    "    else:\n",
    "      comment[index] = 'unknow'\n",
    "  return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLuTFUV5FA3m"
   },
   "source": [
    "## **HMM Implementation**\n",
    "\n",
    "---\n",
    "\n",
    "The code below is just a suggestion for how you may go about building your HMM. Feel free to change it any way you see fit. In fact, you will probably have to tweak it a little to implement smoothing. In the skeleton code below, we have broken down the HMM into its three components: the transition matrix, the emission (i.e., lexical generation, observation) matrix, and the starting state probabilities. We suggest you implement them separately and then use them to build the HMM.\n",
    "\n",
    "Note: it may help to map your classes (named entity types) to discrete values rather than string labels as you will have to do this for your MEMM anyways. However, the HMM can be done without this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhwfqmlVqGaf"
   },
   "outputs": [],
   "source": [
    "# returns the transition probabilities\n",
    "def build_transition_matrix(labels, s_k=0):\n",
    "  '''\n",
    "  input: list of labels\n",
    "  output: dictionary of keys\n",
    "  '''\n",
    "\n",
    "  ### first calc the freq of (xi, xi+1) and (xi) \n",
    "  cnt_dict = {}\n",
    "  temp_dict = {}\n",
    "\n",
    "  for j in range(len(labels)):\n",
    "    for i in range(len(labels[j])-1):\n",
    "      ### consider freq of (xi, xi+1)\n",
    "      if(labels[j][i] not in cnt_dict.keys()):\n",
    "        cnt_dict[labels[j][i]] = {}\n",
    "\n",
    "      if(labels[j][i+1] not in cnt_dict[labels[j][i]].keys()):\n",
    "        cnt_dict[labels[j][i]][labels[j][i+1]] = 1\n",
    "      else:\n",
    "        cnt_dict[labels[j][i]][labels[j][i+1]] += 1 \n",
    "      ### consider freq of (xi)\n",
    "      if(labels[j][i] not in temp_dict.keys()):\n",
    "        temp_dict[labels[j][i]] = 1\n",
    "      else:\n",
    "        temp_dict[labels[j][i]] += 1\n",
    "\n",
    "    ### calc the prob of (xi, xi+1)|(xi)\n",
    "  prob_dict = cnt_dict.copy()\n",
    "  for k in cnt_dict.keys():\n",
    "    for j in cnt_dict[k]:\n",
    "      prob_dict[k][j] = math.log((cnt_dict[k][j]+s_k)/(temp_dict[k]+s_k*5)) # len(cnt_dict.keys())\n",
    "\n",
    "  return prob_dict # cnt_dict, temp_dict, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRt-pjh4FvZ2"
   },
   "outputs": [],
   "source": [
    "def build_emission_matrix(tokens, labels, s=0.5): \n",
    "  '''\n",
    "  Return emission matrix where all values transformed by log\n",
    "  input: tokens - word tokens with unknow in list of list\n",
    "         labels - corresponding NER labels in list of list\n",
    "         s - smoothing factor\n",
    "  '''\n",
    "\n",
    "  matrix = {\"LOC\":{},\"O\":{},\"ORG\":{},\"MISC\":{},\"PER\":{}}\n",
    "\n",
    "  for ind_i, sentence_i in enumerate(tokens):\n",
    "    label_i = labels[ind_i]\n",
    "    \n",
    "    for i,label in enumerate(label_i):\n",
    "        if label == \"LOC\":\n",
    "          if sentence_i[i] not in matrix[\"LOC\"].keys():\n",
    "            for tag in matrix.keys(): matrix[tag][sentence_i[i]] = s  \n",
    "          else:                                      \n",
    "            matrix[\"LOC\"][sentence_i[i]] += 1\n",
    "\n",
    "        if label == \"O\":\n",
    "          if sentence_i[i] not in matrix[\"O\"].keys():\n",
    "            for tag in matrix.keys(): matrix[tag][sentence_i[i]] = s\n",
    "          else:                                      \n",
    "            matrix[\"O\"][sentence_i[i]] += 1\n",
    "        \n",
    "        if label == \"ORG\":\n",
    "          if sentence_i[i] not in matrix[\"ORG\"].keys():\n",
    "            for tag in matrix.keys(): matrix[tag][sentence_i[i]] = s\n",
    "          else:                                      \n",
    "            matrix[\"ORG\"][sentence_i[i]] += 1\n",
    "        \n",
    "        if label == \"MISC\":\n",
    "          if sentence_i[i] not in matrix[\"MISC\"].keys():\n",
    "            for tag in matrix.keys(): matrix[tag][sentence_i[i]] = s\n",
    "          else:                                      \n",
    "            matrix[\"MISC\"][sentence_i[i]] += 1\n",
    "\n",
    "        if label == \"PER\":\n",
    "          if sentence_i[i] not in matrix[\"PER\"].keys():\n",
    "            for tag in matrix.keys(): matrix[tag][sentence_i[i]] = s\n",
    "          else:                                      \n",
    "            matrix[\"PER\"][sentence_i[i]] += 1\n",
    "\n",
    "  # Normalization\n",
    "  for k,v in matrix.items():\n",
    "    den = sum(v.values())\n",
    "    for key, value in v.items():\n",
    "      v[key] = math.log(v[key]/den)\n",
    "    \n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_krHXMlxnE9"
   },
   "outputs": [],
   "source": [
    "def get_start_state_probs(labels):\n",
    "  '''\n",
    "  Return start probability of each NER tag, all values in log transform\n",
    "  input: labels - NER labels in list of list\n",
    "  '''\n",
    "  matrix = {\"LOC\":0,\"O\":0,\"ORG\":0,\"MISC\":0,\"PER\":0}\n",
    "\n",
    "  for sentence in labels:\n",
    "    if sentence[0] == \"LOC\":\n",
    "      matrix[\"LOC\"] +=1\n",
    "    if sentence[0] == \"O\":\n",
    "      matrix[\"O\"] +=1\n",
    "    if sentence[0] == \"ORG\":\n",
    "      matrix[\"ORG\"] +=1\n",
    "    if sentence[0] == \"MISC\":\n",
    "      matrix[\"MISC\"] +=1\n",
    "    if sentence[0] == \"PER\":\n",
    "      matrix[\"PER\"] +=1\n",
    "  \n",
    "  # Normalization\n",
    "  denominator = sum(matrix.values())\n",
    "  for k,v in matrix.items():\n",
    "    matrix[k] = math.log(matrix[k]/denominator)   \n",
    "\n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leXsjeiAKiCm"
   },
   "outputs": [],
   "source": [
    "def build_hmm(tokens, labels,s=1,O_value=0.2): \n",
    "  '''\n",
    "  Return dictionary of necessary entries in HMM\n",
    "  input: tokens - word tokens with unknow in list of list\n",
    "         labels - corresponding NER labels in list of list\n",
    "         s - smoothing factor used in function build_emission_matrix()\n",
    "         O_value - weight of NER tag \"O\" used in function build_tranition_matrix()\n",
    "  '''\n",
    "  \n",
    "  transition = build_transition_matrix(labels,O_value)\n",
    "  emission   = build_emission_matrix(tokens,labels,s)\n",
    "  start      = get_start_state_probs(labels)\n",
    "\n",
    "  hmm = {\"transition\":transition,\"emission\":emission,\"start\":start}\n",
    "\n",
    "  return hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbw3RTHPI31j"
   },
   "source": [
    "## **Viterbi Implementation**\n",
    "\n",
    "---\n",
    "\n",
    "At the end of your implementation, we expect a function or class that maps a sequence of tokens (observation) to a sequence of labels via the Viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_q3U42lI3LQ"
   },
   "outputs": [],
   "source": [
    "def viterbi(hmm, obs):\n",
    "  '''\n",
    "  return the predicted NER tag for the given tokenized text, obs\n",
    "  input: hmm - trained hmm model\n",
    "         obs - test or validation tokenized text data in single list\n",
    "  '''\n",
    "\n",
    "  NER_tag = [\"LOC\",\"O\",\"ORG\",\"MISC\",\"PER\"]\n",
    "  NER_tag_dict = {\"LOC\":0,\"O\":0,\"ORG\":0,\"MISC\":0,\"PER\":0}\n",
    "  score_df = pd.DataFrame(NER_tag_dict,index=obs).T\n",
    "  tag_df = pd.DataFrame(NER_tag_dict,index=obs).T    \n",
    "\n",
    "  for i,tag in enumerate(NER_tag):\n",
    "    score_df.iloc[i,0] = hmm['start'][tag] + hmm['emission'][tag][obs[0]]\n",
    "    tag_df.iloc[i,0]   = 0\n",
    "\n",
    "  for j, word in enumerate(obs[1:]):\n",
    "    for i, tag in enumerate(NER_tag):\n",
    "      five_prob_list = []\n",
    "\n",
    "      for index, prior_tag in enumerate(NER_tag):\n",
    "        prior   = score_df.iloc[index,j]\n",
    "        transit = hmm['transition'][prior_tag][tag]\n",
    "        emit    = hmm['emission'][tag][word]\n",
    "        prob    = prior + transit + emit\n",
    "        five_prob_list.append(prob)\n",
    "\n",
    "      score_df.iloc[i,j+1] = np.max(five_prob_list)\n",
    "      tag_df.iloc[i,j+1] = NER_tag[np.argmax(five_prob_list)]\n",
    " \n",
    "  max_index = np.argmax(score_df.iloc[:,-1])\n",
    "  tag_sq = [NER_tag[max_index]]\n",
    "\n",
    "  col_number = len(obs)-1\n",
    "  while col_number > 0:\n",
    "    last_tag = tag_sq[0]\n",
    "    last_tag_index = NER_tag.index(last_tag)\n",
    "    front_tag = tag_df.iloc[last_tag_index,col_number]\n",
    "    tag_sq.insert(0,front_tag)\n",
    "    col_number -= 1\n",
    "\n",
    "  return tag_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2J8NdM_V_A4u"
   },
   "source": [
    "## **Validation Step**\n",
    "\n",
    "---\n",
    "\n",
    "In this part of the project, we expect you to split the training data into train and validation datasets. You may use whatever split you see fit and use any external libraries to perform this split. You may want to look into the following function for splitting data: [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "Once you have split the data, train your HMM model on the training data and evaluate it on the validation data. Report **Entity Level Mean F1**, which was explained earlier. Please use the code we have provided below to compute this metric.\n",
    "\n",
    "Please also take a look into your misclassified cases, as we will be performing error analysis in the *Evaluation* section. We expect smoothing, unknown word handling and correct emission (i.e., lexical generation) probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTAhu_TG1V0R"
   },
   "source": [
    "Consider the example below. After getting a sequence of NER labels for the sequence of tokens from your Viterbi algorithm implementation, you need to convert the sequence of tokens, associated token indices and NER labels into a format which can be used to calculate **Entity Level Mean F1**. We do this by finding the starting and ending indices of the spans representing each entity (as given in the corpus) and adding it to a list that is associated with the label with which the spans are labelled. To score your validation data on Google Colab or your local device, you can get a dictionary format as shown in the picture below from the function **format_output_labels** of both the predicted and true label sequences, and use the two dictionaries as input to the **mean_f1** function.\n",
    "\n",
    "NOTE: We do **not** include the spans of the tokens labelled as \"O\" in the formatted dictionary output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYoQTBMAxojF"
   },
   "source": [
    "![picture](https://docs.google.com/uc?export=download&id=1M57DEHgfusVPU_hlvmiOpkS3yn9GGEgj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S461GWyv3nEf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdOOQdN7D2rv"
   },
   "outputs": [],
   "source": [
    "def format_output_labels(token_labels, token_indices):\n",
    "    \"\"\"\n",
    "    Returns a dictionary that has the labels (LOC, ORG, MISC or PER) as the keys, \n",
    "    with the associated value being the list of entities predicted to be of that key label. \n",
    "    Each entity is specified by its starting and ending position indicated in [token_indices].\n",
    "\n",
    "    Eg. if [token_labels] = [\"ORG\", \"ORG\", \"O\", \"O\", \"ORG\"]\n",
    "           [token_indices] = [15, 16, 17, 18, 19]\n",
    "        then dictionary returned is \n",
    "        {'LOC': [], 'MISC': [], 'ORG': [(15, 16), (19, 19)], 'PER': []}\n",
    "\n",
    "    :parameter token_labels: A list of token labels (eg. PER, LOC, ORG or MISC).\n",
    "    :type token_labels: List[String]\n",
    "    :parameter token_indices: A list of token indices (taken from the dataset) \n",
    "                              corresponding to the labels in [token_labels].\n",
    "    :type token_indices: List[int]\n",
    "    \"\"\"\n",
    "    label_dict = {\"LOC\":[], \"MISC\":[], \"ORG\":[], \"PER\":[]}\n",
    "    prev_label = token_labels[0]\n",
    "    start = token_indices[0]\n",
    "    for idx, label in enumerate(token_labels):\n",
    "      if prev_label != label:\n",
    "        end = token_indices[idx-1]\n",
    "        if prev_label != \"O\":\n",
    "            label_dict[prev_label].append((start, end))\n",
    "        start = token_indices[idx]\n",
    "      prev_label = label\n",
    "      if idx == len(token_labels) - 1:\n",
    "        if prev_label != \"O\":\n",
    "            label_dict[prev_label].append((start, token_indices[idx]))\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfjVJLNhL_fc"
   },
   "outputs": [],
   "source": [
    "# Code for mean F1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def mean_f1(y_pred_dict, y_true_dict):\n",
    "    \"\"\" \n",
    "    Calculates the entity-level mean F1 score given the actual/true and \n",
    "    predicted span labels.\n",
    "    :parameter y_pred_dict: A dictionary containing predicted labels as keys and the \n",
    "                            list of associated span labels as the corresponding\n",
    "                            values.\n",
    "    :type y_pred_dict: Dict<key [String] : value List[Tuple]>\n",
    "    :parameter y_true_dict: A dictionary containing true labels as keys and the \n",
    "                            list of associated span labels as the corresponding\n",
    "                            values.\n",
    "    :type y_true_dict: Dict<key [String] : value List[Tuple]>\n",
    "\n",
    "    Implementation modified from original by author @shonenkov at\n",
    "    https://www.kaggle.com/shonenkov/competition-metrics.\n",
    "    \"\"\"\n",
    "    F1_lst = []\n",
    "    for key in y_true_dict:\n",
    "        TP, FN, FP = 0, 0, 0\n",
    "        num_correct, num_true = 0, 0\n",
    "        preds = y_pred_dict[key]\n",
    "        trues = y_true_dict[key]\n",
    "        for true in trues:\n",
    "            num_true += 1\n",
    "            if true in preds:\n",
    "                num_correct += 1\n",
    "            else:\n",
    "                continue\n",
    "        num_pred = len(preds)\n",
    "        if num_true != 0:\n",
    "            if num_pred != 0 and num_correct != 0:\n",
    "                R = num_correct / num_true\n",
    "                P = num_correct / num_pred\n",
    "                F1 = 2*P*R / (P + R)\n",
    "            else:\n",
    "                F1 = 0      # either no predictions or no correct predictions\n",
    "        else:\n",
    "            continue\n",
    "        F1_lst.append(F1)\n",
    "    return np.mean(F1_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rwLFPhetVok",
    "outputId": "4def1bc0-192e-483d-e0ce-4b2f99a7096f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_dict is : {'LOC': [(18, 18), (28, 28)], 'MISC': [(23, 23)], 'ORG': [(13, 13)], 'PER': [(15, 16)]}\n",
      "y_true_dict is : {'LOC': [(18, 18), (28, 28)], 'MISC': [(23, 24)], 'ORG': [(13, 13)], 'PER': [(15, 16)]}\n",
      "Entity Level Mean F1 score is : 0.75\n"
     ]
    }
   ],
   "source": [
    "# Usage using above example\n",
    "\n",
    "pred_token_labels = [\"ORG\", \"O\", \"PER\", \"PER\", \"O\", \"LOC\", \"O\", \"O\", \"O\", \"O\", \"MISC\", \"O\", \"O\", \"O\", \"O\", \"LOC\"]\n",
    "true_token_labels = [\"ORG\", \"O\", \"PER\", \"PER\", \"O\", \"LOC\", \"O\", \"O\", \"O\", \"O\", \"MISC\", \"MISC\", \"O\", \"O\", \"O\", \"LOC\"]\n",
    "token_indices = [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "y_pred_dict = format_output_labels(pred_token_labels, token_indices)\n",
    "print(\"y_pred_dict is : \" + str(y_pred_dict))\n",
    "y_true_dict = format_output_labels(true_token_labels, token_indices)\n",
    "print(\"y_true_dict is : \" + str(y_true_dict))\n",
    "\n",
    "print(\"Entity Level Mean F1 score is : \" + str(mean_f1(y_pred_dict, y_true_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ct_DWpoXSNV"
   },
   "outputs": [],
   "source": [
    "def valid(train,beyound_die=0.5,O_value=0.2,s=0.5):\n",
    "  '''\n",
    "  Return the model mean f1 score\n",
    "  input: train - train data\n",
    "         beyound_die - threshold to ignore word type occured only once\n",
    "         s - smoothing factor used in function build_emission_matrix()\n",
    "         O_value - weight of NER tag \"O\" used in function build_tranition_matrix()\n",
    "  '''\n",
    "\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(train['text'], train['NER'], test_size=0.1, random_state=0)\n",
    "  X_pos, X_index, y_pos, y_index = train_test_split(train['POS'], train['index'], test_size=0.1, random_state=0)\n",
    "  train_unk = unknow_train(X_train,beyound_die)\n",
    "  test_unk  = unknow_test(train_unk,X_test) \n",
    "\n",
    "  hmm = build_hmm(train_unk,y_train,s,O_value)\n",
    "\n",
    "  index = []\n",
    "  pred  = []\n",
    "  true  = []\n",
    "  for number, comment in enumerate(X_test):     \n",
    "    pred_tag = viterbi(hmm, comment)\n",
    "    \n",
    "    index = index + y_index[number]\n",
    "    pred = pred + pred_tag\n",
    "    true = true + y_test[number]\n",
    "\n",
    "  y_pred_dict = format_output_labels(pred, index)\n",
    "  y_true_dict = format_output_labels(true, index)\n",
    "\n",
    "  return mean_f1(y_pred_dict, y_true_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geis1ouzXUIg"
   },
   "outputs": [],
   "source": [
    "score = []\n",
    "position = []\n",
    "for j in [0.01,0.05,0.1,0.2]:\n",
    "  for o in [0.01,0.1,0.2,0.3,0.5,0.7,0.8]:\n",
    "    for i in [0.05,0.1,0.3,0.5,0.7,1]:\n",
    "      f1 = valid(train,beyound_die=j,O_value=o,s=i)\n",
    "      score.append(f1)\n",
    "      position.append((j,o,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKh6BJ3JXXiM",
    "outputId": "5a29f306-3dad-4071-a027-dacf3897da8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  516,   111,    43,     4,     5],\n",
       "       [    4, 14379,    21,     8,    72],\n",
       "       [   74,   301,   547,    11,    12],\n",
       "       [    8,   127,    11,   287,     3],\n",
       "       [    9,   325,     5,     1,   575]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train['text'], train['NER'], test_size=0.1, random_state=0)\n",
    "X_pos, X_index, y_pos, y_index = train_test_split(train['POS'], train['index'], test_size=0.1, random_state=0)\n",
    "train_unk = unknow_train(X_train,0.01)\n",
    "test_unk  = unknow_test(train_unk,X_test) \n",
    "hmm = build_hmm(train_unk,y_train,0.05,0.7)\n",
    "\n",
    "index = []\n",
    "pred  = []\n",
    "true  = []\n",
    "for number, comment in enumerate(X_test):     \n",
    "  pred_tag = viterbi(hmm, comment)\n",
    "  \n",
    "  index = index + y_index[number]\n",
    "  pred = pred + pred_tag\n",
    "  true = true + y_test[number]\n",
    "\n",
    "y_pred_dict = format_output_labels(pred, index)\n",
    "y_true_dict = format_output_labels(true, index)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(true,pred,labels=['LOC','O','ORG','MISC','PER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MyQGCohXcs_"
   },
   "outputs": [],
   "source": [
    "unk_train = unknow_train(train['text'],0.01)\n",
    "unk_test  = unknow_test(unk_train,test['text'])\n",
    "\n",
    "hmm = build_hmm(unk_train,train['NER'],s=0.05,O_value=0.7)\n",
    "\n",
    "pred  = []\n",
    "index = []\n",
    "for i, comment in enumerate(unk_test):\n",
    "  tag = viterbi(hmm,comment)\n",
    "  pred = pred + tag\n",
    "  index = index + test['index'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-BBG1YAXhXL",
    "outputId": "717397e5-5ac1-4ace-806b-8cbcb144fcec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': 1428, 'ORG': 1343, 'MISC': 738, 'PER': 1894}\n"
     ]
    }
   ],
   "source": [
    "di = {'LOC':0,'ORG':0,'MISC':0,'PER':0}\n",
    "for i in pred:\n",
    "  if i == 'LOC': di['LOC']+=1\n",
    "  if i == 'ORG': di['ORG']+=1\n",
    "  if i == 'MISC': di['MISC']+=1\n",
    "  if i == 'PER':di['PER']+=1\n",
    "print(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cq99BFkEXhHC"
   },
   "outputs": [],
   "source": [
    "create_submission('output_hmm.csv',pred,index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRLi5TDyuSx_"
   },
   "source": [
    "### **Q2.1: Explain your HMM Implementations**\n",
    "\n",
    "Explain how you implemented the HMM including the Viterbi algorithm (e.g. **which algorithms/data structures** you used). Make clear which parts were implemented from scratch vs. obtained via an existing package. Explain and motivate any design choices providing the intuition behind them (e.g. which methods you used for your HMM implementation, and why?). (Please answer on the written questions template document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHbzRuil-yjG"
   },
   "source": [
    "### **Q2.2: Results Analysis**\n",
    "\n",
    "Explain here how you evaluated the models. Summarize the performance of your system and any variations that you experimented with on the validation datasets. Put the results into clearly labeled tables or diagrams and include your observations and analysis. (Please answer on the written questions template document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTIPGnLFNc43"
   },
   "source": [
    "### **Q2.3: Error Analysis**\n",
    "When did the system work well? When did it fail?  Any ideas as to why? How might you improve the system? (Please answer on the written questions template document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mf6ziT36NteS"
   },
   "source": [
    "### **Q2.4: What is the effect of unknown word handling and smoothing?**\n",
    "(Please answer on the written questions template document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31e3sMHZrLWP"
   },
   "source": [
    "# **Part 3: Maximum Entropy Markov Model** ðŸ’«\n",
    "\n",
    "---\n",
    "\n",
    "In this section, you will implement a Maximum Entropy Markov Model (**MEMM**) to perform the same NER task. Your model should consist of a MaxEnt classifier with Viterbi decoding. \n",
    "\n",
    "1. We have already performed tokenizations for documents. You can either use a MaxEnt classifier from an existing package or write the MaxEnt code yourself. **Important note:  MaxEnt classifiers are statistically equivalent to multi-class logistic regression, so you can use packages for multi-class LR instead of MaxEnt.**\n",
    "\n",
    "2. Use the classifier to learn a probability $P(t_i|features)$. You may replace either the lexical generation probability â€“ $P(w_i|t_i)$ â€“ or the transition probability â€“ $P(t_i|t_{iâˆ’1})$ â€“ in the HMM with it, or you may replace the entire *lexical generation probability * transition probability*  calculation â€“ $P (w_i|t_i) âˆ— P (t_i|t_{iâˆ’1)} â€“ $ in the HMM with it. \n",
    "\n",
    "3. To train such classifier, you need to pick some feature set. The content of the feature set is up to your choice. You should be trying different feature sets, and evaluate your choices on the validation set. Pick the feature set that performs overall the best according to the F1 measure. If you draw inspiration for your features from external sources, please link them in the code.\n",
    "\n",
    "4. Use your own implementation of the **Viterbi algorithm**, which you can modify from the one you developed for the HMM model. You will need the probabilities that you obtain from the MaxEnt classifier. \n",
    "\n",
    "5. Remember to use same training and validation split when evaluating the MEMM to have a **fair comparison** with your **HMM model**.\n",
    "\n",
    "\n",
    "Please also take a look into your misclassified cases, as we will be performing error analysis in *Evaluation* section. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJIosHVJZ-1o"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "Here's a summary of the workflow for Part 3:\n",
    "\n",
    "![alt text](https://drive.google.com/uc?export=view&id=14VfjW3yDyXLojWM_u0LeJYdDOSLkElBn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QGSijPUW_Bi"
   },
   "source": [
    "Note that we have not provided any skeleton code for how you should do feature engineering since this is meant to be an open ended task and we want you to experiment with the dataset. However, please remember to make sure that you code is concise, clean, and readable! Ultimately, we expect a function or class  mapping a sequence of tokens to a sequence of labels. At the end of this section you should have done the following:\n",
    "1. Extract Features\n",
    "2. Build & Train MaxEnt\n",
    "3. Call Viterbi when evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pd2PwG4wYkhQ"
   },
   "source": [
    "### **Feature Engineering**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsqTqGn556zU"
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xBYPGLUHH7n"
   },
   "outputs": [],
   "source": [
    "# Your implementation here\n",
    "def MEMM_features(word, tag, prev_word, prev_tag, prev_2_word, prev_2_tag,  next_word, next_tag, next_2_word, next_2_tag, wordStartList):  \n",
    "  features = {}\n",
    "  features['current_word'] = word\n",
    "  features['current_tag'] = tag\n",
    "\n",
    "  features['capitalization'] = int(str(word)[0].isupper())\n",
    "  features['start_of_sentence'] = int(word in wordStartList)\n",
    "  features['cap_start'] = int(word not in wordStartList and str(word)[0].isupper())\n",
    "  features['cap_end'] = int(str(word)[-1].isupper())\n",
    "\n",
    "  features['previous_tag'] = prev_tag\n",
    "  features['previous_word'] = prev_word\n",
    "  features['previous_2_tag'] = prev_2_tag\n",
    "  features['previous_2_word'] = prev_2_word\n",
    "  features['next_tag'] = next_tag\n",
    "  features['next_word'] = next_word\n",
    "  features['next_2_tag'] = next_2_tag\n",
    "  features['next_2_word'] = next_2_word\n",
    "  # features['current_ner'] = curr_ner\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZxhO3_E1iiK"
   },
   "outputs": [],
   "source": [
    "def MEMM_prob_filter(train_text, train_tag, train_ner, feature_lst, maxiter = 5):\n",
    "  feature_mtx = []\n",
    "  wordStartList = []\n",
    "  sent_len = ' '.join(train_text).split(' . ')\n",
    "  train_prev_tag = [' '] + train_tag \n",
    "  train_prev_text = [' '] + train_text\n",
    "  train_prev_2_tag = [' ', ' '] + train_tag \n",
    "  train_prev_2_text = [' ', ' '] + train_text\n",
    "\n",
    " # train_prev_3_tag = [' ', ' ',' '] + train_tag \n",
    " # train_prev_3_text = [' ', ' ', ' '] + train_text\n",
    "\n",
    "  train_next_tag = train_tag[1:]+[' ']\n",
    "  train_next_text = train_text[1:]+[' ']\n",
    "  train_next_2_tag = train_tag[2:]+ [' ', ' ']\n",
    "  train_next_2_text = train_text[2:]+ [' ', ' ']\n",
    "  \n",
    " # train_next_3_tag = train_tag[3:]+ [' ', ' ', ' ']\n",
    " # train_next_3_text = train_text[3:]+ [' ', ' ', ' ']\n",
    "\n",
    "  for i in range(len(sent_len)):\n",
    "    temp = sent_len[i].split(' ')[0]\n",
    "    wordStartList.append(temp)\n",
    "  train_x = []\n",
    "  train=[]\n",
    "  for j in range(len(train_text)):\n",
    "    temp_feature = MEMM_features(train_text[j], train_tag[j], train_prev_text[j], train_prev_tag[j],train_prev_2_text[j],train_prev_2_tag[j], train_next_text[j], train_next_tag[j], train_next_2_text[j], train_next_2_tag[j], wordStartList)\n",
    "   # temp_feature = dict((key,value) for key, value in temp_feature.iteritems() if key )\n",
    "    temp_feature = {k: v for k, v in temp_feature.items() if k in feature_lst}\n",
    "    train_x.append(temp_feature)\n",
    "    train.append((temp_feature, train_ner[j]))\n",
    "\n",
    "  model = MaxentClassifier.train(train,algorithm=\"iis\", max_iter = maxiter) \n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGv1QsA-56ZY",
    "outputId": "036c7649-d6b8-495c-ebbe-176e823ab0d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.53307        0.984\n",
      "             3          -0.35500        0.984\n",
      "             4          -0.27018        0.984\n",
      "         Final          -0.22021        0.984\n",
      "current_word\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              0.264035  0.735965\n",
      "MISC             0.321622  0.678378\n",
      "O                0.077986  0.922014\n",
      "ORG              0.488102  0.511898\n",
      "PER              0.025498  0.974502\n",
      "All              0.109011  0.890989\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.27748        0.857\n",
      "             3          -0.25697        0.857\n",
      "             4          -0.24475        0.857\n",
      "         Final          -0.23656        0.857\n",
      "current_tag\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              1.000000  0.000000\n",
      "MISC             1.000000  0.000000\n",
      "O                0.030472  0.969528\n",
      "ORG              0.966242  0.033758\n",
      "PER              0.058458  0.941542\n",
      "All              0.135717  0.864283\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.20048        0.830\n",
      "             3          -0.20048        0.830\n",
      "             4          -0.20048        0.830\n",
      "         Final          -0.20048        0.830\n",
      "capitalization\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              1.000000  0.000000\n",
      "MISC             1.000000  0.000000\n",
      "O                0.000000  1.000000\n",
      "ORG              1.000000  0.000000\n",
      "PER              1.000000  0.000000\n",
      "All              0.156597  0.843403\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.35685        0.830\n",
      "             3          -0.35685        0.830\n",
      "             4          -0.35685        0.830\n",
      "         Final          -0.35685        0.830\n",
      "start_of_sentence\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              1.000000  0.000000\n",
      "MISC             1.000000  0.000000\n",
      "O                0.000000  1.000000\n",
      "ORG              1.000000  0.000000\n",
      "PER              1.000000  0.000000\n",
      "All              0.156597  0.843403\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.25179        0.847\n",
      "             3          -0.25179        0.847\n",
      "             4          -0.25179        0.847\n",
      "         Final          -0.25179        0.847\n",
      "cap_start\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              1.000000  0.000000\n",
      "MISC             1.000000  0.000000\n",
      "O                0.035171  0.964829\n",
      "ORG              1.000000  0.000000\n",
      "PER              0.253731  0.746269\n",
      "All              0.150770  0.849230\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.33590        0.833\n",
      "             3          -0.33265        0.833\n",
      "             4          -0.33067        0.833\n",
      "         Final          -0.32931        0.833\n",
      "previous_tag\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              0.875439  0.124561\n",
      "MISC             1.000000  0.000000\n",
      "O                0.004278  0.995722\n",
      "ORG              1.000000  0.000000\n",
      "PER              1.000000  0.000000\n",
      "All              0.156005  0.843995\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.54590        0.885\n",
      "             3          -0.43007        0.885\n",
      "             4          -0.37240        0.885\n",
      "         Final          -0.33773        0.885\n",
      "previous_word\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              0.764912  0.235088\n",
      "MISC             0.913514  0.086486\n",
      "O                0.119048  0.880952\n",
      "ORG              0.862203  0.137797\n",
      "PER              0.485697  0.514303\n",
      "All              0.215361  0.784639\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.49454        0.867\n",
      "             3          -0.41523        0.867\n",
      "             4          -0.37376        0.867\n",
      "         Final          -0.34811        0.867\n",
      "previous_2_tag\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              0.871930  0.128070\n",
      "MISC             0.954054  0.045946\n",
      "O                0.125885  0.874115\n",
      "ORG              0.905368  0.094632\n",
      "PER              0.785448  0.214552\n",
      "All              0.242185  0.757815\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.34843        0.830\n",
      "             3          -0.34797        0.830\n",
      "             4          -0.34770        0.830\n",
      "         Final          -0.34751        0.830\n",
      "previous_2_word\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              1.000000  0.000000\n",
      "MISC             1.000000  0.000000\n",
      "O                0.000000  1.000000\n",
      "ORG              1.000000  0.000000\n",
      "PER              1.000000  0.000000\n",
      "All              0.156597  0.843403\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.31996        0.836\n",
      "             3          -0.31837        0.836\n",
      "             4          -0.31738        0.836\n",
      "         Final          -0.31670        0.836\n",
      "next_tag\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              0.877193  0.122807\n",
      "MISC             1.000000  0.000000\n",
      "O                0.009082  0.990918\n",
      "ORG              1.000000  0.000000\n",
      "PER              0.894900  0.105100\n",
      "All              0.155118  0.844882\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.51061        0.888\n",
      "             3          -0.40547        0.888\n",
      "             4          -0.35212        0.888\n",
      "         Final          -0.31968        0.888\n",
      "next_word\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              0.647368  0.352632\n",
      "MISC             0.827027  0.172973\n",
      "O                0.127498  0.872502\n",
      "ORG              0.773658  0.226342\n",
      "PER              0.485075  0.514925\n",
      "All              0.211871  0.788129\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.51061        0.888\n",
      "             3          -0.40547        0.888\n",
      "             4          -0.35212        0.888\n",
      "         Final          -0.31968        0.888\n",
      "next_2_tag\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              0.647368  0.352632\n",
      "MISC             0.827027  0.172973\n",
      "O                0.127498  0.872502\n",
      "ORG              0.773658  0.226342\n",
      "PER              0.485075  0.514925\n",
      "All              0.211871  0.788129\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.60944        0.057\n",
      "             2          -0.31996        0.836\n",
      "             3          -0.31837        0.836\n",
      "             4          -0.31738        0.836\n",
      "         Final          -0.31670        0.836\n",
      "next_2_word\n",
      "true_pred_judge     False     True \n",
      "true                               \n",
      "LOC              0.877193  0.122807\n",
      "MISC             1.000000  0.000000\n",
      "O                0.009082  0.990918\n",
      "ORG              1.000000  0.000000\n",
      "PER              0.894900  0.105100\n",
      "All              0.155118  0.844882\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(feature_lst)):\n",
    "  mm_temp = MEMM_prob_filter(f_X_train_text, flatten(X_train_pos), f_y_train, feature_lst[i], maxiter = 5)\n",
    "  pred_classifier = mm_temp.classify_many(pd.DataFrame(test_mat)[[feature_lst[i]]].T.to_dict().values())\n",
    "  temp_df = pd.DataFrame({'pred':pred_classifier, 'true':f_y_val})\n",
    "  temp_df['true_pred_judge'] = temp_df['pred'] == temp_df['true']\n",
    "  print(feature_lst[i])\n",
    "  print(pd.crosstab(temp_df['true'], temp_df['true_pred_judge'], margins = True, normalize = 'index'))#.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCkP4A3IYwit"
   },
   "source": [
    "### **MEMM Implementation**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6qx_0FrGBEB"
   },
   "outputs": [],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def MEMM_prob(train_text, train_tag, train_ner, maxiter = 5):\n",
    "  feature_mtx = []\n",
    "  wordStartList = []\n",
    "  sent_len = ' '.join(train_text).split(' . ')\n",
    "  train_prev_tag = [' '] + train_tag \n",
    "  train_prev_text = [' '] + train_text\n",
    "  train_prev_2_tag = [' ', ' '] + train_tag \n",
    "  train_prev_2_text = [' ', ' '] + train_text\n",
    "\n",
    "  # train_prev_3_tag = [' ', ' ',' '] + train_tag \n",
    "  # train_prev_3_text = [' ', ' ', ' '] + train_text\n",
    "\n",
    "  train_next_tag = train_tag[1:]+[' ']\n",
    "  train_next_text = train_text[1:]+[' ']\n",
    "  train_next_2_tag = train_tag[2:]+ [' ', ' ']\n",
    "  train_next_2_text = train_text[2:]+ [' ', ' ']\n",
    "  \n",
    "  # train_next_3_tag = train_tag[3:]+ [' ', ' ', ' ']\n",
    "  # train_next_3_text = train_text[3:]+ [' ', ' ', ' ']\n",
    "\n",
    "  for i in range(len(sent_len)):\n",
    "    temp = sent_len[i].split(' ')[0]\n",
    "    wordStartList.append(temp)\n",
    "  train_x = []\n",
    "  train=[]\n",
    "  for j in range(len(train_text)):\n",
    "    temp_feature =  MEMM_features(train_text[j], train_tag[j], train_prev_text[j], train_prev_tag[j],train_prev_2_text[j],train_prev_2_tag[j], train_next_text[j], train_next_tag[j], train_next_2_text[j], train_next_2_tag[j], wordStartList)\n",
    "  \n",
    "    train_x.append(temp_feature)\n",
    "    train.append((temp_feature, train_ner[j]))\n",
    "\n",
    "  model = MaxentClassifier.train(train,algorithm=\"iis\", max_iter = maxiter) \n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taH7ot8dqDww"
   },
   "outputs": [],
   "source": [
    "# takes in the tokens & labels and returns a representation of the HMM\n",
    "# call the three functions above in this function to build your HMM\n",
    "def build_hmm(tokens, pos_labels, ner_labels, k=1): # training text, training NER\n",
    "  '''\n",
    "  train the hmm model\n",
    "  '''\n",
    "  # encode to \n",
    "  f_tt =  flatten(tokens)\n",
    "  f_pl = flatten(pos_labels)\n",
    "  f_nn = flatten(ner_labels)\n",
    "\n",
    "  model = MEMM_prob(f_tt, f_pl, f_nn, 100)\n",
    "  transition = build_transition_matrix(ner_labels, k)\n",
    "  emission   = build_emission_matrix(tokens,ner_labels,k)\n",
    "  start      = get_start_state_probs(ner_labels)\n",
    "\n",
    "  hmm = {\"model\": model, \"transition\": transition, \"start\": start,'emission':emission}\n",
    "  return hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9SyspdmlvyD"
   },
   "outputs": [],
   "source": [
    "# takes in the hmm build above and an observation: list of tokens\n",
    "# and returns the appropriate named entity mappings for the tokens\n",
    "def viterbi(hmm, obs_dict):\n",
    "  '''\n",
    "  MEMM viterbi implementation\n",
    "  input: hmm - dict, obs_dict - dict of observation test and pos\n",
    "  output: 4 outputs, bptr - storing backpointers, sc - storing scores, tag - storing tags, test_mat - storing observation features \n",
    "  '''\n",
    "  bptr = []\n",
    "  sc = {}\n",
    "  bptr = {}\n",
    "  c = ['PER', 'O', 'ORG', 'LOC', 'MISC']\n",
    "  sc[0] = {}\n",
    "  bptr[0] = {}\n",
    "\n",
    "  # transform test\n",
    "  train_tag = obs_dict['POS']\n",
    "  train_text = obs_dict['text']\n",
    "  train_prev_tag = [' '] + train_tag \n",
    "  train_prev_text = [' '] + train_text\n",
    "  train_prev_2_tag = [' ', ' '] + train_tag \n",
    "  train_prev_2_text = [' ', ' '] + train_text\n",
    "\n",
    "  train_next_tag = train_tag[1:]+[' ']\n",
    "  train_next_text = train_text[1:]+[' ']\n",
    "  train_next_2_tag = train_tag[1:]+ [' ', ' ']\n",
    "  train_next_2_text = train_text[1:]+ [' ', ' ']\n",
    "\n",
    "  feature_mtx = []\n",
    "  wordStartList = []\n",
    "  test_mat = []\n",
    "  sent_len = ' '.join(obs_dict['text']).split(' . ')\n",
    "  for ii in range(len(sent_len)):\n",
    "    temp = sent_len[ii].split(' ')[0]\n",
    "    wordStartList.append(temp)\n",
    "  \n",
    "  # initialization\n",
    "  rnd = 0\n",
    "  temp_feature =   MEMM_features(train_text[rnd], train_tag[rnd], train_prev_text[rnd], train_prev_tag[rnd], train_prev_2_text[rnd],train_prev_2_tag[rnd], train_next_text[rnd], train_next_tag[rnd], train_next_2_text[rnd], train_next_2_tag[rnd], wordStartList)\n",
    "  temp_pred = hmm['model'].prob_classify(temp_feature)    \n",
    "  test_mat.append(temp_feature)\n",
    "\n",
    "  for i in c:\n",
    "    sc_n = hmm['start'][i]+np.log(temp_pred.prob(i)) #np.log(hmm['emissions'][i][j])# \n",
    "    sc[0][i] = sc_n\n",
    "    bptr[0][i] = 0 \n",
    "\n",
    "  # iteration from the second word to the end of observation\n",
    "  for rnd in range(1, len(obs_dict['text'])):\n",
    "    temp_feature =   MEMM_features(train_text[rnd], train_tag[rnd], train_prev_text[rnd], train_prev_tag[rnd], train_prev_2_text[rnd],train_prev_2_tag[rnd], train_next_text[rnd], train_next_tag[rnd], train_next_2_text[rnd], train_next_2_tag[rnd], wordStartList)\n",
    "    temp_pred = hmm['model'].prob_classify(temp_feature)\n",
    "    test_mat.append(temp_feature)\n",
    "  \n",
    "    temp_dict = {} # store scores to find max  \n",
    "    sc[rnd] = {}\n",
    "    bptr[rnd] = {}\n",
    "\n",
    "    for i in c:\n",
    "      temp_dict[i] = {}\n",
    "      max_v = np.log(0) \n",
    "      max_j = 0 \n",
    "      for j in c: \n",
    "        temp_v = sc[rnd-1][j] + np.log(temp_pred.prob(i))#  # np.log(temp_pred.prob(i)) # # # \n",
    "        temp_dict[i][j] = temp_v\n",
    "        if(temp_v > max_v):\n",
    "          max_v = temp_v\n",
    "          max_j = j       \n",
    "      sc_n = max_v + np.log(temp_pred.prob(i)) # np.log(hmm['emission'][i][obs_dict['text'][rnd]])# np.log(hmm['emission'][i][j])\n",
    "      sc[rnd][i] = sc_n\n",
    "      bptr[rnd][i] = max_j\n",
    "  \n",
    "  tag = []\n",
    "  tag_final = max(sc[list(sc.keys())[-1]].keys(), key=(lambda k:sc[list(sc.keys())[-1]][k]))\n",
    "  prev_tg = tag_final\n",
    "  tag.append(prev_tg)\n",
    "  for i in range(len(sc.keys())-1, 0, -1):\n",
    "    prev_tg = bptr[i][prev_tg]\n",
    "    tag.append(prev_tg)\n",
    "  \n",
    "  tag.reverse()\n",
    "\n",
    "  return bptr, sc, tag, test_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvoeAVMlX4gp"
   },
   "source": [
    "### **Validation**\n",
    "---\n",
    "In this section we want you to run your MaxEnt model on the validation dataset you extracted earlier. We want you to play around with different combinations of features in order to find which features work the best for your implementation. You will be asked to write about this process in detail in written question 3.3 so please spend time experimenting with features! Once again, please use the code we provided for computing Entity Level Avg F1 earlier when validating your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmUnuuxvHKXM"
   },
   "outputs": [],
   "source": [
    "# Run your model on validation set\n",
    "# You will need to \n",
    "# 1. Call your function above to get a prediction result on Validation Set\n",
    "# 2. Report Metrics\n",
    "# (See if you need to modify your feature set)\n",
    "\n",
    "import json\n",
    "\n",
    "# TODO: please change the line below with your drive organization\n",
    "path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"CS_4740_FA21_p2\")\n",
    "\n",
    "with open(os.path.join(path,'train.json'), 'r') as f:\n",
    "     train = json.loads(f.read())\n",
    "\n",
    "with open(os.path.join(path,'test.json'), 'r') as f:\n",
    "     test = json.loads(f.read())\n",
    "\n",
    "# Evaluate/validate your model here\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "lng = len(train['text'])\n",
    "\n",
    "X_train_text = train['text'][:int(lng*0.8)]\n",
    "X_val_text = train['text'][int(lng*0.8):]\n",
    "\n",
    "X_train_pos = train['POS'][:int(lng*0.8)]\n",
    "X_val_pos = train['POS'][int(lng*0.8):]\n",
    "\n",
    "y_train = train['NER'][:int(lng*0.8)]\n",
    "y_val = train['NER'][int(lng*0.8):]\n",
    "\n",
    "# X_train_t, X_val_t, y_train, y_val = train_test_split(train['POS'], train['NER'], test_size=0.2)\n",
    "\n",
    "f_X_train_text = flatten(X_train_text)\n",
    "f_X_val_text = flatten(X_val_text)\n",
    "\n",
    "f_y_train = flatten(y_train)\n",
    "f_y_val = flatten(y_val)\n",
    "\n",
    "hmm_t = build_hmm(X_train_text, X_train_pos, y_train, k=1)\n",
    "obs_dict = {'POS':flatten(X_val_pos), 'text':f_X_val_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VdOlpdEYUpf",
    "outputId": "71cb0d92-4526-4597-bfdc-506afbc5a59a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "obs_dict = {'POS':flatten(X_val_pos), 'text':f_X_val_text}\n",
    "bt, sc_c,t, mm = viterbi(hmm_t, obs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeRyaRgmgIgW"
   },
   "outputs": [],
   "source": [
    "pred_token_labels = t\n",
    "true_token_labels = f_y_val\n",
    "token_indices = list(range(len(f_y_val)))\n",
    "\n",
    "y_pred_dict = format_output_labels(pred_token_labels, token_indices)\n",
    "print(\"y_pred_dict is : \" + str(y_pred_dict))\n",
    "y_true_dict = format_output_labels(true_token_labels, token_indices)\n",
    "print(\"y_true_dict is : \" + str(y_true_dict))\n",
    "\n",
    "print(\"Entity Level Mean F1 score is : \" + str(mean_f1(y_pred_dict, y_true_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RgCaX49OzPDP",
    "outputId": "13d3d189-f6c5-4ba5-c352-34500d781163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7.771 previous_word=='Cassidy' and label is 'MISC'\n",
      "   6.780 current_word=='1997--Ruehe' and label is 'MISC'\n",
      "  -6.399 capitalization==0 and label is 'LOC'\n",
      "   6.177 current_word=='1676/96' and label is 'MISC'\n",
      "   5.873 current_word=='disarmament-China' and label is 'MISC'\n",
      "   5.851 previous_2_tag=='NL' and label is 'MISC'\n",
      "   5.822 previous_word=='MOODY' and label is 'ORG'\n",
      "   5.587 previous_word=='Daughters' and label is 'ORG'\n",
      "  -5.580 capitalization==0 and label is 'PER'\n",
      "  -5.439 current_tag=='PRP' and label is 'LOC'\n"
     ]
    }
   ],
   "source": [
    "hmm_t['model'].show_most_informative_features()#most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbOxEtil7fJ5",
    "outputId": "5b98d8be-ebdd-42e9-88ed-9774a2cf4c62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['current_word', 'current_tag', 'capitalization', 'start_of_sentence', 'cap_start', 'previous_tag', 'previous_word', 'previous_2_tag', 'previous_2_word', 'next_tag', 'next_word', 'next_2_tag', 'next_2_word'])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mat[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ils6U47M7a5a"
   },
   "outputs": [],
   "source": [
    "feature_lst=['current_word', 'current_tag', 'capitalization', 'start_of_sentence', 'cap_start', 'previous_tag', 'previous_word', 'previous_2_tag', 'previous_2_word', 'next_tag', 'next_word', 'next_2_tag', 'next_2_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtcdxiLzE63b"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# TODO: please change the line below with your drive organization\n",
    "path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"CS_4740_FA21_p2\")\n",
    "\n",
    "with open(os.path.join(path,'train.json'), 'r') as f:\n",
    "     train = json.loads(f.read())\n",
    "\n",
    "with open(os.path.join(path,'test.json'), 'r') as f:\n",
    "     test = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzGQwBWfvsXG"
   },
   "outputs": [],
   "source": [
    "obs_dict = {'POS':flatten(test['POS']), 'text':flatten(test['text'])} # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-nPXYVSk5FT",
    "outputId": "61fb7037-c385-4f5b-e74b-326f522f57cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "bptr, sc, tag, test_mat = viterbi(hmm_t, obs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGkhL1imxpmH"
   },
   "source": [
    "### **Q3.1: Implementation Details**\n",
    "Explain how you implemented the MEMM and whether/how you modified Viterbi (e.g. which algorithms/data structures you used, what features are included). Make clear which parts were implemented from scratch vs. obtained via an existing package. (Please answer on the written questions template document)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_eDwiILvHGL"
   },
   "source": [
    "### **Q3.2: Results Analysis**\n",
    "Explain here how you evaluated the MEMM model. Summarize the performance of your system and any variations that you experimented with the validation datasets. Put the results into clearly labeled tables or diagrams and include your observations and analysis. (Please answer on the written questions template document)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ammZn20RZn8h"
   },
   "source": [
    "### **Q3.3: Feature Engineering**\n",
    "What features are considered most important by your MaxEnt Classifier? Why do you think these features make sense? Describe your experiments with feature sets. An analysis on feature selection for the MEMM is required â€“ e.g. what features **help most**, why? An **error analysis** is required â€“ e.g. what sorts of errors occurred, why? (Please answer on the written questions template document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4XaDMlcaDBA"
   },
   "source": [
    "### **Q3.4: Room for Improvement**\n",
    "When did the system work well, when did it fail and any ideas as to why? How might you improve the system?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQwPwqU3vMiS"
   },
   "source": [
    "# **Part 4: Comparing HMMs and MEMMs**\n",
    "\n",
    "---\n",
    "\n",
    "In this section you will be asked to analyze and compare the models you have developed!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndVxVFzFagZ5"
   },
   "source": [
    "### **Q4.1: Result Comparison**\n",
    "Compare here your results (validation scores) for your HMM and the MEMM. Which of them performs better? Why? (Please answer on the written questions template document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjGcdm5aafl3"
   },
   "source": [
    "### **Q4.2: Error Analysis 1**\n",
    "Do some error analysis. What are error patterns you observed that the HMM makes but the MEMM does not? Try to justify why/why not? **Please give examples from the dataset.** (Please answer on the written questions template document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "uvCHOoaA3VEV",
    "outputId": "982856b2-d4dc-408d-9d07-abba85ca7198"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>LOC</th>\n",
       "      <th>MISC</th>\n",
       "      <th>O</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.785088</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.072807</td>\n",
       "      <td>0.073684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.035135</td>\n",
       "      <td>0.691892</td>\n",
       "      <td>0.152703</td>\n",
       "      <td>0.072973</td>\n",
       "      <td>0.047297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.005120</td>\n",
       "      <td>0.001473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.063641</td>\n",
       "      <td>0.026563</td>\n",
       "      <td>0.099613</td>\n",
       "      <td>0.727726</td>\n",
       "      <td>0.082457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.030473</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.062811</td>\n",
       "      <td>0.030473</td>\n",
       "      <td>0.872512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.033271</td>\n",
       "      <td>0.017863</td>\n",
       "      <td>0.849496</td>\n",
       "      <td>0.048709</td>\n",
       "      <td>0.050661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred       LOC      MISC         O       ORG       PER\n",
       "true                                                  \n",
       "LOC   0.785088  0.015789  0.052632  0.072807  0.073684\n",
       "MISC  0.035135  0.691892  0.152703  0.072973  0.047297\n",
       "O     0.001403  0.000701  0.991304  0.005120  0.001473\n",
       "ORG   0.063641  0.026563  0.099613  0.727726  0.082457\n",
       "PER   0.030473  0.003731  0.062811  0.030473  0.872512\n",
       "All   0.033271  0.017863  0.849496  0.048709  0.050661"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.DataFrame({'pred':t, 'true':f_y_val})\n",
    "temp_df['true_pred_judge'] = temp_df['pred'] == temp_df['true']\n",
    "\n",
    "pd.crosstab(temp_df['true'], temp_df['pred'], margins = True, normalize = 'index')#.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3D8k4xg-agB"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# TODO: please change the line below with your drive organization\n",
    "path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"CS_4740_FA21_p2\")\n",
    "\n",
    "with open(os.path.join(path,'train.json'), 'r') as f:\n",
    "     train = json.loads(f.read())\n",
    "\n",
    "with open(os.path.join(path,'test.json'), 'r') as f:\n",
    "     test = json.loads(f.read())\n",
    "\n",
    "# Evaluate/validate your model here\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "lng = len(train['text'])\n",
    "\n",
    "X_train_text = train['text'][:int(lng*0.8)]\n",
    "X_val_text = train['text'][int(lng*0.8):]\n",
    "\n",
    "X_train_pos = train['POS'][:int(lng*0.8)]\n",
    "X_val_pos = train['POS'][int(lng*0.8):]\n",
    "\n",
    "y_train = train['NER'][:int(lng*0.8)]\n",
    "y_val = train['NER'][int(lng*0.8):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHIPFSt1v3ju",
    "outputId": "659d04ad-1bad-4518-d702-047cce2e17be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "obs_dict = {'POS':X_val_pos[50], 'text':X_val_text[50]} # \n",
    "bptr, sc, tag_50, test_mat_50 = viterbi(hmm_t, obs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nsvpc2ZgzaUh",
    "outputId": "fd544003-0047-4874-b1d6-e914c9fb7432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O       172\n",
       "LOC      19\n",
       "MISC      6\n",
       "ORG       2\n",
       "PER       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_val[50]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "mvy3IlqGzdb4",
    "outputId": "dcecbd1c-621c-4695-995a-efe38470854d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>LOC</th>\n",
       "      <th>MISC</th>\n",
       "      <th>O</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>171</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred  LOC  MISC    O  ORG  PER  All\n",
       "true                               \n",
       "LOC    19     0    0    0    0   19\n",
       "MISC    0     6    0    0    0    6\n",
       "O       1     0  171    0    0  172\n",
       "ORG     0     0    0    2    0    2\n",
       "PER     0     0    0    0    2    2\n",
       "All    20     6  171    2    2  201"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.DataFrame({'pred':tag_50, 'true':y_val[50]})\n",
    "temp_df['true_pred_judge'] = temp_df['pred'] == temp_df['true']\n",
    "\n",
    "pd.crosstab(temp_df['true'], temp_df['pred'], margins = True)#.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wa510EP78x1D"
   },
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame({'pred':tag_50, 'true':y_val[50], 'word': X_val_text[50]})\n",
    "temp_df['pred_true_match'] = temp_df['pred'] == temp_df['true']\n",
    "print(temp_df['pred_true_match'].value_counts())\n",
    "temp_df[temp_df['pred_true_match'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7ZQCPpvwCag",
    "outputId": "52dcf394-da4d-498b-82ce-59261c1aa804"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "obs_dict = {'POS':X_val_pos[2], 'text':X_val_text[2]} # \n",
    "bptr, sc, tag_2, test_mat_2 = viterbi(hmm_t, obs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBbRoAKBzyM5",
    "outputId": "2e3c828f-d2a2-4de1-933b-e046d4d9f600"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O       460\n",
       "ORG      20\n",
       "LOC      12\n",
       "PER       9\n",
       "MISC      6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_val[2]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "i4IImLmSzyJ0",
    "outputId": "b90eab15-81b3-4ca8-b4c3-e9147b94f413"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>LOC</th>\n",
       "      <th>MISC</th>\n",
       "      <th>O</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>449</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>458</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred  LOC  MISC    O  ORG  PER  All\n",
       "true                               \n",
       "LOC    10     0    0    0    2   12\n",
       "MISC    0     5    1    0    0    6\n",
       "O       0     2  449    7    2  460\n",
       "ORG     2     0    6    5    7   20\n",
       "PER     0     0    2    0    7    9\n",
       "All    12     7  458   12   18  507"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.DataFrame({'pred':tag_2, 'true':y_val[2]})\n",
    "temp_df['true_pred_judge'] = temp_df['pred'] == temp_df['true']\n",
    "\n",
    "pd.crosstab(temp_df['true'], temp_df['pred'], margins = True)#.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIIQ7ioAzyFo"
   },
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame({'pred':tag_2, 'true':y_val[2], 'word': X_val_text[2]})\n",
    "temp_df['pred_true_match'] = temp_df['pred'] == temp_df['true']\n",
    "print(temp_df['pred_true_match'].value_counts())\n",
    "temp_df[temp_df['pred_true_match'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZYfZAgga9UB"
   },
   "source": [
    "### **Q4.3: Error Analysis 2**\n",
    "What are error patterns you observed that MEMM makes but the HMM does not? Try to justify what you observe? **Please give examples from the dataset.** (Please answer on the written questions template document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phgyrgQNwGiN"
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(train['text'], train['NER'], test_size=0.1, random_state=0)\n",
    "# X_pos, X_index, y_pos, y_index = train_test_split(train['POS'], train['index'], test_size=0.1, random_state=0)\n",
    "X_train = X_train_text\n",
    "X_test = X_val_text\n",
    "y_train = y_train \n",
    "y_test = y_val\n",
    "\n",
    "train_unk = unknow_train(X_train,0.01)\n",
    "test_unk  = unknow_test(train_unk,X_test) \n",
    "hmm = build_hmm(train_unk,y_train,0.05,0.7)\n",
    "\n",
    "index = []\n",
    "pred  = []\n",
    "true  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LW7SroWN3qOz"
   },
   "outputs": [],
   "source": [
    "for number, comment in enumerate(X_test):     \n",
    "  pred_tag = viterbi(hmm, comment)\n",
    "  \n",
    "  # index = index + y_index[number]\n",
    "  pred = pred + pred_tag\n",
    "  true = true + y_test[number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5V3JJVw7GDIq"
   },
   "outputs": [],
   "source": [
    "pred_token_labels = pred\n",
    "true_token_labels = f_y_val\n",
    "token_indices = list(range(len(f_y_val)))\n",
    "\n",
    "y_pred_dict = format_output_labels(pred_token_labels, token_indices)\n",
    "print(\"y_pred_dict is : \" + str(y_pred_dict))\n",
    "y_true_dict = format_output_labels(true_token_labels, token_indices)\n",
    "print(\"y_true_dict is : \" + str(y_true_dict))\n",
    "\n",
    "print(\"Entity Level Mean F1 score is : \" + str(mean_f1(y_pred_dict, y_true_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "7B7XnmqNw8lm",
    "outputId": "82914f2d-79bb-4aa7-f570-605b7acf5307"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>LOC</th>\n",
       "      <th>MISC</th>\n",
       "      <th>O</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.709351</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>0.184038</td>\n",
       "      <td>0.079809</td>\n",
       "      <td>0.019059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.020591</td>\n",
       "      <td>0.681289</td>\n",
       "      <td>0.256938</td>\n",
       "      <td>0.015219</td>\n",
       "      <td>0.025962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.984319</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.013510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.057197</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.401894</td>\n",
       "      <td>0.493182</td>\n",
       "      <td>0.029545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.350933</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>0.634502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.028311</td>\n",
       "      <td>0.017435</td>\n",
       "      <td>0.879767</td>\n",
       "      <td>0.031580</td>\n",
       "      <td>0.042908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred       LOC      MISC         O       ORG       PER\n",
       "true                                                  \n",
       "LOC   0.709351  0.007743  0.184038  0.079809  0.019059\n",
       "MISC  0.020591  0.681289  0.256938  0.015219  0.025962\n",
       "O     0.000073  0.000585  0.984319  0.001512  0.013510\n",
       "ORG   0.057197  0.018182  0.401894  0.493182  0.029545\n",
       "PER   0.004096  0.000910  0.350933  0.009558  0.634502\n",
       "All   0.028311  0.017435  0.879767  0.031580  0.042908"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.DataFrame({'pred':pred, 'true':true})\n",
    "temp_df['true_pred_judge'] = temp_df['pred'] == temp_df['true']\n",
    "\n",
    "pd.crosstab(temp_df['true'], temp_df['pred'], margins = True, normalize = 'index')#.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eS-m9V-FyUu7"
   },
   "outputs": [],
   "source": [
    "pred_tag = viterbi(hmm, test_unk[50])\n",
    "true_tag = y_test[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "sPK5dvwQ0Shd",
    "outputId": "c45936a4-1ece-4c36-fdc6-0d8ebc979f38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>LOC</th>\n",
       "      <th>MISC</th>\n",
       "      <th>O</th>\n",
       "      <th>ORG</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>172</td>\n",
       "      <td>6</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred  LOC  MISC    O  ORG  All\n",
       "true                          \n",
       "LOC    17     0    0    2   19\n",
       "MISC    0     6    0    0    6\n",
       "O       0     0  170    2  172\n",
       "ORG     0     0    0    2    2\n",
       "PER     0     0    2    0    2\n",
       "All    17     6  172    6  201"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.DataFrame({'pred':pred_tag, 'true':true_tag, 'word': X_val_text[50]})\n",
    "temp_df['true_pred_judge'] = temp_df['pred'] == temp_df['true']\n",
    "\n",
    "pd.crosstab(temp_df['true'], temp_df['pred'], margins = True)#.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "HqgKdUD3AUk6",
    "outputId": "3388b67d-aba0-40c3-da55-42782e7f81f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     195\n",
      "False      6\n",
      "Name: pred_true_match, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "      <th>word</th>\n",
       "      <th>true_pred_judge</th>\n",
       "      <th>pred_true_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Sudan</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>ORG</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Larnaca</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>Security</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pred true      word  true_pred_judge  pred_true_match\n",
       "0    ORG    O    unknow            False            False\n",
       "1    ORG  LOC     Sudan            False            False\n",
       "134    O  PER    unknow            False            False\n",
       "135    O  PER    unknow            False            False\n",
       "141  ORG  LOC   Larnaca            False            False\n",
       "144  ORG    O  Security            False            False"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df['pred_true_match'] = temp_df['pred'] == temp_df['true']\n",
    "print(temp_df['pred_true_match'].value_counts())\n",
    "temp_df[temp_df['pred_true_match'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emwT4QcL0b-U"
   },
   "outputs": [],
   "source": [
    "pred_tag = viterbi(hmm, test_unk[2])\n",
    "true_tag = y_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciJIs-EG4xIP",
    "outputId": "898e8d02-5727-4512-81e7-59b78bc929b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O       460\n",
       "ORG      20\n",
       "LOC      12\n",
       "PER       9\n",
       "MISC      6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test[2]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "oOke8kdk0bwd",
    "outputId": "1ba8d264-2b6e-4f1f-9c87-5d722a2c735a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>LOC</th>\n",
       "      <th>MISC</th>\n",
       "      <th>O</th>\n",
       "      <th>PER</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>456</td>\n",
       "      <td>4</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>488</td>\n",
       "      <td>5</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred  LOC  MISC    O  PER  All\n",
       "true                          \n",
       "LOC    10     0    1    1   12\n",
       "MISC    1     3    2    0    6\n",
       "O       0     0  456    4  460\n",
       "ORG     0     0   20    0   20\n",
       "PER     0     0    9    0    9\n",
       "All    11     3  488    5  507"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.DataFrame({'pred':pred_tag, 'true':true_tag,'word': X_val_text[2]})\n",
    "temp_df['true_pred_judge'] = temp_df['pred'] == temp_df['true']\n",
    "\n",
    "pd.crosstab(temp_df['true'], temp_df['pred'], margins = True)#.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "q0Lkjc85ApSj",
    "outputId": "4dc47b8c-f827-445a-e84f-d771ec5849eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     469\n",
      "False     38\n",
      "Name: pred_true_match, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "      <th>word</th>\n",
       "      <th>true_pred_judge</th>\n",
       "      <th>pred_true_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PER</td>\n",
       "      <td>LOC</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>O</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PER</td>\n",
       "      <td>O</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PER</td>\n",
       "      <td>O</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PER</td>\n",
       "      <td>O</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>O</td>\n",
       "      <td>LOC</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>O</td>\n",
       "      <td>MISC</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>O</td>\n",
       "      <td>MISC</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>LOC</td>\n",
       "      <td>MISC</td>\n",
       "      <td>Czech</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>O</td>\n",
       "      <td>ORG</td>\n",
       "      <td>unknow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pred  true    word  true_pred_judge  pred_true_match\n",
       "0      O   ORG  unknow            False            False\n",
       "3    PER   LOC  unknow            False            False\n",
       "4    PER     O  unknow            False            False\n",
       "5    PER     O  unknow            False            False\n",
       "6    PER     O  unknow            False            False\n",
       "7    PER     O  unknow            False            False\n",
       "9      O   PER  unknow            False            False\n",
       "10     O   PER  unknow            False            False\n",
       "11     O   LOC  unknow            False            False\n",
       "15     O   ORG  unknow            False            False\n",
       "16     O   ORG  unknow            False            False\n",
       "83     O   PER  unknow            False            False\n",
       "84     O   PER  unknow            False            False\n",
       "90     O   ORG  unknow            False            False\n",
       "91     O   ORG  unknow            False            False\n",
       "94     O  MISC  unknow            False            False\n",
       "165    O   ORG  unknow            False            False\n",
       "183    O   PER  unknow            False            False\n",
       "184    O   PER  unknow            False            False\n",
       "187    O  MISC  unknow            False            False\n",
       "190    O   ORG  unknow            False            False\n",
       "191    O   ORG  unknow            False            False\n",
       "192    O   ORG  unknow            False            False\n",
       "210    O   ORG  unknow            False            False\n",
       "270    O   ORG  unknow            False            False\n",
       "289    O   ORG  unknow            False            False\n",
       "306    O   PER  unknow            False            False\n",
       "318    O   ORG  unknow            False            False\n",
       "336    O   ORG  unknow            False            False\n",
       "351    O   PER  unknow            False            False\n",
       "355    O   ORG  unknow            False            False\n",
       "390    O   PER  unknow            False            False\n",
       "399    O   ORG  unknow            False            False\n",
       "410    O   ORG  unknow            False            False\n",
       "418  LOC  MISC   Czech            False            False\n",
       "421    O   ORG  unknow            False            False\n",
       "422    O   ORG  unknow            False            False\n",
       "439    O   ORG  unknow            False            False"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df['pred_true_match'] = temp_df['pred'] == temp_df['true']\n",
    "print(temp_df['pred_true_match'].value_counts())\n",
    "temp_df[temp_df['pred_true_match'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpugxBD7RBy1"
   },
   "source": [
    "# **Part 5: Kaggle Submission**\n",
    "---\n",
    "\n",
    "Using the best-performing system from among all of your HMM and MEMM models, generate predictions for the test set, and submit them to Kaggle at https://www.kaggle.com/c/cs4740-fa21-p2. Note, you **need** to use our tokenizer as the labels on Kaggle corresponds to these. Below, we provide a function that submits given predicted tokens and associated token indices in the correct format. As a scoring metric on Kaggle, we use **Entity Level Mean F1**.\n",
    "\n",
    "Your submission to Kaggle should be a CSV file consisting of five lines and two columns. The first line is a fixed header, and each of the remaining four lines corresponds to one of the four types of named entities. The first column is the label identifier *Id* (one of PER, LOC, ORG or MISC), and the second column *Predicted* is a list of entities (separated by single space) that you predict to be of that type. Each entity is specified by its starting and ending index (concatenated by a hypen) as given in the test corpus. \n",
    "\n",
    "You can use the function **create_submission** that takes the list of predicted labels and the list of associated token indices as inputs and creates the the output CSV file at a specified path.\n",
    "\n",
    "NOTE: Ensure that there are **no** rows with *Id* = \"O\" in your Kaggle Submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1o4ssI2P_ZSi"
   },
   "source": [
    "![picture](https://docs.google.com/uc?export=download&id=1pQkAyOdWQz62jB-YBaj8mHuwI6iWJ1GZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "893l9j77ETFM"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def create_submission(output_filepath, token_labels, token_inds):\n",
    "    \"\"\"\n",
    "    :parameter output_filepath: The full path (including file name) of the output file, \n",
    "                                with extension .csv\n",
    "    :type output_filepath: [String]\n",
    "    :parameter token_labels: A list of token labels (eg. PER, LOC, ORG or MISC).\n",
    "    :type token_labels: List[String]\n",
    "    :parameter token_indices: A list of token indices (taken from the dataset) \n",
    "                              corresponding to the labels in [token_labels].\n",
    "    :type token_indices: List[int]\n",
    "    \"\"\"\n",
    "    label_dict = format_output_labels(token_labels, token_inds)\n",
    "    with open(output_filepath, mode='w') as csv_file:\n",
    "        fieldnames = ['Id', 'Expected']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for key in label_dict:\n",
    "            p_string = \" \".join([str(start)+\"-\"+str(end) for start,end in label_dict[key]])\n",
    "            writer.writerow({'Id': key, 'Expected': p_string})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kQvLRaBQQ0V"
   },
   "outputs": [],
   "source": [
    "t = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajEmOE0qUs5V"
   },
   "outputs": [],
   "source": [
    "token_indices = list(range(len(t)))\n",
    "y_pred_dict = format_output_labels(t, token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrwFEbeLVzpz"
   },
   "outputs": [],
   "source": [
    "create_submission('output_memm.csv', t, token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FiF3iYfEuoH"
   },
   "outputs": [],
   "source": [
    "create_submission('output_hmm.csv', pred, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEZP_FGivVRI"
   },
   "source": [
    "---\n",
    "### **Q5.1: Competition Score**\n",
    "\n",
    "Include your **team name** and the **screenshot** of your best score from Kaggle. (Please answer on the written questions template document)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS_4740_FA21_p2_ll924_sz649.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
